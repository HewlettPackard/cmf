{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Getting started with CMF","text":""},{"location":"#purpose-and-scope","title":"Purpose and Scope","text":"<p>This document provides a comprehensive overview of the Common Metadata Framework (CMF), which implements a system for collecting, storing, and querying metadata associated with Machine Learning (ML) pipelines. CMF adopts a data-first approach where all artifacts (datasets, ML models, and performance metrics) are versioned and identified by their content hash, enabling distributed metadata tracking and collaboration across ML teams.</p> <p>For detailed API documentation, see Core Library (CMFLib). For server deployment instructions, see Installation &amp; Setup. For web user interface details, see CMF GUI.</p>"},{"location":"#system-architecture","title":"System Architecture","text":"<p>CMF is designed as a distributed system that enables ML teams to track pipeline metadata locally and synchronize with a central server. The framework automatically tracks code versions, data artifacts, and execution metadata to provide end-to-end traceability of ML experiments.</p> <p>Common Metadata Framework (<code>CMF</code>) has the following components:</p> <ul> <li>CMFLib: A Python library that captures and tracks metadata throughout your ML pipeline, including datasets, models, and metrics. It provides APIs for both logging metadata during execution and querying it later for analysis.</li> <li>CMF Client: A command-line tool that synchronizes metadata with the <code>CMF Server</code>, manages artifact transfers to and from storage repositories, and integrates with Git for version control.</li> <li>CMF Server with GUI: A centralized server that aggregates metadata from multiple clients and provides a web-based graphical interface for visualizing pipeline executions, artifacts, and lineage relationships, enabling teams to collaborate effectively.</li> <li>Central Artifact Repositories: Storage backends (such as AWS S3, MinIO, or SSH-based storage) that host your datasets, models, and other pipeline artifacts.</li> </ul> <p> </p>"},{"location":"#core-abstractions","title":"Core Abstractions","text":"<p>CMF uses three primary abstractions to model ML pipeline metadata:</p> Abstraction Purpose Implementation Pipeline Groups related stages and executions Identified by name in <code>cmflib.cmf.Cmf</code> constructor Context Represents a stage type (e.g., \"train\", \"test\") Created via <code>create_context()</code> method Execution Represents a specific run of a stage Created via <code>create_execution()</code> method <pre><code>graph LR\n    PIPELINE[\"Pipeline&lt;br/&gt;'mnist_experiment'\"] --&gt; CONTEXT1[\"Context&lt;br/&gt;'download'\"]\n    PIPELINE --&gt; CONTEXT2[\"Context&lt;br/&gt;'train'\"]\n    PIPELINE --&gt; CONTEXT3[\"Context&lt;br/&gt;'test'\"]\n\n    CONTEXT1 --&gt; EXEC1[\"Execution&lt;br/&gt;'download_data'\"]\n    CONTEXT2 --&gt; EXEC2[\"Execution&lt;br/&gt;'train_model'\"]\n    CONTEXT3 --&gt; EXEC3[\"Execution&lt;br/&gt;'evaluate_model'\"]\n\n    EXEC1 --&gt; DATASET1[\"Dataset&lt;br/&gt;'raw_data.csv'\"]\n    EXEC2 --&gt; MODEL1[\"Model&lt;br/&gt;'trained_model.pkl'\"]\n    EXEC3 --&gt; METRICS1[\"Metrics&lt;br/&gt;'accuracy: 0.95'\"]</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#distributed-metadata-tracking","title":"Distributed Metadata Tracking","text":"<p>CMF enables distributed teams to work independently while maintaining consistent metadata through content-addressable artifacts and Git-like synchronization:</p> <ul> <li>Local Development: Each developer works with a local MLMD database</li> <li>Content Hashing: All artifacts are identified by their content hash for universal identification</li> <li>Synchronization: <code>cmf metadata push/pull</code> commands sync with central server</li> <li>Artifact Storage: Support for MinIO, Amazon S3, SSH, and local storage backends</li> </ul>"},{"location":"#automatic-version-tracking","title":"Automatic Version Tracking","text":"<p>CMF automatically captures:</p> <ul> <li>Code Version: Git commit IDs for reproducibility</li> <li>Data Version: DVC-managed artifact content hashes</li> <li>Environment: Execution parameters and custom properties</li> <li>Lineage: Input/output relationships between executions</li> </ul>"},{"location":"#query-and-visualization","title":"Query and Visualization","text":"<p>The system provides multiple interfaces for exploring metadata:</p> <ul> <li>Programmatic: <code>CmfQuery</code> class for custom queries</li> <li>Web UI: React-based interface for browsing artifacts and executions</li> <li>Lineage Graphs: D3.js visualizations showing data flow between pipeline stages</li> <li>TensorBoard Integration: Training metrics visualization</li> </ul>"},{"location":"_src/","title":"CMF docs development resources","text":"<p>This directory contains files that are used to create some content for the CMF documentation. This process is not automated yet. Files in this directory are not supposed to be referenced from documentation pages.</p> <p>It also should not be required to automatically redeploy documentation (e.g., with GitHub actions) when documentation files change only in this particular directory.</p> <ul> <li>The diagrams.drawio file is created with PyCharm's    Diagram.NET plugin. It contains a number of diagrams used in the documentation. To   update those diagrams, use this file to edit them, take a screenshot, edit with an image editor, and then    overwrite the corresponding files (e.g., ML Pipeline Definition) used on the main page.</li> </ul>"},{"location":"api/public/cmf/","title":"cmflib.cmf","text":""},{"location":"api/public/cmf/#cmflib.cmf.Cmf","title":"<code>cmflib.cmf.Cmf(filepath='mlmd', pipeline_name='', custom_properties=None, graph=False, is_server=False)</code>","text":"<p>This class provides methods to log metadata for distributed AI pipelines. The class instance creates an ML metadata store to store the metadata. It creates a driver to store nodes and its relationships to neo4j. The user has to provide the name of the pipeline, that needs to be recorded with CMF.</p> <pre><code>    from cmflib.cmf import Cmf\nmetawriter = Cmf(\n    filepath=\"mlmd\",\n    pipeline_name=\"test_pipeline\",\n    custom_properties={\"owner\": \"user_a\"},\n    graph=False\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path  to the sqlite file to store the metadata</p> <code>'mlmd'</code> <code>pipeline_name</code> <code>str</code> <p>Name to uniquely identify the pipeline. Note that name is the unique identifier for a pipeline. If a pipeline already exist with the same name, the existing pipeline object is reused.</p> <code>''</code> <code>custom_properties</code> <code>Optional[Dict]</code> <p>Additional properties of the pipeline that needs to be stored.</p> <code>None</code> <code>graph</code> <code>bool</code> <p>If set to true, the libray also stores the relationships in the provided graph database.</p> <code>False</code> <p>The following variables should be set: <code>neo4j_uri</code> (graph server URI), <code>neo4j_user</code> (user name) and <code>neo4j_password</code> (user password), e.g.: <pre><code>cmf init local --path /home/user/local-storage --git-remote-url https://github.com/XXX/exprepo.git --neo4j-user neo4j --neo4j-password neo4j\n                        --neo4j-uri bolt://localhost:7687\n</code></pre></p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.create_context","title":"<code>create_context(pipeline_stage, custom_properties=None)</code>","text":"<p>Create's a  context(stage). Every call creates a unique pipeline stage. Updates Pipeline_stage name.</p> <pre><code># Create context\n# Import CMF\nfrom cmflib.cmf import Cmf\nfrom ml_metadata.proto import metadata_store_pb2 as mlpb\n# Create CMF logger\nmetawriter = Cmf(filepath=\"mlmd\", pipeline_name=\"test_pipeline\")\n# Create context\ncontext: mlmd.proto.Context = metawriter.create_context(\n    pipeline_stage=\"prepare\",\n    custom_properties ={\"user-metadata1\": \"metadata_value\"}\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_stage</code> <code>str</code> <p>Name of the Stage.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Developers can provide key value pairs with additional properties of the execution that need to be stored.</p> <code>None</code> <p>Returns:</p> Type Description <code>Context</code> <p>Context object from ML Metadata library associated with the new context for this stage.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.create_execution","title":"<code>create_execution(execution_type, custom_properties=None, cmd=None, create_new_execution=True)</code>","text":"<p>Create execution. Every call creates a unique execution. Execution can only be created within a context, so create_context must be called first.</p> <pre><code># Import CMF\nfrom cmflib.cmf import Cmf\nfrom ml_metadata.proto import metadata_store_pb2 as mlpb\n# Create CMF logger\nmetawriter = Cmf(filepath=\"mlmd\", pipeline_name=\"test_pipeline\")\n# Create or reuse context for this stage\ncontext: mlmd.proto.Context = metawriter.create_context(\n    pipeline_stage=\"prepare\",\n    custom_properties ={\"user-metadata1\": \"metadata_value\"}\n)\n# Create a new execution for this stage run\nexecution: mlmd.proto.Execution = metawriter.create_execution(\n    execution_type=\"Prepare\",\n    custom_properties = {\"split\": split, \"seed\": seed}\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>execution_type</code> <code>str</code> <p>Type of the execution.(when create_new_execution is False, this is the name of execution)</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Developers can provide key value pairs with additional properties of the execution that need to be stored.</p> <code>None</code> <code>cmd</code> <code>Optional[str]</code> <p>command used to run this execution.</p> <code>None</code> <code>create_new_execution</code> <code>bool</code> <p>bool = True, This can be used by advanced users to re-use executions This is applicable, when working with framework code like mmdet, pytorch lightning etc, where the custom call-backs are used to log metrics. if create_new_execution is True(Default), execution_type parameter will be used as the name of the execution type. if create_new_execution is False, if existing execution exist with the same name as execution_type. it will be reused. Only executions created with  create_new_execution as False will have \"name\" as a property.</p> <code>True</code> <p>Returns:</p> Type Description <code>Execution</code> <p>Execution object from ML Metadata library associated with the new execution for this stage.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.update_execution","title":"<code>update_execution(execution_id, custom_properties=None)</code>","text":"<p>Updates an existing execution. The custom properties can be updated after creation of the execution. The new custom properties is merged with earlier custom properties.</p> <pre><code># Import CMF\nfrom cmflib.cmf import Cmf\nfrom ml_metadata.proto import metadata_store_pb2 as mlpb\n# Create CMF logger\nmetawriter = Cmf(filepath=\"mlmd\", pipeline_name=\"test_pipeline\")\n# Update a execution\nexecution: mlmd.proto.Execution = metawriter.update_execution(\n    execution_id=8,\n    custom_properties = {\"split\": split, \"seed\": seed}\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>execution_id</code> <code>int</code> <p>id of the execution.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Developers can provide key value pairs with additional properties of the execution that need to be updated.</p> <code>None</code> <p>Returns:</p> Type Description <code>Execution</code> <p>Execution object from ML Metadata library associated with the updated execution for this stage.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_dataset","title":"<code>log_dataset(url, event, custom_properties=None, label=None, label_properties=None, external=False)</code>","text":"<p>Logs a dataset as artifact. This call adds the dataset to dvc. The dvc metadata file created (.dvc) will be added to git and committed. The version of the  dataset is automatically obtained from the versioning software(DVC) and tracked as a metadata.</p> <pre><code>artifact: mlmd.proto.Artifact = metawriter.log_dataset(\n    url=\"/repo/data.xml\",\n    event=\"input\",\n    custom_properties={\"source\":\"kaggle\"},\n    label=artifacts/labels.csv,\n    label_properties={\"user\":\"Ron\"}\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The path to the dataset.</p> required <code>event</code> <code>str</code> <p>Takes arguments <code>INPUT</code> OR <code>OUTPUT</code>.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Dataset properties (key/value pairs).</p> <code>None</code> <code>label</code> <code>Optional[str]</code> <p>Labels are usually .csv files containing information regarding the dataset.</p> <code>None</code> <code>label_properties</code> <code>Optional[Dict]</code> <p>Custom properties for a label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>Artifact object from ML Metadata library associated with the new dataset artifact.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_model","title":"<code>log_model(path, event, model_framework='Default', model_type='Default', model_name='Default', custom_properties=None)</code>","text":"<p>Logs a model. The model is added to dvc and the metadata file (.dvc) gets committed to git.</p> <pre><code>artifact: mlmd.proto.Artifact= metawriter.log_model(\n    path=\"path/to/model.pkl\",\n    event=\"output\",\n    model_framework=\"SKlearn\",\n    model_type=\"RandomForestClassifier\",\n    model_name=\"RandomForestClassifier:default\"\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the model file.</p> required <code>event</code> <code>str</code> <p>Takes arguments <code>INPUT</code> OR <code>OUTPUT</code>.</p> required <code>model_framework</code> <code>str</code> <p>Framework used to create the model.</p> <code>'Default'</code> <code>model_type</code> <code>str</code> <p>Type of model algorithm used.</p> <code>'Default'</code> <code>model_name</code> <code>str</code> <p>Name of the algorithm used.</p> <code>'Default'</code> <code>custom_properties</code> <code>Optional[Dict]</code> <p>The model properties.</p> <code>None</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>Artifact object from ML Metadata library associated with the new model artifact.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_execution_metrics","title":"<code>log_execution_metrics(metrics_name, custom_properties=None)</code>","text":"<p>Log the metadata associated with the execution (coarse-grained tracking). It is stored as a metrics artifact. This does not have a backing physical file, unlike other artifacts that we have.</p> <pre><code>exec_metrics: mlpb.Artifact = metawriter.log_execution_metrics(\n    metrics_name=\"Training_Metrics\",\n    {\"auc\": auc, \"loss\": loss}\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>metrics_name</code> <code>str</code> <p>Name to identify the metrics.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Dictionary with metric values.</p> <code>None</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>Artifact object from ML Metadata library associated with the new coarse-grained metrics artifact.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_metric","title":"<code>log_metric(metrics_name, custom_properties=None)</code>","text":"<p>Stores the fine-grained (per step or per epoch) metrics to memory. The metrics provided are stored in a parquet file. The <code>commit_metrics</code> call add the parquet file in the version control framework. The metrics written in the parquet file can be retrieved using the <code>read_metrics</code> call.</p> <pre><code># Can be called at every epoch or every step in the training. This is logged to a parquet file and committed\n# at the commit stage.\n# Inside training loop\nwhile True:\n        metawriter.log_metric(\"training_metrics\", {\"train_loss\": train_loss})\nmetawriter.commit_metrics(\"training_metrics\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>metrics_name</code> <code>str</code> <p>Name to identify the metrics.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Dictionary with metrics.</p> <code>None</code>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.create_dataslice","title":"<code>create_dataslice(name)</code>","text":"<p>Creates a dataslice object. Once created, users can add data instances to this data slice with add_data method. Users are also responsible for committing data slices by calling the commit method.</p> <pre><code>dataslice = metawriter.create_dataslice(\"slice-a\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name to identify the dataslice.</p> required <p>Returns:</p> Type Description <code>DataSlice</code> <p>Instance of a newly created DataSlice.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.update_dataslice","title":"<code>update_dataslice(name, record, custom_properties)</code>","text":"<p>Updates a dataslice record in a Parquet file with the provided custom properties.</p> <pre><code>   dataslice=metawriter.update_dataslice(\"dataslice_file.parquet\", \"record_id\", \n   {\"key1\": \"updated_value\"})\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the Parquet file.</p> required <code>record</code> <code>str</code> <p>Identifier of the dataslice record to be updated.</p> required <code>custom_properties</code> <code>Dict</code> <p>Dictionary containing custom properties to update.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_label","title":"<code>log_label(url, dataset_name, custom_properties=None)</code>","text":"<p>Logs a label artifact associated with a dataset.</p> <p>This function checks whether a label artifact (identified by <code>label_hash</code>) already exists in the metadata store. - If the artifact exists, it links it to the current execution and optionally updates its properties and URL. - If the artifact does not exist, it creates a new artifact with the provided properties and links it to the execution context.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The base URL representing the label (e.g., path or storage location).</p> required <code>dataset_name</code> <code>str</code> <p>The name of the associated dataset.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Additional metadata to associate with the artifact. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>mlpb.Artifact: The logged or linked label artifact.</p>"},{"location":"api/public/cmf/#cmflib.cmf","title":"<code>cmflib.cmf</code>","text":"<p>This module contains all the public API for CMF</p>"},{"location":"api/public/cmf/#cmflib.cmf.cmf_init_show","title":"<code>cmf_init_show()</code>","text":"<p>Initializes and shows details of the CMF command. </p> <pre><code>result = cmf_init_show() \n</code></pre> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _cmf_init_show function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.cmf_init","title":"<code>cmf_init(type='', path='', git_remote_url='', cmf_server_url='http://127.0.0.1:80', url='', endpoint_url='', access_key_id='', secret_key='', session_token='', user='', password='', port=0, osdf_path='', osdf_cache='', key_id='', key_path='', key_issuer='', neo4j_user=None, neo4j_password=None, neo4j_uri=None)</code>","text":"<p>Initializes the CMF configuration based on the provided parameters. </p> Example <pre><code>cmf_init(type=\"local\", \n            path=\"/path/to/repo\",\n            git_remote_url=\"https://github.com/hpe-user/experiment-repo.git\",\n            cmf_server_url=\"http://cmf-server:80\",\n            neo4j_user=\"neo4j\",\n            neo4j_password=\"password\",\n            neo4j_uri=\"bolt://localhost:7687\"\n        )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>Type of repository (\"local\", \"minioS3\", \"amazonS3\", \"sshremote\", \"osdfremote\") (required)</p> <code>''</code> <code>path</code> <code>str</code> <p>Path for the local/ssh repository. (required for \"local\" and \"sshremote\" types)</p> <code>''</code> <code>git_remote_url</code> <code>str</code> <p>Git remote URL for version control. (required)</p> <code>''</code> <code>url</code> <code>str</code> <p>URL for MinioS3 or AmazonS3. (required)</p> <code>''</code> <code>endpoint_url</code> <code>str</code> <p>Endpoint URL for MinioS3. (required)</p> <code>''</code> <code>access_key_id</code> <code>str</code> <p>Access key ID for MinioS3 or AmazonS3. (required)</p> <code>''</code> <code>secret_key</code> <code>str</code> <p>Secret key for MinioS3 or AmazonS3. (required)</p> <code>''</code> <code>session_token</code> <code>str</code> <p>Session token for AmazonS3. (required)</p> <code>''</code> <code>user</code> <code>str</code> <p>SSH remote username. (required)</p> <code>''</code> <code>password</code> <code>str</code> <p>SSH remote password. (required)</p> <code>''</code> <code>port</code> <code>int</code> <p>SSH remote port. (required)</p> <code>0</code> <code>osdf_path</code> <code>str</code> <p>OSDF Origin Path. (required)</p> <code>''</code> <code>osdf_cache</code> <code>str</code> <p>OSDF Cache Path (required).</p> <code>''</code> <code>key_id</code> <code>str</code> <p>OSDF Key ID. (required)</p> <code>''</code> <code>key_path</code> <code>str</code> <p>OSDF Private Key Path. (required)</p> <code>''</code> <code>key_issuer</code> <code>str</code> <p>OSDF Key Issuer URL. (required)</p> <code>''</code> <code>cmf_server_url</code> <code>str</code> <p>CMF server URL. (default: \"http://127.0.0.1:80\")</p> <code>'http://127.0.0.1:80'</code> <code>neo4j_user</code> <code>Optional[str]</code> <p>Neo4j database username. (optional)</p> <code>None</code> <code>neo4j_password</code> <code>Optional[str]</code> <p>Neo4j database password. (optional)</p> <code>None</code> <code>neo4j_uri</code> <code>Optional[str]</code> <p>Neo4j database URI. (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Output based on the initialized repository type.</p>"},{"location":"api/public/cmf/#cmflib.cmf.metadata_push","title":"<code>metadata_push(pipeline_name, file_name='./mlmd', tensorboard_path=None, execution_uuid=None)</code>","text":"<p>Pushes metadata file to CMF-server.</p> <pre><code>result = metadata_push(\"example_pipeline\", \"mlmd_file\", \"eg_execution_uuid\", \"tensorboard_log\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline.</p> required <code>file_name</code> <code>str</code> <p>Specify input metadata file name.</p> <code>'./mlmd'</code> <code>execution_uuid</code> <code>Optional[str]</code> <p>Optional execution UUID.</p> <code>None</code> <code>tensorboard_path</code> <code>Optional[str]</code> <p>Path to tensorboard logs.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Response output from the _metadata_push function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.metadata_pull","title":"<code>metadata_pull(pipeline_name, file_name='./mlmd', execution_uuid=None)</code>","text":"<p>Pulls metadata file from CMF-server. </p> <pre><code>result = metadata_pull(\"example_pipeline\", \"./mlmd_directory\", \"eg_execution_uuid\") \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline. </p> required <code>file_name</code> <code>str</code> <p>Specify output metadata file name.</p> <code>'./mlmd'</code> <code>execution_uuid</code> <code>Optional[str]</code> <p>Optional execution UUID. </p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Message from the _metadata_pull function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.metadata_export","title":"<code>metadata_export(pipeline_name, json_file_name=None, file_name='./mlmd')</code>","text":"<p>Export local mlmd's metadata in json format to a json file. </p> <pre><code>result = metadata_export(\"example_pipeline\", \"./jsonfile\", \"./mlmd_directory\") \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline. </p> required <code>json_file_name</code> <code>Optional[str]</code> <p>File path of json file. </p> <code>None</code> <code>file_name</code> <code>str</code> <p>Specify input metadata file name. </p> <code>'./mlmd'</code> <p>Returns:</p> Type Description <code>str</code> <p>Message from the _metadata_export function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.artifact_pull","title":"<code>artifact_pull(pipeline_name, file_name='./mlmd', artifact_name=None)</code>","text":"<p>Pulls artifacts from the initialized repository.</p> <pre><code>result = artifact_pull(\"example_pipeline\", \"./mlmd_directory\", \"artifact_name)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline.</p> required <code>file_name</code> <code>str</code> <p>Specify input metadata file name.</p> <code>'./mlmd'</code> <code>artifact_name</code> <code>Optional[str]</code> <p>Name of the artifact</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _artifact_pull function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.artifact_push","title":"<code>artifact_push(pipeline_name, filepath='./mlmd', jobs=32)</code>","text":"<p>Pushes artifacts to the initialized repository.</p> <pre><code>result = artifact_push(\"example_pipeline\", \"./mlmd_directory\", \"32\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline. </p> required <code>filepath</code> <code>str</code> <p>Path to store the artifact. </p> <code>'./mlmd'</code> <code>jobs</code> <code>int</code> <p>Number of jobs to use for pushing artifacts.</p> <code>32</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _artifact_push function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.artifact_list","title":"<code>artifact_list(pipeline_name, file_name='./mlmd', artifact_name=None)</code>","text":"<p>Displays artifacts from the input metadata file with a few properties in a 7-column table, limited to 20 records per page.</p> <pre><code>result = _artifact_list(\"example_pipeline\", \"./mlmd_directory\", \"example_artifact_name\") \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline. </p> required <code>file_name</code> <code>str</code> <p>Specify input metadata file name. </p> <code>'./mlmd'</code> <code>artifact_name</code> <code>Optional[str]</code> <p>Artifacts for particular artifact name.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _artifact_list function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.pipeline_list","title":"<code>pipeline_list(file_name='./mlmd')</code>","text":"<p>Display a list of pipeline name(s) from the available input metadata file.</p> <pre><code>result = _pipeline_list(\"./mlmd_directory\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Specify input metadata file name.</p> <code>'./mlmd'</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _pipeline_list function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.execution_list","title":"<code>execution_list(pipeline_name, file_name='./mlmd', execution_uuid=None)</code>","text":"<p>Displays executions from the input metadata file with a few properties in a 7-column table, limited to 20 records per page.</p> <pre><code>result = _execution_list(\"example_pipeline\", \"./mlmd_directory\", \"example_execution_uuid\") \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline. </p> required <code>file_name</code> <code>str</code> <p>Specify input metadata file name.</p> <code>'./mlmd'</code> <code>execution_uuid</code> <code>Optional[str]</code> <p>Specify the execution uuid to retrieve execution.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _execution_list function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.repo_push","title":"<code>repo_push(pipeline_name, filepath='./mlmd', tensorboard_path=None, execution_uuid=None, jobs=32)</code>","text":"<p>Push artifacts, metadata files, and source code to the user's artifact repository, cmf-server, and git respectively.</p> <pre><code>result = _repo_push(\"example_pipeline\", \"./mlmd_directory\", \"example_execution_uuid\", \"./tensorboard_path\", 32) \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline. </p> required <code>filepath</code> <code>str</code> <p>Specify input metadata file path.</p> <code>'./mlmd'</code> <code>execution_uuid</code> <code>Optional[str]</code> <p>Specify execution uuid.</p> <code>None</code> <code>tensorboard_path</code> <code>Optional[str]</code> <p>Path to tensorboard logs.</p> <code>None</code> <code>jobs</code> <code>int</code> <p>Number of jobs to use for pushing artifacts.</p> <code>32</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _repo_push function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.repo_pull","title":"<code>repo_pull(pipeline_name, file_name='./mlmd', execution_uuid=None)</code>","text":"<p>Pull artifacts, metadata files, and source code from the user's artifact repository, cmf-server, and git respectively.</p> Example <pre><code>result = repo_pull(\"example_pipeline\", \"./mlmd_directory\", \"example_execution_uuid\") \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline.</p> required <code>file_name</code> <code>str</code> <p>Specify output metadata file name (default: \"./mlmd\").</p> <code>'./mlmd'</code> <code>execution_uuid</code> <code>Optional[str]</code> <p>Specify execution uuid (optional).</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _repo_pull function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.dvc_ingest","title":"<code>dvc_ingest(file_name='./mlmd')</code>","text":"<p>Ingests metadata from the dvc.lock file into the CMF.      If an existing MLMD file is provided, it merges and updates execution metadata      based on matching commands, or creates new executions if none exist.</p> <pre><code>result = _dvc_ingest(\"./mlmd_directory\") \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Specify input metadata file name.</p> <code>'./mlmd'</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _dvc_ingest function.</p>"},{"location":"api/public/cmf_ray_logger/","title":"cmflib.cmf_ray_logger.CmfRayLogger","text":""},{"location":"api/public/cmf_ray_logger/#cmfraylogger-user-guide","title":"CmfRayLogger User Guide","text":""},{"location":"api/public/cmf_ray_logger/#overview","title":"Overview","text":"<p>The <code>CmfRayLogger</code> class is designed to log Ray Tune metrics for the CMF (Common Metadata Framework). It tracks the performance and outputs of trials during the tuning process, directly linking metrics to stages of your CMF pipeline.</p>"},{"location":"api/public/cmf_ray_logger/#requirements","title":"Requirements","text":"<ul> <li>Ensure both <code>cmf</code> and <code>raytune</code> are installed on your system to use the <code>CmfRayLogger</code>.</li> </ul>"},{"location":"api/public/cmf_ray_logger/#installation","title":"Installation","text":"<p>To use <code>CmfRayLogger</code>, import it in your Python script:</p> <pre><code>from cmflib import cmf_ray_logger\n</code></pre>"},{"location":"api/public/cmf_ray_logger/#usage","title":"Usage","text":""},{"location":"api/public/cmf_ray_logger/#initialization","title":"Initialization","text":"<p>Create an instance of CmfRayLogger by providing the following parameters:</p> <ul> <li>pipeline_name: A string representing the name of the CMF pipeline.</li> <li>file_path: The file path to the metadata file associated with the CMF pipeline.</li> <li>pipeline_stage: The name of the current stage of the CMF pipeline.</li> <li>data_dir (optional): A directory path where trial data should be logged. If the path is within the CMF directory, it should be relative. If it is outside, it must be an absolute path. Default vale is <code>None</code>.</li> </ul> <p>Example of instantiation: <pre><code>logger = cmf_ray_logger.CmfRayLogger(pipeline_name, file_path, pipeline_stage. data_dir)\n</code></pre> Here, the <code>data_dir</code> argument is used to log the dataset at the start of each trial. Ensure that this path is relative if within the CMF directory and absolute if external to the CMF directory.</p>"},{"location":"api/public/cmf_ray_logger/#integration-with-ray-tune","title":"Integration with Ray Tune","text":"<p>After initializing the logger, it should be passed to Ray Tune\u2019s <code>tune.run</code> method via the <code>callbacks</code> parameter. This setup allows <code>CmfRayLogger</code> to log metrics for each trial based on the pipeline configuration and trial execution details.</p> <pre><code>from ray import tune\n\n# Example configuration for Ray Tune\nconfig = {\n    # Your configuration details\n}\n\ntune.run(\n    &lt;your_trainable&gt;,\n    config=config,\n    callbacks=[logger]\n)\n</code></pre>"},{"location":"api/public/cmf_ray_logger/#model-logging","title":"Model Logging","text":"<p><code>CmfRayLogger</code> can now log the model during trials. To enable this, the <code>train.report</code> method must include a special key: <code>\"model_path\"</code>. The value of <code>\"model_path\"</code> should be a relative path pointing to the saved model within the CMF directory.</p> <p>Important: Ensure that the <code>\"model_path\"</code> is relative, as the DVC wrapper expects all paths nested within the CMF directory to be relative. <pre><code>train.report({\n    \"accuracy\": 0.95,\n    \"loss\": 0.05,\n    \"model_path\": \"models/example_model.pth\"\n})\n</code></pre></p>"},{"location":"api/public/cmf_ray_logger/#output","title":"Output","text":"<p>During each trial, <code>CmfRayLogger</code> will automatically create a CMF object with attributes set as <code>pipeline_name</code>, <code>pipeline_stage</code>, and the CMF execution as <code>trial_id</code>. It captures the trial's output and logs it under the metric key <code>'Output'</code>. Additionally, it logs the dataset at the start of each trial (if data_dir is specified) and logs the model based on the <code>\"model_path\"</code> key in <code>train.report</code>.</p>"},{"location":"api/public/cmf_ray_logger/#example","title":"Example","text":"<p>Here is a complete example of how to use <code>CmfRayLogger</code> with Ray Tune:</p> <pre><code>from cmflib import cmf_ray_logger\nfrom ray import tune\n\n# Initialize the logger\nlogger = cmf_ray_logger.CmfRayLogger(\"ExamplePipeline\", \"/path/to/metadata.json\", \"Stage1\", \"path/to/data_dir\")\n\n# Configuration for tuning\nconfig = {\n    # Configuration details\n}\n\n# Execute the tuning process\ntune.run(\n    &lt;your_trainable&gt;,\n    config=config,\n    callbacks=[logger]\n)\n\n# Reporting within your trainable function\ntrain.report({\n    \"accuracy\": 0.95,\n    \"loss\": 0.05,\n    \"model_path\": \"path/to/models/example_model.pth\"\n})\n</code></pre>"},{"location":"api/public/cmfquery/","title":"cmflib.cmfquery.CmfQuery","text":""},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery","title":"<code>cmflib.cmfquery.CmfQuery(filepath='mlmd', is_server=False)</code>","text":"<p>               Bases: <code>object</code></p> <p>CMF Query communicates with the MLMD database and implements basic search and retrieval functionality.</p> <p>This class has been designed to work with the CMF framework. CMF alters names of pipelines, stages and artifacts in various ways. This means that actual names in the MLMD database will be different from those originally provided by users via CMF API. When methods in this class accept <code>name</code> parameters, it is expected that values of these parameters are fully-qualified names of respective entities.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the MLMD database file.</p> <code>'mlmd'</code>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_pipeline_names","title":"<code>get_pipeline_names()</code>","text":"<p>Return names of all pipelines.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of all pipeline names.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_pipeline_id","title":"<code>get_pipeline_id(pipeline_name)</code>","text":"<p>Return pipeline identifier for the pipeline names <code>pipeline_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Pipeline identifier or -1 if one does not exist.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_pipeline_stages","title":"<code>get_pipeline_stages(pipeline_name)</code>","text":"<p>Return list of pipeline stages for the pipeline with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline for which stages need to be returned. In CMF, there are no different pipelines with the same name.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of stage names associated with the given pipeline.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_exe_in_stage","title":"<code>get_all_exe_in_stage(stage_name)</code>","text":"<p>Return list of all executions for the stage with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Name of the stage. Before stages are recorded in MLMD, they are modified (e.g., pipeline name         will become part of the stage name). So stage names from different pipelines will not collide.</p> required <p>Returns:</p> Type Description <code>List[Execution]</code> <p>List of executions for the given stage.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_executions_by_ids_list","title":"<code>get_all_executions_by_ids_list(exe_ids)</code>","text":"<p>Return executions for given execution ids list as a pandas data frame.</p> <p>Parameters:</p> Name Type Description Default <code>exe_ids</code> <code>List[int]</code> <p>List of execution identifiers.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with all executions for the list of given execution identifiers.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_artifacts_by_context","title":"<code>get_all_artifacts_by_context(pipeline_name)</code>","text":"<p>Return artifacts for given pipeline name as a pandas data frame.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with all artifacts associated with given pipeline name.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_artifacts_by_ids_list","title":"<code>get_all_artifacts_by_ids_list(artifact_ids)</code>","text":"<p>Return all artifacts for the given artifact ids list.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_ids</code> <code>List[int]</code> <p>List of artifact identifiers</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with all artifacts for the given artifact ids list.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_executions_in_stage","title":"<code>get_all_executions_in_stage(stage_name)</code>","text":"<p>Return executions of the given stage as pandas data frame.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage name. See doc strings for the prev method.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with all executions associated with the given stage.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_artifact_df","title":"<code>get_artifact_df(artifact, d=None)</code>","text":"<p>Return artifact's data frame representation.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>MLMD entity representing artifact.</p> required <code>d</code> <code>Optional[Dict]</code> <p>Optional initial content for data frame.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A data frame with the single row containing attributes of this artifact.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_artifacts","title":"<code>get_all_artifacts()</code>","text":"<p>Return names of all artifacts.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of all artifact names.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_artifact","title":"<code>get_artifact(name)</code>","text":"<p>Return artifact's data frame representation using artifact name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Artifact name.</p> required <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>Pandas data frame with one row containing attributes of this artifact.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_artifacts_for_execution","title":"<code>get_all_artifacts_for_execution(execution_id)</code>","text":"<p>Return input and output artifacts for the given execution.</p> <p>Parameters:</p> Name Type Description Default <code>execution_id</code> <code>int</code> <p>Execution identifier.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame containing input and output artifacts for the given execution, one artifact per row.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_artifact_types","title":"<code>get_all_artifact_types()</code>","text":"<p>Return names of all artifact types.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of all artifact types.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_executions_for_artifact","title":"<code>get_all_executions_for_artifact(artifact_name)</code>","text":"<p>Return executions that consumed and produced given artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>Artifact name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pandas data frame containing stage executions, one execution per row.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_one_hop_child_artifacts","title":"<code>get_one_hop_child_artifacts(artifact_name, pipeline_id=None)</code>","text":"<p>Get artifacts produced by executions that consume given artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>Name of an artifact.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Output artifacts of all executions that consumed given artifact.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_child_artifacts","title":"<code>get_all_child_artifacts(artifact_name)</code>","text":"<p>Return all downstream artifacts starting from the given artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>Artifact name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame containing all child artifacts.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_one_hop_parent_artifacts","title":"<code>get_one_hop_parent_artifacts(artifact_name)</code>","text":"<p>Return input artifacts for the execution that produced the given artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>Artifact name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame containing immediate parent artifact of given artifact.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_parent_artifacts","title":"<code>get_all_parent_artifacts(artifact_name)</code>","text":"<p>Return all upstream artifacts.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>Artifact name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame containing all parent artifacts.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_parent_executions","title":"<code>get_all_parent_executions(artifact_name)</code>","text":"<p>Return all executions that produced upstream artifacts for the given artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>Artifact name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame containing all parent executions.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_metrics","title":"<code>get_metrics(metrics_name)</code>","text":"<p>Return metric data frame.</p> <p>Parameters:</p> Name Type Description Default <code>metrics_name</code> <code>str</code> <p>Metrics name.</p> required <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>Data frame containing all metrics.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.dumptojson","title":"<code>dumptojson(pipeline_name, exec_uuid=None)</code>","text":"<p>Return JSON-parsable string containing details about the given pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of an AI pipelines.</p> required <code>exec_uuid</code> <code>Optional[str]</code> <p>Optional stage execution_uuid - filter stages by this execution_uuid.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Pipeline in JSON format.</p>"},{"location":"api/public/cmfserver/","title":"CMF Server API Reference","text":"<p>CMF Server is a key interface for users to explore and track their ML training runs by storing metadata files on the CMF Server. Users can retrieve the saved metadata files and view their content using the UI provided by the CMF Server.</p> <p>For CMF Server installation and setup instructions, see the Installation &amp; Setup guide.</p>"},{"location":"api/public/cmfserver/#api-reference","title":"API Reference","text":"<p>CMF Server APIs are organized around FastAPI. They accept and return JSON-encoded request bodies and responses and return standard HTTP response codes.</p>"},{"location":"api/public/cmfserver/#list-of-apis","title":"List of APIs","text":"Method URL Description <code>POST</code> <code>/mlmd_push</code> Pushes JSON-encoded data to the CMF Server. <code>GET</code> <code>/mlmd_pull/{pipeline_name}</code> Retrieves an MLMD file from the CMF Server. <code>GET</code> <code>/executions/{pipeline_name}</code> Retrieves all executions from the CMF Server. <code>GET</code> <code>/list-of-executions/{pipeline_name}</code> Retrieves a list of execution types. <code>GET</code> <code>/execution-lineage/tangled-tree/{uuid}/{pipeline_name}</code> Retrieves a dictionary of nodes and links for a given execution type. <code>GET</code> <code>/artifacts/{pipeline_name}/{type}</code> Retrieves all artifacts of the specified type from the CMF Server. <code>GET</code> <code>/artifact-lineage/tangled-tree/{pipeline_name}</code> Retrieves a nested list of dictionaries with <code>id</code> and <code>parents</code> keys for artifacts. <code>GET</code> <code>/artifact_types</code> Retrieves a list of artifact types. <code>GET</code> <code>/pipelines</code> Retrieves all pipelines present in the MLMD file. <code>POST</code> <code>/tensorboard</code> Uploads TensorBoard logs to the CMF Server. <code>GET</code> <code>/model-card</code> Retrieves model data, input/output artifacts, and executions for a model. <code>GET</code> <code>/artifact-execution-lineage/tangled-tree/{pipeline_name}</code> Retrieves a nested list of dictionaries with <code>id</code> and <code>parents</code> keys for artifacts and executions. <code>POST</code> <code>/python-env</code> Pushes Python environment data to the CMF Server. <code>GET</code> <code>/python-env</code> Retrieves environment data from the <code>/cmf_server/data/env</code> folder."},{"location":"api/public/cmfserver/#http-response-status-codes","title":"HTTP Response Status Codes","text":"Code Title Description <code>200</code> <code>OK</code> MLMD is successfully pushed (e.g., when using <code>GET</code>, <code>POST</code>). <code>400</code> <code>Bad Request</code> When the CMF Server is not available. <code>404</code> <code>Not Found</code> Requested resource not found (e.g., pipeline, database, file, or registered server). <code>406</code> <code>Not Acceptable</code> Pipeline not found in the database. <code>422</code> <code>Unprocessable Entity</code> Version update required. The metadata schema version is incompatible. <code>500</code> <code>Internal Server Error</code> Server error occurred (e.g., target server unreachable, file read error, sync failure)."},{"location":"api/public/dataslice/","title":"cmflib.cmf.Cmf.DataSlice","text":""},{"location":"api/public/dataslice/#cmflib.cmf.Cmf.DataSlice","title":"<code>cmflib.cmf.Cmf.DataSlice(name, writer)</code>","text":"<p>A data slice represents a named subset of data. It can be used to track performance of an ML model on different slices of the training or testing dataset splits. This can be useful from different perspectives, for instance, to mitigate model bias.</p> <p>Instances of data slices are not meant to be created manually by users. Instead, use Cmf.create_dataslice method.</p>"},{"location":"api/public/dataslice/#cmflib.cmf.Cmf.DataSlice.add_data","title":"<code>add_data(path, custom_properties=None)</code>","text":"<p>Add data to create the dataslice. Currently supported only for file abstractions. Pre-condition - the parent folder, containing the file should already be versioned.</p> <pre><code>dataslice.add_data(f\"data/raw_data/{j}.xml)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Name to identify the file to be added to the dataslice.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Properties associated with this datum.</p> <code>None</code>"},{"location":"api/public/dataslice/#cmflib.cmf.Cmf.DataSlice.commit","title":"<code>commit(custom_properties=None)</code>","text":"<p>Commit the dataslice. The created dataslice is versioned and added to underneath data versioning software.</p> <pre><code>dataslice.commit()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>custom_properties</code> <code>Optional[Dict]</code> <p>Dictionary to store key value pairs associated with Dataslice</p> <code>None</code> <p>Example {\"mean\":2.5, \"median\":2.6}</p>"},{"location":"architecture/","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>Interactions in data pipelines can be complex. The different stages in the pipeline, (which may not be next to each other) may have to interact to produce or transform artifacts. </p> <p>As the artifacts navigate and undergo transformations through this pipeline, they can take a complicated path, which might also involve bidirectional movement across these stages.  Also there could be dependencies between the multiple stages, where the metrics produced by a stage could influence the metrics at a subsequent stage.  </p> <p>It is important to track the metadata across a pipeline to provide features like, lineage tracking, provenance and reproducibility.  </p> <p>The tracking of metadata through these complex pipelines have multiple challenges, some of them being,  </p> <ul> <li>Each stage in the pipeline could be executed in a different datacenter or an edge site having intermittent connection to the core datacenter.   </li> <li>Each stage in the pipeline could be possibly managed by different teams.  </li> <li>The artifacts (input or output) need to be uniquely identified across different sites and across multiple pipelines. </li> </ul> <p>Common Metadata Framework (CMF) addresses the problems associated with tracking of pipeline metadata from distributed sites and tracks code, data and metadata together for end-to-end traceability.   </p> <p>The framework automatically tracks the code version as one of the metadata for an execution. Additionally the data artifacts are also versioned automatically using a data versioning framework (like DVC) and the metadata regarding the data version is stored along with the code. </p> <p>The framework stores the Git commit ID of the metadata file associated with the artifact and content hash of the artifact as metadata. The framework provides APIs to track the hyperparameters and other metadata of pipelines.  Therefore from the metadata stored, users can identify the hyperparameters, code version and the artifact version used for the experiment. </p> <p>Identifying the artifacts by content hash allows the framework, to uniquely identify an artifact anywhere in the distributed sites. This enables the metadata from the distributed sites to be precisely merged to a central repository, thereby providing a single global metadata from the distributed sites.   </p> <p>On this backbone, we build the Git-like experience for metadata, enabling users to push their local metadata to the remote repository, where it is merged to create the global metadata and pull metadata from the global metadata to the local, to create a local view, which would contain only the metadata of interest. </p> <p>The framework can be used to track various types of pipelines such as data pipelines or AI pipelines. </p> <p> </p>"},{"location":"architecture/advantages/","title":"Advantages","text":"<ol> <li>Enables tracking of metadata for distributed pipelines, thereby facilitating efficient pipeline orchestration and management.</li> <li>Provides unified tracking of code, data, and metadata within a single framework.</li> <li>Offers Git-like ease of management for metadata versioning and control.</li> <li>Facilitates collaboration across teams with shared metadata standards.</li> </ol>"},{"location":"architecture/components/","title":"Components","text":""},{"location":"architecture/components/#cmflib","title":"CMFLib","text":"<p>The APIs and abstractions provided by CMFLib enable tracking of pipeline metadata. </p> <p>CMFLib tracks the stages in the pipeline, the input and output artifacts at each stage, and metrics. </p> <p>The framework allows metrics to be tracked at both coarse and fine-grained intervals. Stage metrics can be captured at the end of a stage, while fine-grained metrics can be tracked per step (epoch) or at regular intervals during the execution of the stage. </p> <p>The metadata logged through the APIs is written to a backend relational database. <code>CMFLib</code> also provides APIs to query the metadata stored in the relational database, allowing users to inspect pipelines.   </p> <p>In addition to explicit tracking through the APIs, <code>CMFLib</code> provides implicit tracking. This automatically tracks the software version used in the pipelines.  </p> <p>All artifacts are versioned using a data versioning framework (e.g., DVC). The content hash of the artifacts is generated and stored along with the user-provided metadata. A special artifact metadata file called a \".dvc\" file is created for every artifact (file or folder) that is added to the data version management system. The .dvc file contains the content hash of the artifact.</p> <p>For every pipeline, the metadata tracker creates a new branch to track the code. </p> <p>The special metadata file created for artifacts, \u201c.dvc\u201d file is too committed to Git, and its commit ID is tracked as a metadata information. Whenever there is a change in the artifact, the metadata file is modified to reflect its current content hash, and the file is tracked as a new version of the metadata file.  </p> <p>The metadata tracker automatically tracks the start commit when CMFLib was initialized and creates a separate commit for each change in the artifact during the experiment. This helps to track the transformations of the artifacts across the different stages in the pipeline. </p>"},{"location":"architecture/components/#cmf-client","title":"CMF Client","text":"<p>The CMF Client interacts with the CMF Server for metadata synchronization.  </p> <p>After the experiment is completed, the user invokes the <code>cmf push</code> command to push the collected metadata to the CMF Server. This transfers the existing metadata journal to the server.  </p> <p>The metadata from the CMF Server can be pulled to the local repository using either artifacts or the project as the identifier, or both.</p> <p>When an artifact is used as the identifier, all metadata associated with the artifacts currently present in the branch of the cloned Git repository is pulled from the central repository to the local repository. The pulled metadata consists of not only the immediate metadata associated with the artifacts, but also the metadata of all the artifacts in its chain of lineage. </p> <p>When a project is used as the identifier, all the metadata associated with the current branch of the pipeline code that is checked out is pulled to the local repository. </p>"},{"location":"architecture/components/#cmf-server","title":"CMF Server","text":"<p>The CMF Server exposes REST APIs that can be called from remote CMF Clients. </p> <p>In deployments with robust connectivity between the core datacenter and remote clients, the CMF Client can call the APIs exposed by the CMF Server to log metadata directly to the central metadata repository.</p> <p>In scenarios where connectivity with the central server is intermittent, remote clients log the metadata to the local repository. The journaled metadata is then pushed by the remote client to the CMF Server. The central server replays the journal and merges the incoming metadata with the metadata already existing in the central repository. The ability to accurately identify artifacts anywhere using their content hash makes this merge operation robust and reliable.</p>"},{"location":"architecture/components/#central-repositories","title":"Central Repositories","text":"<p>The Common Metadata Framework (CMF) consists of three central repositories for code, data, and metadata. </p>"},{"location":"architecture/components/#central-metadata-repository","title":"Central Metadata Repository","text":"<p>The central metadata repository holds the metadata pushed from distributed sites. It stores metadata about all the different pipelines that were tracked using the common metadata tracker. The consolidated view of the metadata stored in the central repository helps users learn across various stages of the pipeline executed at different locations. Using the query layer that points to the central repository, users get a global view of the metadata, which provides them with a deeper understanding of the pipelines and their metadata. The metadata helps to understand non-obvious results such as the performance of a dataset with respect to other datasets, or the performance of a particular pipeline with respect to other pipelines. </p>"},{"location":"architecture/components/#central-artifact-storage-repository","title":"Central Artifact Storage Repository","text":"<p>The central artifact storage repository stores all the artifacts related to experiments. The data versioning framework (DVC) stores the artifacts in a content-addressable layout. The artifacts are stored inside folders named with the first two characters of the content hash, and the artifact filename is the remaining part of the content hash. This layout enables efficient retrieval of artifacts.   </p>"},{"location":"architecture/components/#git-repository","title":"Git Repository","text":"<p>The Git repository is used to track the code. Along with the code, the metadata files of the artifacts (which contain the content hash of the artifacts) are also stored in Git. The data versioning framework (DVC) uses these files to retrieve the artifacts from the artifact storage repository. </p>"},{"location":"cmf_client/","title":"\ud83d\udcbb CLI Reference","text":""},{"location":"cmf_client/#quick-start-with-cmf-client","title":"Quick start with CMF Client","text":"<p>Common Metadata Framework (<code>CMF</code>) has the following components:</p> <ul> <li>CMFLib: A Python library that captures and tracks metadata throughout your ML pipeline, including datasets, models, and metrics. It provides APIs for both logging metadata during execution and querying it later for analysis.</li> <li>CMF Client: A command-line tool that synchronizes metadata with the <code>CMF Server</code>, manages artifact transfers to and from storage repositories, and integrates with Git for version control.</li> <li>CMF Server with GUI: A centralized server that aggregates metadata from multiple clients and provides a web-based graphical interface for visualizing pipeline executions, artifacts, and lineage relationships, enabling teams to collaborate effectively.</li> <li>Central Artifact Repositories: Storage backends (such as AWS S3, MinIO, or SSH-based storage) that host your datasets, models, and other pipeline artifacts.</li> </ul> <p>This tutorial walks you through the process of setting up the <code>CMF Client</code>.</p>"},{"location":"cmf_client/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the setup, ensure the following components are up and running:</p> <ul> <li>CMFLib</li> <li>CMF Server</li> </ul> <p>Make sure there are no errors during their startup, as <code>CMF Client</code> depends on both of these components.</p>"},{"location":"cmf_client/#setup-a-cmf-client","title":"Setup a <code>CMF Client</code>","text":"<p><code>CMF Client</code> is a command-line tool that facilitates metadata collaboration between different teams or two team members. It allows users to pull or push metadata from or to the <code>CMF Server</code>.</p> <p>Follow the below-mentioned steps for the end-to-end setup of <code>CMF Client</code>:-</p> <p>Configuration</p> <ol> <li>Create working directory <code>mkdir &lt;workdir&gt;</code></li> <li>Execute <code>cmf init</code> to configure the Data Version Control (DVC) remote directory, Git remote URL, CMF server, and Neo4j. Follow the <code>cmf init</code> for more details.</li> </ol>"},{"location":"cmf_client/#how-to-effectively-use-cmf-client","title":"How to effectively use CMF Client?","text":"<p>Let's assume we are tracking the metadata for a pipeline named <code>Test-env</code> with a MinIO S3 bucket as the artifact repository and a CMF Server.</p> <p>Create a folder <pre><code>mkdir example-folder\n</code></pre></p>"},{"location":"cmf_client/#initialize-cmf","title":"Initialize cmf","text":"<p>CMF initialization is the first and foremost step to use CMF Client commands. This command completes the initialization process in one step, making the CMF Client user-friendly. Execute <code>cmf init</code> in the <code>example-folder</code> directory created in the above step. <pre><code>cmf init minioS3 --url s3://dvc-art\n                 --endpoint-url http://x.x.x.x:9000\n                 --access-key-id minioadmin\n                 --secret-key minioadmin\n                 --git-remote-url https://github.com/user/experiment-repo.git\n                 --cmf-server-url http://x.x.x.x:80\n                 --neo4j-user neo4j\n                 --neo4j-password password\n                 --neo4j-uri bolt://localhost:7687\n</code></pre></p> <p>Here, \"dvc-art\" is provided as an example bucket name. However, users can change it as needed. If the user chooses to change it, they will need to update the Dockerfile for minioS3 accordingly.</p> <p>Check cmf init minioS3 for more details.</p> <p>Check status of CMF initialization (Optional) <pre><code>cmf init show\n</code></pre> Check cmf init show for more details.</p> <p>Track metadata using CMFLib</p> <p>Use Sample projects as a reference to create a new project to track metadata for ML pipelines.</p> <p>More information is available inside Getting Started Tutorial.</p> <p>Before pushing artifacts or metadata, ensure that the CMF Server and minioS3 are up and running.</p> <p>Push artifacts</p> <p>Push artifacts in the artifact repository initialized in the Initialize cmf step. <pre><code>cmf artifact push -p 'Test-env'\n</code></pre> Check cmf artifact push for more details.</p> <p>Push metadata to CMF Server <pre><code>cmf metadata push -p 'Test-env'\n</code></pre> Check cmf metadata push for more details.</p>"},{"location":"cmf_client/#cmf-client-with-collaborative-development","title":"CMF Client with collaborative development","text":"<p>In the case of collaborative development, in addition to the above commands, users can follow the commands below to pull metadata and artifacts from a common CMF Server and a central artifact repository.</p> <p>Pull metadata from the server</p> <p>Execute <code>cmf metadata pull</code> command in the <code>example_folder</code>. <pre><code>cmf metadata pull -p 'Test-env'\n</code></pre> Check cmf metadata pull for more details.</p> <p>Pull artifacts from the central artifact repository</p> <p>Execute <code>cmf artifact pull</code> command in the <code>example_folder</code>. <pre><code>cmf artifact pull -p 'Test-env'\n</code></pre> Check cmf artifact pull page for more details.</p>"},{"location":"cmf_client/#flow-chart-for-cmf","title":"Flow Chart for cmf","text":"<p> <code>CMF Client</code> is a command-line tool that facilitates metadata collaboration between different teams or two team members. It allows users to pull/push metadata from or to the <code>CMF Server</code> with similar functionalities for artifact repositories and other commands.</p>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/","title":"Getting started with cmf","text":"<p>Common Metadata Framework (CMF) has the following components:</p> <ul> <li>Metadata Library exposes APIs to track pipeline metadata. It also provides APIs to query the stored metadata.</li> <li>CMF Client interacts with the CMF Server to pull or push metadata.</li> <li>CMF Server with GUI interacts with remote CMF Clients and merges the metadata transferred by each   client. This server also provides a GUI that can render the stored metadata.</li> <li>Central Artifact Repositories host the code and data.</li> </ul>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/#setup-a-cmf-client","title":"Setup a CMF Client","text":"<p><code>CMF Client</code> is a tool that facilitates metadata collaboration between different teams and team members. These clients interact with the CMF Server to push/pull metadata.</p> <p>Pre-Requisites</p> <ul> <li>Python 3.9+</li> <li>Git latest version</li> </ul> <p>Install cmf library i.e. cmflib <pre><code>pip install https://github.com/HewlettPackard/cmf\n</code></pre> OR <pre><code>pip install cmflib\n</code></pre> Documentation for more details.</p>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/#install-cmf-server","title":"Install CMF Server","text":"<p>CMF Server is the primary interface for the user to explore and track their ML training runs by browsing the stored metadata. Users can retrieve the saved metadata file and can view the content of the saved metadata file using the UI provided by the CMF Server.</p> <p>Details on how to set up a CMF Server can be found here.</p>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/#simple-example-of-using-the-cmf-client","title":"Simple Example of using the CMF Client","text":"<p>In this example, CMF is used to track the metadata for a pipeline named <code>Test-env</code> which interacts with a MinIO</p> <p>S3 bucket as the artifact repository and a CMF Server.</p> <p>Setup the example directory <pre><code>mkdir example-folder &amp;&amp; cd example-folder\n</code></pre></p>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/#initialize-cmf","title":"Initialize cmf","text":"<p>CMF must be initialized to use CMF Client commands. The following command configures authentication to an S3 bucket and specifies the connection to a CMF server. <pre><code>cmf init minioS3 --url s3://bucket-name --endpoint-url http://localhost:9000 \\\n  --access-key-id minioadmin --secret-key minioadmin --git-remote-url https://github.com/user/experiment-repo.git \\\n  --cmf-server-url http://x.x.x.x:80  --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://X.X.X.X:7687\n</code></pre> Check here for more details.</p> <p>Check status of CMF initialization (Optional) <pre><code>cmf init show\n</code></pre> Check here for more details.</p> <p>Track metadata using cmflib</p> <p>Use Sample projects as a reference to create a new project to track metadata for ML pipelines.</p> <p>More info is available here.</p> <p>Push artifacts</p> <p>Push artifacts in the artifact repository initialized in the Initialize cmf step. <pre><code>cmf artifact push\n</code></pre> Check here for more details.</p> <p>Push metadata to CMF Server <pre><code>cmf metadata push -p 'Test-env'\n</code></pre> Check here for more details.</p>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/#cmf-client-with-collaborative-development","title":"CMF Client with collaborative development","text":"<p>In the case of collaborative development, in addition to the above commands, users can follow the commands below to pull metadata and artifacts from a common CMF Server and a central artifact repository.</p> <p>Pull metadata from the server</p> <p>Execute <code>cmf metadata</code> command in the <code>example_folder</code>. <pre><code>cmf metadata pull -p 'Test-env'\n</code></pre> Check here for more details.</p> <p>Pull artifacts from the central artifact repository</p> <p>Execute <code>cmf artifact</code> command in the <code>example_folder</code>. <pre><code>cmf artifact pull -p \"Test-env\"\n</code></pre> Check here for more details.</p>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/#flow-chart-for-cmf","title":"Flow Chart for cmf","text":""},{"location":"cmf_client/cmf-dvc-ingest-guide/","title":"Guide to <code>cmf dvc ingest</code> Command","text":"<p>The <code>cmf dvc ingest</code> command is used to ingest metadata from the <code>dvc.lock</code> file into the <code>CMF Server</code>. If an existing MLMD (Metadata) file is provided, the command will merge and update execution metadata based on matching commands or create new executions if none exist.</p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#steps-to-get-started","title":"\ud83d\udccc Steps to Get Started","text":"<p>Follow the steps below to set up <code>cmf</code> client and use the <code>dvc ingest</code> command.</p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#1-navigate-to-your-project-directory","title":"1. Navigate to Your Project Directory","text":"<p>Open your terminal and go to the directory (for e.g., <code>example-get-started</code>) where you want to use <code>cmf</code> commands.</p> <pre><code>cd /path/to/your/project\n</code></pre>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#2-initialize-cmf-with-neo4j-credentials","title":"2. Initialize <code>cmf</code> with Neo4j Credentials","text":"<p>Use the following command to initialize <code>cmf</code>. You can choose from various storage options like <code>local</code>, <code>ssh</code>, <code>amazons3</code>, <code>osdfremote</code>, or <code>minios3</code>.</p> <pre><code>   cmf init local \\\n  --path /home/XXXX/local-storage \\\n  --git-remote-url https://github.com/user/experiment-repo.git \\\n  --cmf-server-url http://x.x.x.x:80 \\\n  --neo4j-user neo4j \\\n  --neo4j-password password \\\n  --neo4j-uri bolt://x.x.x.x:7687\n</code></pre> <p>\ud83d\udd01 Replace the following: - <code>x.x.x.x</code> with your IP address - <code>XXXX</code> with your system username - Provide your correct Neo4j username and password</p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#3-start-the-neo4j-server","title":"3. Start the Neo4j Server","text":"<p>Start the Neo4j server using Docker. Follow the guide provided below:</p> <p>\ud83d\udcc4 Start Neo4j with Docker </p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#4-create-a-dvcyaml-file","title":"4. Create a <code>dvc.yaml</code> File","text":"<p>Inside your project directory (for e.g., <code>example-get-started</code>), create a <code>dvc.yaml</code> file.</p> <p>Here\u2019s a sample <code>dvc.yaml</code>:</p> <pre><code>stages:\n  prepare:\n    cmd: python src/parse.py artifacts/data.xml.gz artifacts/parsed/\n    deps:\n      - artifacts/data.xml.gz\n    outs:\n      - artifacts/parsed/train.tsv\n      - artifacts/parsed/test.tsv\n\n  featurize:\n    cmd: python src/featurize.py artifacts/parsed/ artifacts/features/\n    deps:\n      - artifacts/parsed/train.tsv\n      - artifacts/parsed/test.tsv\n    outs:\n      - artifacts/features/train.pkl\n      - artifacts/features/test.pkl\n\n  train:\n    cmd: python src/train.py artifacts/features/ artifacts/model/\n    deps:\n      - artifacts/features/train.pkl\n      - artifacts/features/test.pkl\n    outs:\n      - artifacts/model/model.pkl\n\n  test:\n    cmd: python src/test.py artifacts/model/ artifacts/features/ artifacts/test_results/\n    deps:\n      - artifacts/model/model.pkl\n    outs:\n      - artifacts/test_results/prc.json\n      - artifacts/test_results/roc.json\n      - artifacts/test_results/scores.json\n</code></pre> <p>\u26a0\ufe0f Be Consistent with deps and outs: When defining deps and outs in your dvc.yaml, ensure consistency in the format used. Either define both as directories (e.g., artifacts/parsed/) or both as individual files (e.g., artifacts/parsed/train.tsv, artifacts/parsed/test.tsv).</p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#5-remove-cmf-code-from-src-directory","title":"5. Remove <code>cmf</code> code from <code>src</code> Directory","text":"<p>Ensure that your source files inside the <code>example-get-started/src</code> directory do not contain any <code>cmf</code>-related code. Keep them clean and focused on their tasks.</p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#6-run-the-dvc-pipeline","title":"6. Run the DVC Pipeline","text":"<p>Execute your pipeline using the following command. This will also generate a <code>dvc.lock</code> file.</p> <pre><code>dvc repro\n</code></pre>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#7-ingest-metadata-with-cmf","title":"7. Ingest Metadata with <code>cmf</code>","text":"<p>Run the following command to create metadata file based on your <code>dvc.lock</code> file:</p> <pre><code>cmf dvc ingest\n</code></pre> <p>\u26a0\ufe0f Troubleshooting: If you see an error like the one below, your Neo4j server might not be running properly.</p> <p></p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#8-pushpull-metadata-and-artifacts","title":"8. Push/Pull Metadata and Artifacts","text":"<p>Use CMF Client commands to push or pull your metadata and artifacts as required:</p> <ul> <li><code>cmf metadata push</code></li> <li><code>cmf metadata pull</code></li> <li><code>cmf artifact push</code></li> <li><code>cmf artifact pull</code></li> </ul>"},{"location":"cmf_client/cmf_client_commands/","title":"Getting started with CMF Client commands","text":""},{"location":"cmf_client/cmf_client_commands/#cmf","title":"cmf","text":"<pre><code>Usage: cmf [-h] {init, artifact, metadata, execution, pipeline, repo, dvc}\n</code></pre> <p>The <code>cmf</code> command is a comprehensive tool designed to initialize an artifact repository and perform various operations on artifacts, execution, pipeline and metadata.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-init","title":"cmf init","text":"<pre><code>Usage: cmf init [-h] {minioS3, amazonS3, local, sshremote, osdfremote, show}\n</code></pre> <p><code>cmf init</code> initializes an artifact repository for cmf. Local directory, Minio S3 bucket, Amazon S3 bucket, SSH Remote and Remote OSDF directory are the options available. Additionally, users can provide the CMF Server URL.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-init-show","title":"cmf init show","text":"<pre><code>Usage: cmf init show\n</code></pre> <p><code>cmf init show</code> displays current cmf configuration.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-init-minios3","title":"cmf init minioS3","text":"<pre><code>Usage: cmf init minioS3 [-h] --url [url]\n                             --endpoint-url [endpoint_url]\n                             --access-key-id [access_key_id]\n                             --secret-key [secret_key]\n                             --git-remote-url[git_remote_url]\n                             --cmf-server-url [cmf_server_url]\n                             --neo4j-user [neo4j_user]\n                             --neo4j-password [neo4j_password]\n                             --neo4j-uri [neo4j_uri]\n</code></pre> <p><code>cmf init minioS3</code> configures Minio S3 bucket as a cmf artifact repository. Refer minio-server.md to set up a minio server.</p> <pre><code>cmf init minioS3 --url s3://dvc-art --endpoint-url http://x.x.x.x:9000 --access-key-id minioadmin --secret-key minioadmin --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-url http://x.x.x.x:80 --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre> <p>Here, \"dvc-art\" is provided as an example bucket name. However, users can change it as needed, if the user chooses to change it, they will need to update the Dockerfile for MinIOS3 accordingly.</p> <p>Required Arguments</p> <pre><code>  --url [url]                           Specify MinioS3 bucket url.\n  --endpoint-url [endpoint_url]         Specify the endpoint url which is used to access Minio's locally/remotely running UI.\n  --access-key-id [access_key_id]       Specify Access Key Id.\n  --secret-key [secret_key]             Specify Secret Key.\n  --git-remote-url [git_remote_url]     Specify git repo url. eg: https://github.com/XXX/example.git\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                          show this help message and exit\n  --cmf-server-url [cmf_server_url]   Specify CMF Server URL. (default: http://127.0.0.1:80)\n  --neo4j-user [neo4j_user]           Specify neo4j user. (default: None)\n  --neo4j-password [neo4j_password]   Specify neo4j password. (default: None)\n  --neo4j-uri [neo4j_uri]             Specify neo4j uri. Eg bolt://localhost:7687 (default: None)\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-init-local","title":"cmf init local","text":"<pre><code>Usage: cmf init local [-h] --path [path] -\n                           --git-remote-url [git_remote_url]\n                           --cmf-server-url [cmf_server_url]\n                           --neo4j-user [neo4j_user]\n                           --neo4j-password [neo4j_password]\n                           --neo4j-uri [neo4j_uri]\n</code></pre> <p><code>cmf init local</code> initialises local directory as a cmf artifact repository.</p> <pre><code>cmf init local --path /home/XXXX/local-storage --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-url http://x.x.x.x:80 --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre> <p>Replace 'XXXX' with your system username in the following path: /home/XXXX/local-storage</p> <p>Required Arguments</p> <pre><code>  --path [path]                         Specify local directory path.\n  --git-remote-url [git_remote_url]     Specify git repo url. eg: https://github.com/XXX/example.git\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                          show this help message and exit\n  --cmf-server-url [cmf_server_url]   Specify CMF Server URL. (default: http://127.0.0.1:80)\n  --neo4j-user [neo4j_user]           Specify neo4j user. (default: None)\n  --neo4j-password [neo4j_password]   Specify neo4j password. (default: None)\n  --neo4j-uri [neo4j_uri]             Specify neo4j uri. Eg bolt://localhost:7687 (default: None)\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-init-amazons3","title":"cmf init amazonS3","text":"<p>Before setting up, obtain AWS temporary security credentials using the AWS Security Token Service (STS). These credentials are short-term and can last from minutes to hours. They are dynamically generated and provided to trusted users upon request, and expire after use. Users with appropriate permissions can request new credentials before or upon expiration. For further information, refer to the Temporary security credentials in IAM page.</p> <p>To retrieve temporary security credentials using multi-factor authentication (MFA) for an IAM user, you can use the below command.</p> <pre><code>aws sts get-session-token --duration-seconds &lt;duration&gt; --serial-number &lt;MFA_device_serial_number&gt; --token-code &lt;MFA_token_code&gt;\n</code></pre> <p>Required Arguments</p> <pre><code>  --serial-number                Specifies the serial number of the MFA device associated with the IAM user.\n  --token-code                   Specifies the one-time code generated by the MFA device.\n</code></pre> <p>Optional Arguments</p> <pre><code>  --duration-seconds             Specifies the duration for which the temporary credentials will be valid, in seconds.\n</code></pre> <p>Example</p> <pre><code>aws sts get-session-token --duration-seconds 3600 --serial-number arn:aws:iam::123456789012:mfa/user --token-code 123456\n</code></pre> <p>This will return output like</p> <pre><code>{\n    \"Credentials\": {\n        \"AccessKeyId\": \"ABCDEFGHIJKLMNO123456\",\n        \"SecretAccessKey\": \"PQRSTUVWXYZ789101112131415\",\n        \"SessionToken\": \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijlmnopqrstuvwxyz12345678910ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijlmnopqrstuvwxyz12345678910ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijlmnopqrstuvwxyz12345678910\",\n        \"Expiration\": \"2021-05-10T15:31:08+00:00\"\n    }\n}\n</code></pre> <p>Initialization of amazonS3</p> <pre><code>Usage: cmf init amazonS3 [-h] --url [url]\n                              --access-key-id [access_key_id]\n                              --secret-key [secret_key]\n                              --session-token [session_token]\n                              --git-remote-url [git_remote_url]\n                              --cmf-server-url [cmf_server_url]\n                              --neo4j-user [neo4j_user]\n                              --neo4j-password [neo4j_password]\n                              --neo4j-uri [neo4j_uri]\n</code></pre> <p><code>cmf init amazonS3</code> initialises Amazon S3 bucket as a CMF artifact repository.</p> <pre><code>cmf init amazonS3 --url s3://bucket-name --access-key-id XXXXXXXXXXXXX --secret-key XXXXXXXXXXXXX --session-token XXXXXXXXXXXXX --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-url http://x.x.x.x:80 --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre> <p>Here, use the --access-key-id, --secret-key and --session-token generated from the <code>aws sts</code> command which is mentioned above.</p> <p>The bucket-name must exist within Amazon S3 before executing the <code>cmf artifact push</code> command.</p> <p>Required Arguments</p> <pre><code>  --url [url]                           Specify Amazon S3 bucket url.\n  --access-key-id [access_key_id]       Specify Access Key Id.\n  --secret-key [secret_key]             Specify Secret Key.\n  --session-token                       Specify session token. (default: None)\n  --git-remote-url [git_remote_url]     Specify git repo url. eg: https://github.com/XXX/example.git\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                          show this help message and exit\n  --cmf-server-url [cmf_server_url]   Specify CMF Server URL. (default: http://127.0.0.1:80)\n  --neo4j-user [neo4j_user]           Specify neo4j user. (default: None)\n  --neo4j-password [neo4j_password]   Specify neo4j password. (default: None)\n  --neo4j-uri [neo4j_uri]             Specify neo4j uri. Eg bolt://localhost:7687 (default: None)\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-init-sshremote","title":"cmf init sshremote","text":"<pre><code>Usage: cmf init sshremote [-h] --path [path]\n                               --user [user]\n                               --port [port]\n                               --password [password]\n                               --git-remote-url [git_remote_url]\n                               --cmf-server-url [cmf_server_url]\n                               --neo4j-user [neo4j_user]\n                               --neo4j-password [neo4j_password]\n                               --neo4j-uri [neo4j_uri]\n</code></pre> <p><code>cmf init sshremote</code> command initialises remote ssh directory as a cmf artifact repository.</p> <pre><code>cmf init sshremote --path ssh://127.0.0.1/home/user/ssh-storage --user XXXXX --port 22 --password example@123 --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-url http://x.x.x.x:80 --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre> <p>Required Arguments</p> <pre><code>  --path [path]                           Specify remote ssh directory path.\n  --user [user]                           Specify username.\n  --port [port]                           Specify port.\n  --password [password]                   Specify password. This will be saved only on local.\n  --git-remote-url [git_remote_url]       Specify git repo url. eg: https://github.com/XXX/example.git\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                          show this help message and exit\n  --cmf-server-url [cmf_server_url]   Specify CMF Server URL. (default: http://127.0.0.1:80)\n  --neo4j-user [neo4j_user]           Specify neo4j user. (default: None)\n  --neo4j-password [neo4j_password]   Specify neo4j password. (default: None)\n  --neo4j-uri [neo4j_uri]             Specify neo4j uri. Eg bolt://localhost:7687 (default: None)\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-init-osdfremote","title":"cmf init osdfremote","text":"<pre><code>Usage: cmf init osdfremote [-h] --path [path]\n                             --cache [cache]\n                             --key-id [key_id]\n                             --key-path [key_path]\n                             --key-issuer [key_issuer]\n                             --git-remote-url[git_remote_url]\n                             --cmf-server-url [cmf_server_url]\n                             --neo4j-user [neo4j_user]\n                             --neo4j-password [neo4j_password]\n                             --neo4j-uri [neo4j_uri]\n</code></pre> <p><code>cmf init osdfremote</code> configures a OSDF Origin as a cmf artifact repository.</p> <pre><code>cmf init osdfremote --path https://[Some Origin]:8443/nrp/fdp/ --cache http://[Some Redirector] --key-id XXXX --key-path ~/.ssh/private.pem --key-issuer https://[Token Issuer] --git-remote-url https://github.com/user/experiment-repo.git --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-url http://127.0.0.1:80 --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre> <p>Required Arguments</p> <pre><code>  --path [path]                        Specify FQDN for OSDF origin including including port and directory path if any\n  --key-id [key_id]                    Specify key_id for provided private key. eg. b2d3\n  --key-path [key_path]                Specify path for private key on local filesystem. eg. ~/.ssh/XXX.pem\n  --key-issuer [key_issuer]            Specify URL for Key Issuer. eg. https://t.nationalresearchplatform.org/XXX\n  --git-remote-url [git_remote_url]    Specify git repo url. eg: https://github.com/XXX/example.git\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                          show this help message and exit\n  --cmf-server-url [cmf_server_url]   Specify CMF Server URL. (default: http://127.0.0.1:80)\n  --neo4j-user [neo4j_user]           Specify neo4j user. (default: None)\n  --neo4j-password [neo4j_password]   Specify neo4j password. (default: None)\n  --neo4j-uri [neo4j_uri]             Specify neo4j uri. Eg bolt://localhost:7687 (default: None)\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-artifact","title":"cmf artifact","text":"<pre><code>Usage: cmf artifact [-h] {pull, push, list}\n</code></pre> <p><code>cmf artifact</code> pull, push or list artifacts from or to the user configured artifact repository, respectively.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-artifact-pull","title":"cmf artifact pull","text":"<pre><code>Usage: cmf artifact pull [-h] -p [pipeline_name] -f [file_name] -a [artifact_name]\n</code></pre> <p><code>cmf artifact pull</code> command pull artifacts from the user configured repository to the user's local machine.</p> <pre><code>cmf artifact pull -p 'pipeline-name' -f '/path/to/mlmd-file-name' -a 'artifact-name'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline-name [pipeline_name]   Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                            show this help message and exit.\n  -a [artifact_name], --artifact_name [artifact_name]   Specify artifact name only (do not include folder path or absolute path).\n  -f [file_name], --file_name [file_name]               Specify input metadata file name.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-artifact-push","title":"cmf artifact push","text":"<pre><code>Usage: cmf artifact push [-h] -p [pipeline_name] -f [file_name] -j [jobs]\n</code></pre> <p><code>cmf artifact push</code> command push artifacts from the user's local machine to the user configured artifact repository.</p> <pre><code>cmf artifact push -p 'pipeline_name' -f '/path/to/mlmd-file-name' -j 'jobs'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline_name [pipeline_name]   Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                            show this help message and exit.\n  -f [file_name], --file-name [file_name]               Specify mlmd file name.\n  -j [jobs], --jobs [jobs]                              Number of parallel jobs for uploading artifacts to remote storage. Default is 4 * cpu_count().\n                                                        Increasing jobs may speed up uploads but will use more resources.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-artifact-list","title":"cmf artifact list","text":"<pre><code>Usage: cmf artifact list [-h] -p [pipeline_name] -f [file_name] -a [artifact_name]\n</code></pre> <p><code>cmf artifact list</code> command displays artifacts from the input metadata file with a few properties in a 7-column table, limited to 20 records per page.</p> <pre><code>cmf artifact list -p 'pipeline_name' -f '/path/to/mlmd-file-name' -a 'artifact_name'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline_name [pipeline_name]   Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                            show this help message and exit.\n  -f [file_name], --file_name [file_name]               Specify input metadata file name.\n  -a [artifact_name], --artifact_name [artifact_name]   Specify the artifact name to display detailed information about the given artifact name.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-metadata","title":"cmf metadata","text":"<pre><code>Usage: cmf metadata [-h] {pull, push, export}\n</code></pre> <p><code>cmf metadata</code> push, pull or export the metadata file to and from the CMF Server, respectively.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-metadata-pull","title":"cmf metadata pull","text":"<pre><code>Usage: cmf metadata pull [-h] -p [pipeline_name] -f [file_name]  -e [exec_uuid]\n</code></pre> <p><code>cmf metadata pull</code> command pulls the metadata file from the CMF Server to the user's local machine.</p> <pre><code>cmf metadata pull -p 'pipeline-name' -f '/path/to/mlmd-file-name' -e 'execution_uuid'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline_name [pipeline_name]     Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>-h, --help                                                show this help message and exit.\n-e [exec_uuid], --execution_uuid [exec_uuid]              Specify execution uuid.\n-f [file_name], --file_name [file_name]                   Specify output metadata file name.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-metadata-push","title":"cmf metadata push","text":"<pre><code>Usage: cmf metadata push [-h] -p [pipeline_name] -f [file_name] -e [exec_uuid] -t [tensorboard_path]\n</code></pre> <p><code>cmf metadata push</code> command pushes the metadata file from the local machine to the CMF Server.</p> <pre><code>cmf metadata push -p 'pipeline-name' -f '/path/to/mlmd-file-name' -e 'execution_uuid' -t '/path/to/tensorboard-log'\n</code></pre> <p>Required Arguments</p> <pre><code>-p [pipeline_name], --pipeline_name [pipeline_name]     Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                                            show this help message and exit.\n  -f [file_name], --file_name [file_name]                               Specify input metadata file name.\n  -e [exec_uuid], --execution_uuid [exec_uuid]                          Specify execution uuid.\n  -t [tensorboard_path], --tensorboard_path [tensorboard_path]          Specify path to tensorboard logs for the pipeline.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-metadata-export","title":"cmf metadata export","text":"<pre><code>Usage: cmf metadata export [-h] -p [pipeline_name] -j [json_file_name] -f [file_name]\n</code></pre> <p><code>cmf metadata export</code> export local metadata's metadata in json format to a json file.</p> <pre><code>cmf metadata export -p 'pipeline-name' -j '/path/to/json-file-name' -f '/path/to/mlmd-file-name'\n</code></pre> <p>Required Arguments</p> <pre><code>-p [pipeline_name], --pipeline_name [pipeline_name]        Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                               show this help message and exit.\n  -f [file_name], --file_name [file_name]                  Specify the absolute or relative path for the input metadata file.\n  -j [json_file_name], --json_file_name [json_file_name]   Specify output json file name with full path.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-execution","title":"cmf execution","text":"<pre><code>Usage: cmf execution [-h] {list}\n</code></pre> <p><code>cmf execution</code> command to displays executions from the metadata file.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-execution-list","title":"cmf execution list","text":"<pre><code>Usage: cmf execution list [-h] -p [pipeline_name] -f [file_name] -e [execution_uuid]\n</code></pre> <p><code>cmf execution list</code> command to displays executions from the input metadata file with a few properties in a 7-column table, limited to 20 records per page.</p> <pre><code>cmf execution list -p 'pipeline_name' -f '/path/to/mlmd-file-name' -e 'execution_uuid'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline_name [pipeline_name]       Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                                    show this help message and exit.\n  --f [file_name], --file_name [file_name]                      Specify input metadata file name.\n  -e [exec_uuid], --execution_uuid [exec_uuid]                  Specify the execution uuid to retrieve execution.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-pipeline","title":"cmf pipeline","text":"<pre><code>Usage: cmf pipeline [-h] {list}\n</code></pre> <p><code>cmf pipeline</code> command displays a list of pipeline name(s) from the available metadatafile.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-pipeline-list","title":"cmf pipeline list","text":"<pre><code>Usage: cmf pipeline list [-h] -f [file_name]\n</code></pre> <p><code>cmf pipeline list</code> command displays a list of pipeline name(s) from the available metadatafile.</p> <pre><code>cmf pipeline list -f '/path/to/mlmd-file-name'\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                            show this help message and exit.\n  --f [file_name], --file_name [file_name]              Specify input metadata file name.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-repo","title":"cmf repo","text":"<pre><code>Usage: cmf repo [-h] {push, pull}\n</code></pre> <p><code>cmf repo</code> command push and pull artifacts, metadata files, and source code to and from the user's artifact repository, CMF Server, and Git respectively.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-repo-push","title":"cmf repo push","text":"<pre><code>Usage: cmf repo push [-h] -p [pipeline_name] -f [file_name] -e [exec_uuid] -t [tensorboard] -j [jobs]\n</code></pre> <p><code>cmf repo push</code> command push artifacts, metadata files, and source code to the user's artifact repository, CMF Server, and Git respectively.</p> <pre><code>cmf repo push -p 'pipeline-name' -f '/path/to/mlmd-file-name' -e 'execution_uuid' -t 'tensorboard_log_path' -j 'jobs'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline_name [pipeline_name]            Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                                     show this help message and exit.\n  -f [file_name], --file-name [file_name]                        Specify mlmd file name.\n  -e [exec_uuid], --execution_uuid [exec_uuid]                   Specify execution uuid.\n  -t [tensorboard], --tensorboard [tensorboard]                  Specify path to tensorboard logs for the pipeline.\n  -j [jobs], --jobs [jobs]                                       Number of parallel jobs for uploading artifacts to remote storage. Default is 4 * cpu_count().\n                                                                 Increasing jobs may speed up uploads but will use more resources.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-repo-pull","title":"cmf repo pull","text":"<pre><code>Usage: cmf repo pull [-h] -p [pipeline_name] -f [file_name] -e [exec_uuid]\n</code></pre> <p><code>cmf repo pull</code> command pull artifacts, metadata files, and source code from the user's artifact repository, CMF Server, and Git respectively.</p> <pre><code>cmf repo pull -p 'pipeline-name' -f '/path/to/mlmd-file-name' -e 'execution_uuid'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline_name [pipeline_name]            Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                                     show this help message and exit.\n  -f [file_name], --file_name [file_name]                        Specify output metadata file name.\n  -e [exec_uuid], --execution_uuid [exec_uuid]                   Specify execution uuid.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-dvc","title":"cmf dvc","text":"<pre><code>Usage: cmf dvc [-h] {ingest}\n</code></pre> <p><code>cmf dvc</code> command ingests metadata from the dvc.lock file into CMF.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-dvc-ingest","title":"cmf dvc ingest","text":"<pre><code>Usage: cmf dvc ingest [-h] -f [file_name]\n</code></pre> <p><code>cmf dvc ingest</code> command ingests metadata from the dvc.lock file into the CMF. If an existing MLMD file is provided, it merges and updates execution metadata based on matching commands, or creates new executions if none exist.</p> <pre><code>cmf dvc ingest -f '/path/to/mlmd-file-name'\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                                     show this help message and exit.\n  -f [file_name], --file-name [file_name]                        Specify input mlmd file name. (default: mlmd)\n</code></pre>"},{"location":"cmf_client/cmf_osdf/","title":"OSDF Remote Artifact Setup","text":""},{"location":"cmf_client/cmf_osdf/#steps-to-set-up-an-osdf-remote-repo","title":"Steps to set up an OSDF Remote Repo","text":"<p>Backed by the Pelican Platform, the Open Science Data Federation (OSDF) is an OSG service hosting data origins and caches across the globe.</p> <p>OSDF facilitates the distributed nature of a national compute pool. CMF supports OSDF Origins and caches as remotes to push/pull data from/to. </p> <p>OSDF provides a distributed data federation that allows researchers to store and access data across multiple institutions and geographic locations. It uses tokens for authentication and supports caching for improved performance.</p> <p>Proceed with the following steps to set up an OSDF Remote Repository:</p> <ol> <li>Initialize the <code>project directory</code> with an OSDF remote.</li> <li> <p>Check whether cmf is initialized in your project directory with the following command.    <pre><code>cmf init show\n</code></pre>    If cmf is not initialized, the following message will appear on the screen.    <pre><code>'cmf' is not configured.\nExecute the 'cmf init' command.\n</code></pre></p> </li> <li> <p>Execute the following command to initialize the OSDF remote storage as a CMF artifact repository.    <pre><code>cmf init osdfremote --path https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test \\\n    --cache https://osdf-director.osg-htc.org/ \\\n    --key-id XXXX \\\n    --key-path ~/private_hpe.pem \\\n    --key-issuer https://t.nationalresearchplatform.org/fdp-hpe \\\n    --git-remote-url https://github.com/user/experiment-repo.git\n</code></pre></p> </li> </ol> <p>Parameters:    - <code>--path</code>: The OSDF origin server URL where data will be stored    - <code>--cache</code>: The OSDF cache/director URL for retrieving data    - <code>--key-id</code>: The key identifier for authentication    - <code>--key-path</code>: Path to the private key file for token generation    - <code>--key-issuer</code>: The issuer URL for token generation    - <code>--git-remote-url</code>: Git repository URL for version control</p> <ol> <li> <p>Execute <code>cmf init show</code> to check the CMF configuration.</p> </li> <li> <p>After initialization, you can push artifacts to OSDF (It is presumed you have done cmf artifact add to register the artifacts):    <pre><code>cmf artifact push -p Test-env\n</code></pre></p> </li> <li> <p>To pull artifacts from OSDF (using the cache for performance):    <pre><code>cmf artifact pull -p Test-env\n</code></pre></p> </li> </ol> <p>The pull command will automatically use the configured cache server for faster downloads.</p>"},{"location":"cmf_client/cmf_osdf/#notes","title":"Notes","text":"<ul> <li>Ensure you have the proper authentication credentials from your OSDF provider</li> <li>private key, </li> <li>key ID, </li> <li>issuerURL, </li> <li>origin FQDN and Federation Path </li> <li>The cache server (<code>--cache</code>) is used for reading data to improve performance (It identifies files directly from their path and contacts the topologically nearest cache to pull from)</li> <li>The origin server (<code>--path</code>) is used for writing data (push). Caches cannot be written to. </li> <li>OSDF uses token-based authentication that is automatically generated from your private key. This is handled internally by CMF under the covers. </li> <li>CMF also verifies that the retrieved file has the same Hash value as is recorded in MLMD</li> </ul>"},{"location":"cmf_client/cmf_osdf/#end-to-end-examples","title":"End to End Examples","text":""},{"location":"cmf_client/cmf_osdf/#single-site","title":"Single Site","text":""},{"location":"cmf_client/cmf_osdf/#pushing-artifacts","title":"Pushing Artifacts","text":"<ul> <li>Running CMF's examples/example-getting-started workflow and pushing artifacts to a specific remote</li> </ul> <pre><code>(cmf) tripataa@ai07:~/cmf-examples/example-get-started$ cmf init osdfremote --path https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test           --cache https://osdf-director.osg-htc.org/   --key-id XXXX    --key-path ~/private_hpe.pem    --key-issuer https://t.nationalresearchplatform.org/fdp-hpe --git-remote-url https://github.com/user/experiment-repo.git\ngit_dir /home/tripataa/cmf-examples/example-get-started/.git\nStarting cmf init.\nSetting 'osdf' as a default remote.\nSUCCESS: cmf init complete.\n(cmf) tripataa@ai07:~/cmf-examples/example-get-started$\n\n\n(cmf) tripataa@ai07:~/cmf-examples/example-get-started$ chmod +x test_script.sh\n(cmf) tripataa@ai07:~/cmf-examples/example-get-started$ ./test_script.sh\n\n[1/5] [RUNNING PARSE STEP         ]\n*** Note: CMF will check out a new branch in git to commit the metadata files ***\n*** The checked out branch is mlmd. ***\n100% Adding...|########################################|1/1 [00:00, 39.58file/s]\n100% Adding...|########################################|1/1 [00:00, 25.10file/s]\n100% Adding...|########################################|1/1 [00:00, 17.65file/s]\n100% Adding...|########################################|1/1 [00:00, 37.17file/s]\n\n[2/5] [RUNNING FEATURIZE STEP     ]\n*** Note: CMF will check out a new branch in git to commit the metadata files ***\n*** The checked out branch is mlmd. ***\n100% Adding...|########################################|1/1 [00:00, 62.53file/s]\n100% Adding...|########################################|1/1 [00:00, 31.05file/s]\n100% Adding...|########################################|1/1 [00:00, 47.68file/s]\nThe input data frame artifacts/parsed/train.tsv size is (20017, 3)\nThe output matrix artifacts/features/train.pkl size is (20017, 3002) and data type is float64\nThe input data frame artifacts/parsed/test.tsv size is (4983, 3)\nThe output matrix artifacts/features/test.pkl size is (4983, 3002) and data type is float64\n100% Adding...|########################################|1/1 [00:00, 26.49file/s]\n100% Adding...|########################################|1/1 [00:00, 47.62file/s]\n\n(cmf) tripataa@ai07:~/cmf-examples/example-get-started$ cmf artifact push -p Test-env\nCollecting                                            |0.00 [00:00,    ?entry/s]\nPushing...\nEverything is up to date.\n</code></pre>"},{"location":"cmf_client/cmf_osdf/#pulling-artifacts-via-cache","title":"Pulling Artifacts (via Cache)","text":"<ul> <li>Pulling artifacts via a cache. </li> <li>The --cache path is specified </li> <li>The artifacts for the pipeline: <code>Test-env</code> is pulled into a test/ folder (as an example)</li> <li>CMF looks at it's MLMD index and fetches the necessary files form the cache when configured. </li> </ul> <pre><code>(cmf) tripataa@ai07:~/cmf-examples/test$ ls\nmlmd\n(cmf) tripataa@ai07:~/cmf-examples/test$ cmf init osdfremote --path https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test \\\n          --cache https://osdf-director.osg-htc.org/ \\\n        --key-id XXX \\\n        --key-path ~/private_hpe.pem \\\n        --key-issuer https://t.nationalresearchplatform.org/fdp-hpe \\\n--git-remote-url https://github.com/user/experiment-repo.git\ngit_dir /home/tripataa/cmf-examples/test/.git\nStarting git init.\n*** Note: CMF will check out a new branch in git to commit the metadata files\n***\n*** The checked out branch is master. ***\ngit init complete.\nStarting cmf init.\nSetting 'osdf' as a default remote.\nSUCCESS: cmf init complete.\n\n(cmf) tripataa@ai07:~/cmf-examples/test$ cmf artifact pull -p Test-env\nFetching\nartifact=cmf_artifacts/python_env_4927239f4c14b700b637ff03ab787e65.yaml,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/49/27239f4c14b\n700b637ff03ab787e65 to\n./cmf_artifacts/python_env_4927239f4c14b700b637ff03ab787e65.yaml\nobject cmf_artifacts/python_env_4927239f4c14b700b637ff03ab787e65.yaml downloaded\nat ./cmf_artifacts/python_env_4927239f4c14b700b637ff03ab787e65.yaml in 0.00\nseconds and matches MLMD records.\nFetching artifact=artifacts/data.xml.gz,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/23/6d9502e0283\nd91f689d7038b8508a2 to ./artifacts/data.xml.gz\nobject artifacts/data.xml.gz downloaded at ./artifacts/data.xml.gz in 0.02\nseconds and matches MLMD records.\nFetching artifact=artifacts/parsed/train.tsv,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/32/b715ef0d71f\nf4c9e61f55b09c15e75 to ./artifacts/parsed/train.tsv\nobject artifacts/parsed/train.tsv downloaded at ./artifacts/parsed/train.tsv in\n0.03 seconds and matches MLMD records.\nFetching artifact=artifacts/parsed/test.tsv,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/6f/597d341ceb7\nd8fbbe88859a892ef81 to ./artifacts/parsed/test.tsv\nobject artifacts/parsed/test.tsv downloaded at ./artifacts/parsed/test.tsv in\n0.01 seconds and matches MLMD records.\nFetching artifact=artifacts/features/train.pkl,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/f2/17d9bdb2523\nc54e71dcb95fcf16ff2 to ./artifacts/features/train.pkl\nobject artifacts/features/train.pkl downloaded at ./artifacts/features/train.pkl\nin 0.02 seconds and matches MLMD records.\nFetching artifact=artifacts/features/test.pkl,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/30/4cfed883c9c\n0cecaeb7f0079e4b98c to ./artifacts/features/test.pkl\nobject artifacts/features/test.pkl downloaded at ./artifacts/features/test.pkl\nin 0.00 seconds and matches MLMD records.\nFetching\nartifact=cmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce8c58/metrics/training_met\nrics,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/64/d2da21bb624\n9afc9ea5a3a5cd67cc7 to\n./cmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce8c58/metrics/training_metrics\nobject\ncmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce8c58/metrics/training_metrics\ndownloaded at\n./cmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce8c58/metrics/training_metrics in\n0.00 seconds and matches MLMD records.\nFetching artifact=artifacts/model/model.pkl,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/a7/ea44151cfca\n33d7cfe3a1760973373 to ./artifacts/model/model.pkl\nobject artifacts/model/model.pkl downloaded at ./artifacts/model/model.pkl in\n0.00 seconds and matches MLMD records.\nSUCCESS: Number of files downloaded = 8.\n</code></pre>"},{"location":"cmf_client/cmf_osdf/#pulling-artifacts-without-cache-directly-form-origin","title":"Pulling Artifacts (without Cache. Directly form Origin)","text":"<ul> <li>Pulling artifacts without cache</li> <li>The --cache path is not specified. CMF automatically pulls from the origin which is configured  </li> <li>The artifacts for the pipeline: <code>Test-env</code> is pulled into a test2/ folder (as an example)</li> <li>CMF looks at it's MLMD index and fetches the necessary files form the cache when configured. </li> </ul> <pre><code>(cmf) tripataa@ai07:~/cmf-examples/test2$ ls\nmlmd\n(cmf) tripataa@ai07:~/cmf-examples/test2$ cmf init osdfremote --path https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test \\\n        --key-id XXXX \\\n        --key-path ~/private_hpe.pem \\\n        --key-issuer https://t.nationalresearchplatform.org/fdp-hpe \\\n--git-remote-url https://github.com/user/experiment-repo.git\ngit_dir /home/tripataa/cmf-examples/test2/.git\nStarting git init.\n*** Note: CMF will check out a new branch in git to commit the metadata files\n***\n*** The checked out branch is master. ***\ngit init complete.\nStarting cmf init.\nSetting 'osdf' as a default remote.\nSUCCESS: cmf init complete.\n(cmf) tripataa@ai07:~/cmf-examples/test2$ cmf artifact pull -p Test-env\nFetching\nartifact=/home/tripataa/cmf-examples/test2/cmf_artifacts/python_env_4927239f4c14\nb700b637ff03ab787e65.yaml,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/49/27239f4c\n14b700b637ff03ab787e65 to\n./cmf_artifacts/python_env_4927239f4c14b700b637ff03ab787e65.yaml\nobject\n/home/tripataa/cmf-examples/test2/cmf_artifacts/python_env_4927239f4c14b700b637f\nf03ab787e65.yaml downloaded at\n./cmf_artifacts/python_env_4927239f4c14b700b637ff03ab787e65.yaml in 0.00 seconds\nand matches MLMD records.\nFetching artifact=/home/tripataa/cmf-examples/test2/artifacts/data.xml.gz,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/23/6d9502e0\n283d91f689d7038b8508a2 to ./artifacts/data.xml.gz\nobject /home/tripataa/cmf-examples/test2/artifacts/data.xml.gz downloaded at\n./artifacts/data.xml.gz in 0.02 seconds and matches MLMD records.\nFetching artifact=/home/tripataa/cmf-examples/test2/artifacts/parsed/train.tsv,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/32/b715ef0d\n71ff4c9e61f55b09c15e75 to ./artifacts/parsed/train.tsv\nobject /home/tripataa/cmf-examples/test2/artifacts/parsed/train.tsv downloaded\nat ./artifacts/parsed/train.tsv in 0.03 seconds and matches MLMD records.\nFetching artifact=/home/tripataa/cmf-examples/test2/artifacts/parsed/test.tsv,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/6f/597d341c\neb7d8fbbe88859a892ef81 to ./artifacts/parsed/test.tsv\nobject /home/tripataa/cmf-examples/test2/artifacts/parsed/test.tsv downloaded at\n./artifacts/parsed/test.tsv in 0.01 seconds and matches MLMD records.\nFetching\nartifact=/home/tripataa/cmf-examples/test2/artifacts/features/train.pkl,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/f2/17d9bdb2\n523c54e71dcb95fcf16ff2 to ./artifacts/features/train.pkl\nobject /home/tripataa/cmf-examples/test2/artifacts/features/train.pkl downloaded\nat ./artifacts/features/train.pkl in 0.01 seconds and matches MLMD records.\nFetching artifact=/home/tripataa/cmf-examples/test2/artifacts/features/test.pkl,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/30/4cfed883\nc9c0cecaeb7f0079e4b98c to ./artifacts/features/test.pkl\nobject /home/tripataa/cmf-examples/test2/artifacts/features/test.pkl downloaded\nat ./artifacts/features/test.pkl in 0.00 seconds and matches MLMD records.\nFetching\nartifact=/home/tripataa/cmf-examples/test2/cmf_artifacts/f3d287ba-5cf8-11f0-bf02\n-d4c9efce8c58/metrics/training_metrics,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/64/d2da21bb\n6249afc9ea5a3a5cd67cc7 to\n./cmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce8c58/metrics/training_metrics\nobject\n/home/tripataa/cmf-examples/test2/cmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce\n8c58/metrics/training_metrics downloaded at\n./cmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce8c58/metrics/training_metrics in\n0.00 seconds and matches MLMD records.\nFetching artifact=/home/tripataa/cmf-examples/test2/artifacts/model/model.pkl,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/a7/ea44151c\nfca33d7cfe3a1760973373 to ./artifacts/model/model.pkl\nobject /home/tripataa/cmf-examples/test2/artifacts/model/model.pkl downloaded at\n./artifacts/model/model.pkl in 0.00 seconds and matches MLMD records.\nSUCCESS: Number of files downloaded = 8.\n(cmf) tripataa@ai07:~/cmf-examples/test2$\n</code></pre>"},{"location":"cmf_client/cmf_osdf/#multi-site","title":"Multi-Site","text":"<p>CMF supports multi-site deployments where artifacts can be shared across different institutions through OSDF's distributed federation. In a multi-site setup:</p> <ul> <li>Origins can be deployed at different institutions to store local data</li> <li>Caches are distributed geographically to provide optimal performance</li> <li>OSDF Director coordinates access to the nearest available cache</li> <li>Token-based authentication ensures secure access across sites</li> <li>CMF metadata tracks artifact lineage across the entire federation</li> </ul> <p>This enables collaborative workflows where: 1. Researchers at Site A can push artifacts to their local origin 2. Researchers at Site B can pull those same artifacts through their nearest cache 3. All metadata and provenance information is preserved across sites 4. Performance is optimized through intelligent cache selection</p> <p>TBD</p>"},{"location":"cmf_client/minio-server/","title":"MinIO S3 Artifact Repository Setup","text":""},{"location":"cmf_client/minio-server/#steps-to-set-up-a-minio-server","title":"Steps to set up a MinIO server","text":"<p>Object storage is an abstraction layer above the file system that provides an API to interact with data. MinIO is an easy way to start working with object storage. It is compatible with S3, easy to deploy, manage locally, and scale if needed.</p> <p>Follow the steps below to set up a MinIO server:</p> <ol> <li> <p>Copy the contents of the <code>example-get-started</code> directory to a separate directory outside the cmf repository.</p> </li> <li> <p>Check whether cmf is initialized.    <pre><code>cmf init show\n</code></pre>    If cmf is not initialized, the following message will appear on the screen:    <pre><code>'cmf' is not configured.\nExecute the 'cmf init' command.\n</code></pre></p> </li> <li> <p>Execute the following command to initialize the MinIO S3 bucket as a CMF artifact repository:     <pre><code>cmf init minioS3 --url s3://dvc-art --endpoint-url http://x.x.x.x:9000 \\\n  --access-key-id minioadmin --secret-key minioadmin \\\n  --git-remote-url https://github.com/user/experiment-repo.git \\\n  --cmf-server-url http://x.x.x.x:80  --neo4j-user neo4j \\\n  --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre></p> </li> </ol> <p>Here, \"dvc-art\" is provided as an example bucket name. However, users can change it as needed. If the user chooses to change it, they will need to update the Dockerfile for minioS3 accordingly.</p> <ol> <li> <p>Execute <code>cmf init show</code> to check the CMF configuration. The sample output looks as follows:    <pre><code>remote.minio.url=s3://bucket-name\nremote.minio.endpointurl=http://localhost:9000\nremote.minio.access_key_id=minioadmin\nremote.minio.secret_access_key=minioadmin\ncore.remote=minio\n</code></pre></p> </li> <li> <p>Build a MinIO server using a Docker container. The <code>docker-compose.yml</code> available in the    <code>example-get-started</code> directory provides two services: <code>minio</code> and <code>aws-cli</code>. The user    will initialize the repository with the bucket name, storage URL, and credentials to    access MinIO.</p> </li> <li> <p>Execute the following command to start the Docker container. The MYIP variable is the IP address of the machine on which you are executing the following command. The following command requires root privileges.    <pre><code>MYIP=XX.XX.XXX.XXX docker-compose up\n</code></pre>    or    <pre><code>MYIP=XX.XX.XXX.XXX docker compose up\n</code></pre>    You should see output confirming that MinIO is up and running.</p> <p>Also, you can adjust <code>$MYIP</code> in <code>examples/example-get-started/docker-compose.yml</code> to specify     the server IP and run the <code>docker compose</code> command without specifying MYIP above.</p> </li> <li> <p>Login into <code>remote.minio.endpointurl</code> (in the above example - http://localhost:9000) using    the access-key and the secret-key defined in the <code>cmf init</code> command.</p> </li> <li> <p>The following image is a snapshot of this example using a MinIO server and a bucket named 'dvc-art'.</p> </li> </ol> <p></p>"},{"location":"cmf_client/neo4j_docker/","title":"Start Neo4j with Docker","text":""},{"location":"cmf_client/neo4j_docker/#prerequisite","title":"\ud83d\udee0\ufe0f Prerequisite","text":"<p>Docker should be installed.</p>"},{"location":"cmf_client/neo4j_docker/#command-to-start-neo4j-docker-container","title":"\ud83d\ude80 Command to Start Neo4j Docker Container","text":"<pre><code>docker run     --name testneo4j \\\n        -p7474:7474 -p7687:7687  \\\n        -d   \\\n        -v $HOME/neo4j/data:/data  \\\n        -v $HOME/neo4j/logs:/logs     \\\n        -v $HOME/neo4j/import:/var/lib/neo4j/import   \\\n        -v $HOME/neo4j/plugins:/plugins   \\\n        --env NEO4J_AUTH=neo4j/test1234    \\\n        neo4j:latest\n</code></pre> <p>The <code>docker run</code> command creates and starts a container. On the next line, <code>--name testneo4j</code> defines the name we want to use for the container as <code>testneo4j</code>.</p> <p>Using the <code>-p</code> option with ports <code>7474</code> and <code>7687</code> allows us to expose and listen for traffic on both the HTTP and Bolt ports. Having the HTTP port means we can connect to our database with Neo4j Browser, and the Bolt port means efficient and type-safe communication requests between other layers and the database.</p> <p>Next, we have <code>-d</code>. This detaches the container to run in the background, meaning we can access the container separately and see into all of its processes.</p> <p>The next several lines start with the <code>-v</code> option. These lines define volumes we want to bind in our local directory structure so we can access certain files locally.</p> <ul> <li>The first one is for our <code>/data</code> directory, which stores the system information and graph data.</li> <li>The second <code>-v</code> option is for the <code>/logs</code> directory. Outputting the Neo4j logs to a place outside the container ensures we can troubleshoot any errors in Neo4j, even if the container crashes.</li> <li>The third line with the <code>-v</code> option binds the import directory, so we can copy CSV or other flat files into that directory for importing into Neo4j. Load scripts for importing that data can also be placed in this folder for us to execute.</li> <li>The next <code>-v</code> option line sets up our plugins directory.</li> </ul> <p>On the next line with the <code>--env</code> parameter, we initiate our Neo4j instance with a username and password. Neo4j automatically sets up basic authentication with the <code>neo4j</code> username as a foundation for security. Since it will initiate authentication and require a password change when first connecting, we can handle all of that in this parameter.</p> <p>Finally, the last line of the command above references the Docker image we want to pull from DockerHub (<code>neo4j</code>), as well as any specified version (in this case, just the latest edition).</p> <p>\ud83d\udd17 More details</p> <p>To access Neo4j from browser, open: \u27a1\ufe0f http://IP:7474/</p> <p>Replace <code>IP</code> with your system IP address.</p>"},{"location":"cmf_client/neo4j_docker/#some-useful-neo4j-cypher-queries","title":"\ud83e\uddea Some Useful Neo4j Cypher Queries","text":"<p>Replace <code>&lt;pipelinename&gt;</code> with your actual pipeline name.</p>"},{"location":"cmf_client/neo4j_docker/#1-artifact-lineage-with-processing-steps","title":"1. Artifact Lineage with Processing Steps","text":"<pre><code>MATCH (a:Execution{pipeline_name:'&lt;pipelinename&gt;'})-[r]-(b) \nWHERE (b:Dataset or b:Model or b:Metrics) \nRETURN a, r, b \n</code></pre>"},{"location":"cmf_client/neo4j_docker/#2-artifact-lineage","title":"2. Artifact Lineage","text":"<pre><code>MATCH (b) \nWHERE (b:Dataset or b:Model or b:Metrics) \nAND '&lt;pipelinename&gt;' IN b.pipeline_name \nRETURN b\n</code></pre>"},{"location":"cmf_client/neo4j_docker/#3-lineage-of-processing-steps","title":"3. Lineage of Processing Steps","text":"<pre><code>MATCH (n:Execution{pipeline_name:'&lt;pipelinename&gt;'}) \nRETURN n\n</code></pre>"},{"location":"cmf_client/neo4j_docker/#4-clean-up","title":"4. Clean Up","text":"<p>All execution/stage nodes related to a pipeline</p> <pre><code>MATCH (n {pipeline_name:'&lt;pipelinename&gt;'}) \nDETACH DELETE n\n</code></pre> <p>All Dataset nodes</p> <pre><code>MATCH (n) \nWHERE '&lt;pipelinename&gt;' IN n.pipeline_name \nDETACH DELETE n\n</code></pre>"},{"location":"cmf_client/ssh-setup/","title":"SSH Remote Artifact Repository Setup","text":""},{"location":"cmf_client/ssh-setup/#steps-to-set-up-an-ssh-remote-repo","title":"Steps to set up an SSH Remote Repo","text":"<p>SSH (Secure Shell) remote storage refers to using the SSH protocol to securely access and manage files and data on a remote server or storage system over a network. SSH is a cryptographic network protocol that allows secure communication and data transfer between a local computer and a remote server.</p> <p>Proceed with the following steps to set up an SSH Remote Repository:</p> <ol> <li>Initialize the <code>project directory</code> with an SSH remote.</li> <li> <p>Check whether cmf is initialized in your project directory with the following command.    <pre><code>cmf init show\n</code></pre>    If cmf is not initialized, the following message will appear on the screen.    <pre><code>'cmf' is not configured.\nExecute the 'cmf init' command.\n</code></pre></p> </li> <li> <p>Execute the following command to initialize the SSH remote storage as a CMF artifact repository.     <pre><code>cmf init sshremote --path ssh://127.0.0.1/home/user/ssh-storage \\\n    --user XXXXX --port 22 --password example@123 \\\n    --git-remote-url https://github.com/user/experiment-repo.git \\\n    --cmf-server-url http://127.0.0.1:80\n</code></pre>     &gt; When running <code>cmf init sshremote</code>, ensure that the specified IP address allows access for \\     the specified user XXXX. If the IP address or user does not exist, this command will fail.</p> </li> <li> <p>Execute <code>cmf init show</code> to check the CMF configuration.</p> </li> <li>To troubleshoot SSH permissions-related issues, check the <code>/etc/ssh/sshd_config</code> file on the    remote machine. This configuration file serves as the primary starting point for diagnosing    and resolving SSH permission-related challenges.</li> </ol>"},{"location":"cmf_client/tensorflow_guide/","title":"How to Use TensorBoard with CMF","text":"<ol> <li> <p>Copy the contents of the <code>example-get-started</code> directory from <code>cmf/examples/example-get-started</code> into a separate directory outside the CMF repository.</p> </li> <li> <p>Execute the following command to install the TensorFlow library in the     current directory:     <pre><code>pip install tensorflow\n</code></pre></p> </li> <li> <p>Create a new Python file (e.g., <code>tensorflow_log.py</code>) and copy the following code:</p> <pre><code>import datetime\nimport tensorflow as tf\n\nmnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\ndef create_model():\n     return tf.keras.models.Sequential([\n          tf.keras.layers.Flatten(input_shape=(28, 28), name='layers_flatten'),\n          tf.keras.layers.Dense(512, activation='relu', name='layers_dense'),\n          tf.keras.layers.Dropout(0.2, name='layers_dropout'),\n          tf.keras.layers.Dense(10, activation='softmax', name='layers_dense_2')\n     ])\n\nmodel = create_model()\nmodel.compile(\n     optimizer='adam',\n     loss='sparse_categorical_crossentropy',\n     metrics=['accuracy']\n)\n\nlog_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\nmodel.fit(x=x_train,y=y_train,epochs=5,validation_data=(x_test, y_test),callbacks=[tensorboard_callback])\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n\ntrain_dataset = train_dataset.shuffle(60000).batch(64)\ntest_dataset = test_dataset.batch(64)\n\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\noptimizer = tf.keras.optimizers.Adam()\n\n# Define our metrics\ntrain_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\ntest_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')\n\ndef train_step(model, optimizer, x_train, y_train):\n     with tf.GradientTape() as tape:\n          predictions = model(x_train, training=True)\n          loss = loss_object(y_train, predictions)\n     grads = tape.gradient(loss, model.trainable_variables)\n     optimizer.apply_gradients(zip(grads, model.trainable_variables))\n     train_loss(loss)\n     train_accuracy(y_train, predictions)\n\ndef test_step(model, x_test, y_test):\n     predictions = model(x_test)\n     loss = loss_object(y_test, predictions)\n     test_loss(loss)\n     test_accuracy(y_test, predictions)\n\ncurrent_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntrain_log_dir = 'logs/gradient_tape/' + current_time + '/train'\ntest_log_dir = 'logs/gradient_tape/' + current_time + '/test'\ntrain_summary_writer = tf.summary.create_file_writer(train_log_dir)\ntest_summary_writer = tf.summary.create_file_writer(test_log_dir)\n\nmodel = create_model()  # reset our model\nEPOCHS = 5\nfor epoch in range(EPOCHS):\n     for (x_batch, y_batch) in train_dataset:\n          train_step(model, optimizer, x_batch, y_batch)\n     with train_summary_writer.as_default():\n          tf.summary.scalar('loss', train_loss.result(), step=epoch)\n          tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n\n     for (x_batch, y_batch) in test_dataset:\n          test_step(model, x_batch, y_batch)\n     with test_summary_writer.as_default():\n          tf.summary.scalar('loss', test_loss.result(), step=epoch)\n          tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n     template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n     print(template.format(\n          epoch + 1,\n          train_loss.result(),\n          train_accuracy.result() * 100,\n          test_loss.result(),\n          test_accuracy.result() * 100\n     ))\n</code></pre> <p>For more detailed information, check out the TensorBoard documentation.</p> </li> <li> <p>Execute the TensorFlow log script using the following command:     <pre><code>python3 tensorflow_log.py\n</code></pre></p> </li> <li> <p>The above script will automatically create a <code>logs</code> directory inside your current directory.</p> </li> <li> <p>Start the CMF Server and configure the CMF Client.</p> </li> <li> <p>Use the following command to run the test script, which will generate the MLMD file:     <pre><code>sh test_script.sh\n</code></pre> <code>Note</code> - <code>MLMD</code> stands for ML Metadata, which is typically stored as a SQLite file generated during pipeline runs. The file extension and name may vary depending on your setup.</p> </li> <li> <p>Use the following command to push the generated MLMD and TensorFlow log files to the CMF server:     <pre><code>cmf metadata push -p 'pipeline-name' -t 'tensorboard-log-file-name'\n</code></pre></p> </li> <li> <p>Go to the CMF server and navigate to the TensorBoard tab. You will see an interface similar to the following image: </p> </li> </ol>"},{"location":"cmf_server/metahub-tab-usage/","title":"Introduction to Metahub Feature","text":"<p>The Metahub feature is introduced to synchronize metadata between two CMF servers. This document explains how to use this feature effectively via the GUI.</p>"},{"location":"cmf_server/metahub-tab-usage/#steps-to-use-metahub-feature","title":"Steps to Use Metahub feature","text":""},{"location":"cmf_server/metahub-tab-usage/#1-start-the-cmf-server","title":"1. Start the CMF Server","text":"<p>Ensure that the CMF server is up and running. you can follow below document.</p> <p>\ud83d\udcc4 Guide to start CMF Server</p>"},{"location":"cmf_server/metahub-tab-usage/#2-navigate-to-metahub-tab","title":"2. Navigate to Metahub Tab","text":"<p>In the GUI, locate and click the Metahub tab from the navigation panel.</p> <p>After clicking the Metahub tab, you will see three tabs:</p> <ul> <li>Registration \u2192 Register a new server for syncing.</li> <li>Sync Server \u2192 Perform sync with a registered server.</li> <li>Registered Server \u2192 View the list of registered servers.</li> </ul> <p></p>"},{"location":"cmf_server/metahub-tab-usage/#registration-tab","title":"Registration Tab","text":"<p>The Registration tab allows you to register another server that you want to sync with.</p>"},{"location":"cmf_server/metahub-tab-usage/#functionality","title":"\ud83d\udcdd Functionality:","text":"<ol> <li>Register the target server you want to sync with.</li> <li>You can provide either:<ul> <li>Server Name + IP Address or </li> <li>Server Name + Hostname</li> </ul> </li> <li>Click Submit. You\u2019ll receive a message confirming whether the server registration was successful.</li> </ol>"},{"location":"cmf_server/metahub-tab-usage/#sync-server-tab","title":"Sync Server Tab","text":"<p>The Sync Server tab is used to sync metadata with a registered target server.</p>"},{"location":"cmf_server/metahub-tab-usage/#functionality_1","title":"\ud83d\udcdd Functionality:","text":"<ol> <li>A dropdown will show all the registered target servers.</li> <li>Select a server from the list.</li> <li>Click to sync. If the selected server is available and properly registered, the sync will succeed. Otherwise, it will fail.</li> </ol>"},{"location":"cmf_server/metahub-tab-usage/#_1","title":"Metahub","text":""},{"location":"cmf_server/metahub-tab-usage/#registered-server-tab","title":"Registered Server Tab","text":"<p>This section displays all registered servers in a table format.</p>"},{"location":"cmf_server/metahub-tab-usage/#functionality_2","title":"\ud83d\udcdd Functionality:","text":"<ol> <li>View the list of all registered target servers.</li> <li>The table includes a <code>last_sync_time</code> column to indicate when each server was last successfully synced.</li> </ol> <p>\ud83d\udccc Make sure all servers involved are running and reachable via the provided IP or hostname.</p>"},{"location":"cmflib/","title":"CMFLib","text":"<p>The <code>cmflib</code> package is the foundational Python library that provides the core metadata tracking capabilities for the Common Metadata Framework (CMF). It offers a unified API for logging metadata across distributed AI/ML pipelines, integrating with multiple storage backends and versioning systems.</p> <p>This document covers the core library components, main API classes, and integration mechanisms. For CLI usage patterns, see Quick Start with CMF Client. For server-side components, see CMF Server.</p>"},{"location":"cmflib/#core-architecture","title":"Core Architecture","text":"<p>Note: The CMF library should be imported as <code>from cmflib.cmf import Cmf</code>. See examples below for the correct usage pattern.</p> <p>Complex ML projects rely on <code>ML pipelines</code> to train and test ML models. An ML pipeline is a sequence of stages where each stage performs a particular task, such as data loading, pre-processing, ML model training, and testing stages. Each stage can have multiple Executions which:</p> <ul> <li>consume <code>inputs</code> and produce <code>outputs</code>.</li> <li>are parameterized by parameters that guide the process of producing outputs.</li> </ul> <p></p> <p>CMF uses the abstractions of <code>Pipeline</code>, <code>Context</code>, and <code>Executions</code> to store the metadata of complex ML pipelines. Each pipeline has a name. Users provide it when they initialize the CMF. Each stage is represented by a <code>Context</code> object. Metadata associated with each run of a stage is captured in the Execution object. Inputs and outputs of Executions can be logged as dataset, model, or metrics. While parameters of executions are recorded as properties of executions.</p> <p></p>"},{"location":"cmflib/#main-api","title":"Main API","text":"<p>The <code>Cmf</code> class is the primary interface for metadata tracking in CMF. It provides methods for creating pipelines, contexts, executions, and logging artifacts.</p>"},{"location":"cmflib/#key-methods","title":"Key Methods","text":"Method Purpose Usage <code>__init__(filepath, pipeline_name)</code> Initialize CMF instance <code>metawriter = Cmf(filepath=\"mlmd\", pipeline_name=\"my_pipeline\")</code> <code>create_context(pipeline_stage)</code> Create a pipeline stage context <code>context = metawriter.create_context(pipeline_stage=\"train\")</code> <code>create_execution(execution_type)</code> Create an execution within a context <code>execution = metawriter.create_execution(execution_type=\"training_run\")</code> <code>log_dataset(url, event, custom_properties)</code> Log dataset artifacts <code>metawriter.log_dataset(url=\"data.csv\", event=\"input\")</code> <code>log_model(path, event, model_framework)</code> Log model artifacts <code>metawriter.log_model(path=\"model.pkl\", event=\"output\")</code> <code>log_metrics(metrics_name, custom_properties)</code> Log metrics <code>metawriter.log_metrics(metrics_name=\"accuracy\", custom_properties={\"value\": 0.95})</code> 1 Init2 Stage type3 New execution4 Log Artifacts <p>Start tracking the pipeline metadata by initializing the CMF runtime. The metadata will be associated with the pipeline named <code>test_pipeline</code>. <pre><code>from cmflib.cmf import Cmf\nfrom ml_metadata.proto import metadata_store_pb2 as mlpb\n\nmetawriter = Cmf(\n    filename=\"mlmd\",\n    pipeline_name=\"test_pipeline\",\n)\n</code></pre></p> <p>Before we can start tracking metadata, we need to let CMF know about the stage type. This is not yet associated with this particular execution. <pre><code>context: mlmd.proto.Context = metawriter.create_context(\n    pipeline_stage=\"train\"\n)\n</code></pre></p> <p>Now we can create a new stage execution associated with the <code>train</code> stage. The CMF always creates a new execution, and will adjust its name, so it's unique. This is also the place where we can log execution <code>parameters</code> like seed, hyper-parameters, etc. <pre><code>execution: mlmd.proto.Execution = metawriter.create_execution(\n    execution_type=\"train\",\n    custom_properties = {\"num_epochs\": 100, \"learning_rate\": 0.01}\n)\n</code></pre></p> <p>Finally, we can log an input (train dataset), and once trained, an output (ML model) artifact. <pre><code>metawriter.log_dataset(\n    'artifacts/test_dataset.csv',   # Dataset path\n    \"input\"                         # This is INPUT artifact\n)\nmetawriter.log_model(\n    \"artifacts/model.pkl\",          # Model path\n    event=\"output\"                  # This is OUTPUT artifact\n)\n</code></pre></p>"},{"location":"cmflib/#quick-example","title":"Quick Example","text":"<p>Go through the Getting Started page to learn more about CMF API usage.</p>"},{"location":"cmflib/#api-overview","title":"API Overview","text":"<p>Import CMF. <pre><code>from cmflib.cmf import Cmf\n</code></pre></p> <p>Initialize CMF. The CMF object is responsible for managing a CMF backend to record the pipeline metadata. Internally, it creates a pipeline abstraction that groups individual stages and their executions. All stages, their executions, and produced artifacts will be associated with a pipeline with the given name. <pre><code>metawriter = Cmf(\n   filepath=\"mlmd\",                # Path to ML Metadata file.\n   pipeline_name=\"mnist\"           # Name of an ML pipeline.\n)\n</code></pre></p> <p>Define a stage. An ML pipeline can have multiple stages, and each stage can be associated with multiple executions. A stage is described by a context, which specifies its name and optional properties. You can create a context using the create_context method: <pre><code>context = metawriter.create_context(\n    pipeline_stage=\"download\",     # Stage name\n    custom_properties={            # Optional properties\n        \"uses_network\": True,      #  Downloads from the Internet\n        \"disk_space\": \"10GB\"       #  Needs this much space\n    }\n)\n</code></pre></p> <p>Create a stage execution. A stage in an ML pipeline can have multiple executions. Every run is marked as an execution. This API helps to track the metadata associated with the execution, like stage parameters (e.g., number of epochs and learning rate for train stages). The stage execution name does not need to be the same as the name of its context. Moreover, the CMF will adjust this name to ensure every execution has a unique name. The CMF will internally associate this execution with the context created previously. Stage executions are created by calling the create_execution method. <pre><code>execution = metawriter.create_execution(\n    execution_type=\"download\",            # Execution name.\n    custom_properties = {                 # Execution parameters\n        \"url\": \"https://a.com/mnist.gz\"   #  Data URL.\n    }\n)\n</code></pre></p> <p>Log artifacts. A stage execution can consume (inputs) and produce (outputs) multiple artifacts (datasets, models, and performance metrics). The path of these artifacts must be relative to the project (repository) root path. Artifacts might have optional metadata associated with them. This metadata could include feature statistics for ML datasets, or useful parameters for ML models (such as, for instance, number of trees in a random forest classifier).</p> <ul> <li> <p>Datasets are logged with the log_dataset method.     <pre><code>metawriter.log_dataset('data/mnist.gz', \"input\", custom_properties={\"name\": \"mnist\", \"type\": 'raw'})\nmetawriter.log_dataset('data/train.csv', \"output\", custom_properties={\"name\": \"mnist\", \"type\": \"train_split\"})\nmetawriter.log_dataset('data/test.csv', \"output\", custom_properties={\"name\": \"mnist\", \"type\": \"test_split\"})\n</code></pre></p> </li> <li> <p>ML models produced by training stages are logged using the log_model API. ML models can be   both input and output artifacts. The metadata associated with the artifact could be logged as an optional argument.     <pre><code># In train stage\nmetawriter.log_model(\n   path=\"model/rf.pkl\", event=\"output\", model_framework=\"scikit-learn\", model_type=\"RandomForestClassifier\",\n   model_name=\"RandomForestClassifier:default\"\n)\n\n# In test stage\nmetawriter.log_model(\n   path=\"model/rf.pkl\", event=\"input\"\n)\n</code></pre></p> </li> <li> <p>Metrics of every optimization step (one epoch of Stochastic Gradient Descent, or one boosting round in   Gradient Boosting Trees) are logged using the log_metric API.     <pre><code># Can be called at every epoch or every step in the training. This is logged to a parquet file and committed at the\n# commit stage.\n\n# Inside training loop\nwhile True:\n     metawriter.log_metric(\"training_metrics\", {\"loss\": loss})\nmetawriter.commit_metrics(\"training_metrics\")\n</code></pre></p> </li> <li> <p>Stage metrics, or final metrics, are logged with the log_execution_metrics   method. These are final metrics of a stage, such as final train or test accuracy.     <pre><code>metawriter.log_execution_metrics(\"metrics\", {\"avg_prec\": avg_prec, \"roc_auc\": roc_auc})\n</code></pre></p> </li> </ul> <p>Dataslices are intended to be used to track subsets of the data. For instance, this can be used to track and compare accuracies of ML models on these subsets to identify model bias. Data slices are created with the create_dataslice method. <pre><code>dataslice = metawriter.create_dataslice(\"slice-a\")\nfor i in range(1, 20, 1):\n    j = random.randrange(100)\n    dataslice.add_data(\"data/raw_data/\"+str(j)+\".xml\")\ndataslice.commit()\n</code></pre></p>"},{"location":"cmflib/#graph-layer-overview","title":"Graph Layer Overview","text":"<p>The CMF library has an optional <code>graph layer</code> which stores the relationships in a Neo4J graph database. To use the graph layer, the <code>graph</code> parameter in the library init call must be set to true (it is set to false by default). The library reads the configuration parameters of the graph database from the <code>cmf config</code> generated by the <code>cmf init</code> command.</p> <pre><code>cmf init minioS3 --url s3://dvc-art \n                 --endpoint-url http://x.x.x.x:9000\n                 --access-key-id minioadmin\n                 --secret-key minioadmin\n                 --git-remote-url https://github.com/user/experiment-repo.git\n                 --cmf-server-url http://x.x.x.x:80\n                 --neo4j-user neo4j\n                 --neo4j-password password \n                 --neo4j-uri bolt://localhost:7687\n</code></pre> <p>Here, \"dvc-art\" is provided as an example bucket name. However, users can change it as needed. If the user chooses to change it, they will need to update the Dockerfile for minioS3 accordingly.</p> <p>To use the graph layer, instantiate the CMF with the <code>graph=True</code> parameter: <pre><code>from cmflib.cmf import Cmf\n\nmetawriter =  Cmf(\n   filepath=\"mlmd\",\n   pipeline_name=\"anomaly_detection_pipeline\",\n   graph=True\n)\n</code></pre></p>"},{"location":"common-metadata-ontology/readme/","title":"\ud83d\udd17 Ontology","text":""},{"location":"common-metadata-ontology/readme/#common-metadata-ontology","title":"Common Metadata Ontology","text":"<p>Common Metadata Ontology (CMO) integrates and aggregates pipeline metadata from various sources such as Papers-with-Code, OpenML, and Hugging Face. CMF's data model is a manifestation of CMO, specifically designed to capture the pipeline-centric metadata of AI pipelines. It consists of nodes to represent a pipeline, components of a pipeline (stages), relationships to capture interactions among pipeline entities, and properties. CMO offers interoperability of diverse metadata, search, and recommendation with reasoning capabilities. CMO offers flexibility to incorporate various executions implemented for each stage, such as dataset preprocessing, feature engineering, training (including hyperparameter optimization), testing, and evaluation. This enables robust search capabilities to identify the best execution path for a given pipeline. Additionally, CMO also facilitates the inclusion of additional semantic and statistical properties to enhance the richness and comprehensiveness of the metadata associated with them. An overview of CMO can be found below.</p> <p></p> <p>The external link to arrows.app can be found here</p>"},{"location":"common-metadata-ontology/readme/#sample-pipeline-represented-using-cmo","title":"Sample pipeline represented using CMO","text":"<p>The sample figure shows a pipeline titled \"Robust outlier detection by de-biasing VAE likelihoods\" executed for the \"Outlier Detection\" task, focusing on the stage train/test. The model used in the pipeline was \"Variational Autoencoder\". Several datasets were used in the pipeline:</p> <ul> <li>German Traffic Sign</li> <li>Street View House Numbers</li> <li>CelebFaces Attributes dataset.</li> </ul> <p>The corresponding hyperparameters used and the metrics generated as a result of execution are included in the figure. The external link to the source figure created using arrows.app can be found here</p>"},{"location":"common-metadata-ontology/readme/#turtle-syntax","title":"Turtle Syntax","text":"<p>The Turtle format of the formal ontology can be found here</p>"},{"location":"common-metadata-ontology/readme/#properties-of-each-node","title":"Properties of each node","text":"<p>The properties of each node can be found below.</p>"},{"location":"common-metadata-ontology/readme/#pipeline","title":"Pipeline","text":"<p>AI pipeline executed to solve a machine or deep learning task</p>"},{"location":"common-metadata-ontology/readme/#properties","title":"Properties","text":"<ul> <li>pipeline_id</li> <li>pipeline_name</li> <li>pipeline_source</li> <li>source_id</li> <li>custom_properties*</li> </ul>"},{"location":"common-metadata-ontology/readme/#report","title":"Report","text":"<p>Any published text document regarding the pipeline implementation</p>"},{"location":"common-metadata-ontology/readme/#properties_1","title":"Properties","text":"<ul> <li>report_id</li> <li>report_title</li> <li>report_pdf_url</li> <li>source</li> <li>source_id</li> <li>abstract*</li> <li>custom_properties*</li> </ul>"},{"location":"common-metadata-ontology/readme/#task","title":"Task","text":"<p>The AI task for which the pipeline is implemented. Example: image classification</p>"},{"location":"common-metadata-ontology/readme/#properties_2","title":"Properties","text":"<ul> <li>task_id</li> <li>task_name</li> <li>task_description</li> <li>task_type</li> <li>modality</li> <li>category</li> <li>source</li> <li>custom_properties*</li> </ul>"},{"location":"common-metadata-ontology/readme/#framework","title":"Framework","text":"<p>The framework used to implement the pipeline and its code repository</p>"},{"location":"common-metadata-ontology/readme/#properties_3","title":"Properties","text":"<ul> <li>framework_id</li> <li>framework_name</li> <li>code_repo_url</li> <li>framework_version</li> <li>source</li> </ul>"},{"location":"common-metadata-ontology/readme/#stage","title":"Stage","text":"<p>Various stages of the pipeline, such as data preprocessing, training, testing, or evaluation</p>"},{"location":"common-metadata-ontology/readme/#properties_4","title":"Properties","text":"<ul> <li>stage_id</li> <li>stage_name</li> <li>source</li> <li>pipeline_id</li> <li>pipeline_name</li> <li>custom_properties</li> </ul>"},{"location":"common-metadata-ontology/readme/#execution","title":"Execution","text":"<p>Multiple executions of a given stage in a pipeline</p>"},{"location":"common-metadata-ontology/readme/#properties_5","title":"Properties","text":"<ul> <li>execution_id</li> <li>execution_name</li> <li>stage_id</li> <li>stage_name</li> <li>pipeline_id</li> <li>pipeline_name</li> <li>source</li> <li>command (CLI command to run the execution)</li> <li>custom_properties</li> </ul>"},{"location":"common-metadata-ontology/readme/#artifact","title":"Artifact","text":"<p>Artifacts such as model, dataset, and metric generated at the end of each execution</p>"},{"location":"common-metadata-ontology/readme/#properties_6","title":"Properties","text":"<ul> <li>artifact_id</li> <li>artifact_name</li> <li>pipeline_id</li> <li>pipeline_name</li> <li>execution_id</li> <li>source</li> <li>custom_properties</li> </ul>"},{"location":"common-metadata-ontology/readme/#dataset","title":"Dataset","text":"<p>Subclass of artifact. The dataset used in each execution of a pipeline</p>"},{"location":"common-metadata-ontology/readme/#properties_7","title":"Properties","text":"<ul> <li>dataset_id</li> <li>dataset_name</li> <li>dataset_url</li> <li>modality</li> <li>description</li> <li>source</li> <li>custom_properties</li> </ul>"},{"location":"common-metadata-ontology/readme/#model","title":"Model","text":"<p>Subclass of artifact. The model used in each execution or produced as a result of an execution</p>"},{"location":"common-metadata-ontology/readme/#properties_8","title":"Properties","text":"<ul> <li>model_id</li> <li>model_name</li> <li>model_class</li> <li>description</li> <li>artifact_id</li> <li>source</li> <li>custom_properties</li> </ul>"},{"location":"common-metadata-ontology/readme/#metric","title":"Metric","text":"<p>Subclass of artifact. The evaluation result of each execution</p>"},{"location":"common-metadata-ontology/readme/#properties_9","title":"Properties","text":"<ul> <li>metric_id</li> <li>metric_name</li> <li>artifact_id</li> <li>evaluations</li> <li>source</li> <li>custom_properties**</li> </ul>"},{"location":"common-metadata-ontology/readme/#hyperparameters","title":"Hyperparameters","text":"<p>Parameter settings used for each execution of a stage</p>"},{"location":"common-metadata-ontology/readme/#properties_10","title":"Properties","text":"<ul> <li>parameter_id</li> <li>parameter_setting (key-value pair)</li> <li>source</li> <li>model_id</li> <li>custom_properties</li> </ul> <p>NOTE: * are optional properties * There is additional information on each node, different for each source. As of now, these are included in the KG for efficient search, but they are available to be used in the future to extract the data and populate as node properties. * *For metric, there are umpteen possible metric names and values. Therefore, we capture all of them as a key-value pair under evaluations. * custom_properties are where users can enter custom properties for each node while executing a pipeline. * source is the source from which the node is obtained - Papers-with-Code, OpenML, Hugging Face.</p>"},{"location":"common-metadata-ontology/readme/#published-works","title":"Published works","text":"<ul> <li>R. Venkataramanan, A. Tripathy, M. Foltin, H. Y. Yip, A. Justine, and A. Sheth, \"Knowledge Graph Empowered Machine Learning Pipelines for Improved Efficiency, Reusability, and Explainability,\" in IEEE Internet Computing, vol. 27, no. 1, pp. 81-88, 1 Jan.-Feb. 2023, doi: 10.1109/MIC.2022.3228087. Link: https://www.computer.org/csdl/magazine/ic/2023/01/10044293/1KL6TPO5huw</li> </ul>"},{"location":"common-metadata-ontology/readme/#related-works","title":"Related works","text":"<ul> <li>Publio, G. C., Esteves, D., \u0141awrynowicz, A., Panov, P., Soldatova, L., Soru, T., ... &amp; Zafar, H. (2018). ML-schema: exposing the semantics of machine learning with schemas and ontologies. arXiv preprint arXiv:1807.05351. Link - http://ml-schema.github.io/documentation/ML%20Schema.html</li> <li>Nguyen, A., Weller, T., F\u00e4rber, M., &amp; Sure-Vetter, Y. (2020). Making neural networks fair. In Knowledge Graphs and Semantic Web: Second Iberoamerican Conference and First Indo-American Conference, KGSWC 2020, M\u00e9rida, Mexico, November 26\u201327, 2020, Proceedings 2 (pp. 29-44). Springer International Publishing. Link - https://arxiv.org/pdf/1907.11569.pdf</li> <li>Humm, B. G., &amp; Zender, A. (2021). An ontology-based concept for meta AutoML. In Artificial Intelligence Applications and Innovations: 17th IFIP WG 12.5 International Conference, AIAI 2021, Hersonissos, Crete, Greece, June 25\u201327, 2021, Proceedings 17 (pp. 117-128). Springer International Publishing. Link - https://www.researchgate.net/profile/Alexander-Zender-2/publication/352574909_An_Ontology-Based_Concept_for_Meta_AutoML/links/619691e107be5f31b796d2fd/An-Ontology-Based-Concept-for-Meta-AutoML.pdf</li> </ul>"},{"location":"examples/getting_started/","title":"Using <code>Cmf</code> to track metadata for a ML Pipeline","text":"<p>example-get-started demonstrates how <code>Cmf</code> tracks metadata associated with executions of various machine learning (ML) pipelines. ML pipelines differ from other pipelines (e.g., data Extract-Transform-Load pipelines) by the presence of ML stages, such as training and testing ML models. </p> <p>More comprehensive ML pipelines may include stages such as deploying a trained model and tracking its inference parameters (such as response latency, memory consumption, etc.).</p> <p>This example, implements a simple pipeline consisting of five stages:</p> <ul> <li>The parse stage splits   the raw data into   <code>train</code> and <code>test</code> raw datasets for training and testing a machine learning model. This stage registers one   input artifact (raw <code>dataset</code>) and two output artifacts (train and test <code>datasets</code>).</li> <li>The featurize   stage creates two machine learning splits - train and test splits - that will be used by an ML training algorithm to   train ML models. This stage registers two input artifacts (raw train and test datasets) and two output artifacts (   train and test ML datasets).</li> <li>The next train stage   trains an ML model (random forest classifier). It registers one input artifact (the dataset from the previous step)   and one output artifact (trained ML model).</li> <li>The fourth test stage   evaluates the performance and execution of the ML model trained in the <code>train</code> step. This stage registers two input   artifacts (ML model and test dataset) and one output artifact (performance metrics).</li> <li>The last query stage   displays each stage of the pipeline's metadata as retrieved from the <code>CMF Server</code>, aggregated over all executions.   For example, if you rerun the pipeline again, the output will include not only metadata associated with the latest   run, but also the metadata associated with previous runs.</li> </ul>"},{"location":"examples/getting_started/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure that the <code>cmflib</code> is installed on your system. If not, follow the installation instructions provided in the Installation &amp; Setup page.</p> <p>The initial setup requires creating a workspace directory that will contain all files for this example, cloning the <code>cmf</code> repository that contains source code and data for this example.</p> <pre><code># Create workspace directory\nmkdir cmf_example\ncd cmf_example\n\n# Clone the CMF project from GitHub and install CMF\ngit clone https://github.com/HewlettPackard/cmf\n</code></pre>"},{"location":"examples/getting_started/#project-initialization","title":"Project initialization","text":"<p>First, copy the code and data for this example into its own directory (that must be outside the <code>cmf</code> source tree). Execute the <code>cmf init</code> command specifying the Data Version Control (dvc) directory, the URL of the git remote, address of the <code>CMF Server</code>, and neo4j credentials along with the appropriate dvc backend for this project.</p> <pre><code># Create a separate copy of the example project\ncp -r ./cmf/examples/example-get-started/ ./example-get-started\ncd ./example-get-started\n</code></pre>"},{"location":"examples/getting_started/#cmf-init","title":"cmf init","text":"<p><pre><code>Usage: cmf init local [-h] --path [path]\n                           --git-remote-url [git_remote_url]\n                           --cmf-server-url [cmf_server_url]\n                           --neo4j-user [neo4j_user]\n                           --neo4j-password [neo4j_password]\n                           --neo4j-uri [neo4j_uri]\n</code></pre> <code>cmf init local</code> initializes the local directory as a cmf artifact repository. <pre><code>cmf init local --path /home/XXXX/local-storage\n               --git-remote-url https://github.com/user/experiment-repo.git\n               --cmf-server-url http://x.x.x.x:80\n               --neo4j-user neo4j\n               --neo4j-password password\n               --neo4j-uri bolt://localhost:7687\n</code></pre></p> <p>For path provide a folder outside the current working directory, which would serve as the artifact repository. eg :-  /home/username/local-storage</p> <p>Required Arguments <pre><code>  --path [path]                         Specify local directory path.\n  --git-remote-url [git_remote_url]     Specify git repo url.\n</code></pre> Optional Arguments <pre><code>  -h, --help                          show this help message and exit\n  --cmf-server-url [cmf_server_url]   Specify CMF Server URL. (default: http://127.0.0.1:80)\n  --neo4j-user [neo4j_user]           Specify neo4j user. (default: None)\n  --neo4j-password [neo4j_password]   Specify neo4j password. (default: None)\n  --neo4j-uri [neo4j_uri]             Specify neo4j uri. Eg bolt://localhost:7687 (default: None)\n</code></pre> Follow here for more details.</p>"},{"location":"examples/getting_started/#project-execution","title":"Project execution","text":"<p>To execute the example pipeline, run the test_script.sh file. In brief, this script runs a sequence of stages typical of machine learning pipelines - getting raw data, splitting that data into machine learning train/test datasets, training the model, and evaluating a model. The execution of these steps (and parent pipeline) will be recorded by the <code>cmf</code>. <pre><code># Run the example pipeline\nsh ./test_script.sh\n</code></pre></p>"},{"location":"examples/getting_started/#setup-a-cmf-server","title":"Setup a <code>CMF Server</code>","text":"<p>Note: This setup step is not required if the CMF Server is already configured.</p> <p>CMF Server is a key interface for users to explore and track their ML training runs, allowing them to store metadata files on the CMF Server. Users can retrieve saved metadata files and view their content using the UI provided by the CMF Server.</p> <p>Follow here to set up a common CMF Server.</p>"},{"location":"examples/getting_started/#syncing-metadata-on-the-cmf-server","title":"Syncing metadata on the <code>CMF Server</code>","text":"<p>Metadata generated at each step of the pipeline will be stored in a sqlite file named mlmd. Commits in this repository correspond to the creation of pipeline artifacts and can be viewed with <code>git log</code>.</p> <p>In production settings, the next steps would be to:</p> <ol> <li>Execute the <code>cmf artifact push</code> command to push the artifacts to the central artifact repository.</li> <li>Execute the <code>cmf metadata push</code> command to track the metadata of the generated artifacts on a common CMF Server.</li> </ol> <p>Follow cmf artifact and cmf metadata for more details.</p>"},{"location":"examples/getting_started/#query","title":"Query","text":"<p>The stored metadata can be explored using the query layer of <code>cmf</code>. The Jupyter notebook Query_Tester-base_mlmd.ipynb demonstrates this functionality and can be adapted for your own uses.</p>"},{"location":"examples/getting_started/#clean-up","title":"Clean Up","text":"<p>Metadata is stored in a sqlite file named \"mlmd\". To clean up, delete the \"mlmd\" file.</p>"},{"location":"examples/getting_started/#steps-to-test-dataslice","title":"Steps to test dataslice","text":"<p>Run the following command: <code>python test-data-slice.py</code>.</p>"},{"location":"setup/","title":"CMF Installation &amp; Setup Guide","text":"<p>This guide provides step-by-step instructions for installing, configuring, and using CMF (Common Metadata Framework) for ML pipeline metadata tracking.</p>"},{"location":"setup/#overview","title":"Overview","text":"<p>The installation process consists of the following components:</p> <ol> <li>CMFLib: A Python library that captures and tracks metadata throughout your ML pipeline, including datasets, models, and metrics.</li> <li>CMF Server with GUI: A centralized server that aggregates metadata from multiple clients and provides a web-based graphical interface for visualizing pipeline executions, artifacts, and lineage relationships.</li> </ol> <p>Note: Every CMF setup requires a CMF Server instance. In collaborative environments, multiple users working on the same project can share a single CMF Server to centralize metadata and facilitate team coordination.</p>"},{"location":"setup/#prerequisites","title":"Prerequisites","text":"<p>Before installing CMF, ensure you have the following prerequisites:</p>"},{"location":"setup/#common-prerequisites","title":"Common Prerequisites","text":"<ul> <li> <p>Linux/Ubuntu/Debian</p> </li> <li> <p>Python: Version 3.9 to 3.11 (3.10 recommended)</p> <p>Note: If you encounter issues with Python 3.9 on Ubuntu, refer to the Troubleshooting section at the end of this guide.</p> </li> </ul>"},{"location":"setup/#cmf-client-prerequisites","title":"CMF Client Prerequisites","text":"<ul> <li> <p>Git: Latest version for code versioning</p> <p>Make sure Git is properly configured using <code>git config</code>, as it's required for the product. At minimum, set your user identity:</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\n</code></pre> </li> <li> <p>Storage Backend: S3, MinIOS3, ssh storage, OSDF or local storage for artifacts.</p> </li> </ul>"},{"location":"setup/#cmf-server-prerequisites","title":"CMF Server Prerequisites","text":"<ul> <li> <p>Docker: For containerized deployment of <code>CMF Server</code> and <code>CMF UI</code></p> <ol> <li>Install Docker Engine with non-root user privileges.</li> <li>Install Docker Compose Plugin.</li> </ol> <p>In earlier versions of Docker Compose, <code>docker compose</code> was independent of Docker. Hence, <code>docker-compose</code> was the command. However, after the introduction of Docker Compose Desktop V2, the compose command became part of Docker Engine. The recommended way to install Docker Compose is by installing a Docker Compose plugin on Docker Engine. For more information - Docker Compose Reference.</p> </li> <li> <p>Docker Proxy Settings: Needed for some of the server packages</p> <p>Refer to the official Docker documentation for comprehensive instructions: Configure the Docker Client for Proxy.</p> </li> </ul>"},{"location":"setup/#installation","title":"Installation","text":""},{"location":"setup/#install-cmf-library-ie-cmflib","title":"1. Install cmf library i.e. CMFLib","text":""},{"location":"setup/#step-1-set-up-python-virtual-environment","title":"Step 1: Set up Python Virtual Environment","text":"Using CondaUsing VirtualEnv <pre><code>conda create -n cmf python=3.10\nconda activate cmf\n</code></pre> <pre><code>virtualenv --python=3.10 .cmf\nsource .cmf/bin/activate\n</code></pre>"},{"location":"setup/#step-2-install-cmflib","title":"Step 2: Install CMFLib","text":"Latest version from GitHubStable version from PyPI <pre><code>pip install git+https://github.com/HewlettPackard/cmf\n</code></pre> <pre><code>pip install cmflib\n</code></pre>"},{"location":"setup/#install-cmf-server-with-gui","title":"2. Install CMF Server with GUI","text":"<p>Note: Every CMF setup requires a CMF Server instance. In collaborative environments, multiple users working on the same project can share a single CMF Server to centralize metadata and facilitate team coordination.</p>"},{"location":"setup/#prerequisites-check","title":"Prerequisites Check","text":"<p>Ensure that Docker is installed on your machine, as mentioned in the CMF Server Prerequisites. If not, please install it before proceeding.</p>"},{"location":"setup/#installation-steps","title":"Installation Steps","text":"<p>Step 1: Clone the GitHub Repository</p> <pre><code>git clone https://github.com/HewlettPackard/cmf\n</code></pre> <p>Step 2: Navigate to the CMF Directory</p> <pre><code>cd cmf\n</code></pre> <p>Step 3: Create Environment Configuration</p> <p>Create a <code>.env</code> file in the same directory as <code>docker-compose-server.yml</code> with the following environment variables:</p> <pre><code>CMF_DATA_DIR=./data                    \nNGINX_HTTP_PORT=80                  \nNGINX_HTTPS_PORT=443\nREACT_APP_CMF_API_URL=http://your-server-ip:80\n</code></pre> <p>\ud83d\udcdd Note:  - <code>CMF_DATA_DIR</code> controls where all data (PostgreSQL, TensorBoard logs, etc.) is stored. Use an absolute path for better control. - <code>REACT_APP_CMF_API_URL</code> should point to your server's accessible address.</p> <p>Step 4: Start the Containers</p> <p>\ud83d\udca1 Recommended Approach: Using <code>docker compose</code> starts the <code>CMF Server</code>, PostgreSQL database, and <code>CMF UI</code> together.</p> <p>Note: It's essential to start the PostgreSQL database before the <code>CMF Server</code>.</p> <pre><code>docker compose -f docker-compose-server.yml up\n</code></pre> <p>\ud83d\udcdd Note: Replace <code>docker compose</code> with <code>docker-compose</code> if you're using an older version of Docker.</p> <p>This command starts all services:</p> <ul> <li>PostgreSQL: Database backend for metadata storage</li> <li>CMF Server: API server for metadata management</li> <li>UI: Web interface for visualization</li> <li>TensorBoard: For viewing ML training metrics</li> <li>Nginx: Reverse proxy serving all components</li> </ul>"},{"location":"setup/#accessing-the-cmf-ui","title":"Accessing the CMF UI","text":"<p>Once the containers are successfully started, the CMF UI will be available at the URL specified in your <code>.env</code> file:</p> <pre><code>http://your-server-ip:80\n</code></pre> <p>Replace <code>your-server-ip</code> with the actual IP address or hostname configured in the <code>REACT_APP_CMF_API_URL</code> environment variable.</p> <p>\ud83d\udcdd Note: Ensure that port 80 (or your configured <code>NGINX_HTTP_PORT</code>) is accessible and not blocked by firewall rules.</p> <p>Step 5: Stop the Containers</p> <pre><code>docker compose -f docker-compose-server.yml stop\n</code></pre>"},{"location":"setup/#important-notes","title":"Important Notes","text":"<p>\ud83d\udca1 Rebuild Required:  Rebuild the images for <code>CMF Server</code> and <code>CMF UI</code> after a CMF version update or pulling the latest changes from Git to ensure compatibility.</p> <pre><code>docker compose -f docker-compose-server.yml build --no-cache\ndocker compose -f docker-compose-server.yml up\n</code></pre>"},{"location":"setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"setup/#python-39-installation-issues-on-ubuntu","title":"Python 3.9 Installation Issues on Ubuntu","text":"<p>If you are using Python 3.9 on Ubuntu systems, you may encounter installation or virtual environment issues.</p> <p>Issue: When creating Python 3.9 virtual environments, you may encounter:</p> <pre><code>ModuleNotFoundError: No module named 'distutils.cmd'\n</code></pre> <p>Root Cause: Python 3.9 may be missing required modules like <code>distutils</code> or <code>venv</code> when installed on Ubuntu systems.</p> <p>Resolution:</p> <ol> <li>Add the deadsnakes PPA (provides newer Python versions):</li> </ol> <pre><code>sudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt-get update\n</code></pre> <ol> <li>Install Python 3.9 with required modules:</li> </ol> <pre><code>sudo apt install python3.9 python3.9-dev python3.9-distutils python3.9-venv\n</code></pre> <ol> <li>Verify the installation:</li> </ol> <pre><code>python3.9 --version\npython3.9 -m venv test_env\n</code></pre> <p>This ensures Python 3.9 and its essential modules are fully installed and functional.</p> <p>\ud83d\udca1 Recommendation: If you're starting fresh, we recommend using Python 3.10 to avoid these compatibility issues.</p>"},{"location":"ui/","title":"Getting Started with CMF GUI","text":"<p>The CMF GUI provides an intuitive, browser-based interface for exploring ML pipeline metadata, visualizing lineage relationships, and synchronizing metadata between multiple CMF servers. Built with React and D3.js, it offers interactive dashboards for artifacts, executions, and pipeline lineage.</p>"},{"location":"ui/#overview","title":"Overview","text":"<p>The CMF GUI consists of several main sections:</p> <ul> <li>Artifacts: Browse and search datasets, models, and metrics</li> <li>Executions: View pipeline runs and execution history</li> <li>Lineage: Visualize data flow and dependencies</li> <li>Metahub: Synchronize metadata between CMF servers</li> <li>TensorBoard: View ML training metrics</li> </ul>"},{"location":"ui/#quick-start","title":"Quick Start","text":""},{"location":"ui/#accessing-the-cmf-gui","title":"Accessing the CMF GUI","text":"<ol> <li>Ensure the CMF Server is running</li> <li>Open your browser and navigate to the server URL (default: <code>http://your-server-ip:80</code>)</li> <li>The GUI will display the available pipelines in the sidebar</li> </ol>"},{"location":"ui/#lineage-visualization","title":"Lineage Visualization","text":"<p>The Lineage page offers interactive visualizations of data flow and dependencies in your ML pipelines. It provides following different visualization modes:</p>"},{"location":"ui/#visualization-types","title":"Visualization Types","text":"<ol> <li>Artifact Tree: Hierarchical view of artifact dependencies</li> <li>Execution Tree: Hierarchical view of execution flow</li> <li>Artifact-Execution Tree: Combined view showing both artifacts and executions</li> </ol>"},{"location":"ui/#metahub","title":"Metahub","text":"<p>The Metahub feature enables synchronization of metadata between two CMF servers, allowing distributed teams to collaborate and share ML pipeline metadata.</p> <p>\ud83d\udc49 Read more about Metahub</p>"},{"location":"ui/#tensorboard-integration","title":"TensorBoard Integration","text":"<p>CMF integrates with TensorBoard to visualize training metrics, model graphs, and other ML-specific visualizations alongside CMF metadata.</p> <p>\ud83d\udc49 Read more about TensorBoard Integration</p>"},{"location":"ui/#prerequisites","title":"Prerequisites","text":"<p>Before using the CMF GUI, ensure you have:</p> <ol> <li>CMF Server Running: Follow the installation guide</li> <li>Metadata Pushed: Use <code>cmf metadata push</code> to send metadata to server</li> <li>Browser Compatibility: Modern browser (Chrome, Firefox, Safari, Edge)</li> </ol>"},{"location":"ui/#getting-help","title":"Getting Help","text":"<ul> <li>For API details, see CMFLib Documentation</li> <li>For CLI commands, see CMF Client Commands</li> <li>For server setup, see Installation &amp; Setup</li> </ul>"},{"location":"ui/artifacts/","title":"Artifacts Page","text":"<p>The Artifacts page provides a comprehensive interface for exploring all datasets, models, and metrics tracked by CMF across your ML pipelines. This page enables users to search, filter, and analyze artifacts with detailed metadata and version history.</p>"},{"location":"ui/artifacts/#overview","title":"Overview","text":"<p>Artifacts represent the data entities in your ML pipeline: - Datasets: Training data, test data, validation sets, feature matrices - Models: Trained ML models, model checkpoints, exported models - Metrics: Performance metrics, evaluation results, quality measures</p> <pre><code>graph TB\n    subgraph \"Artifacts Page Architecture\"\n        UI[\"React UI&lt;br/&gt;Artifacts Page\"]\n        API[\"FastAPI Backend&lt;br/&gt;/artifact_types&lt;br/&gt;/display_artifacts\"]\n        MLMD[(\"PostgreSQL&lt;br/&gt;MLMD Database\")]\n        STORAGE[(\"Artifact Storage&lt;br/&gt;MinIO/S3/Local\")]\n    end\n\n    UI --&gt;|\"Fetch artifact types\"| API\n    UI --&gt;|\"Fetch artifacts with filters\"| API\n    API --&gt;|\"Query metadata\"| MLMD\n    MLMD -.-&gt;|\"Artifact references\"| STORAGE</code></pre>"},{"location":"ui/artifacts/#page-features","title":"Page Features","text":""},{"location":"ui/artifacts/#1-filter-panel","title":"1. Filter Panel","text":"<p>The filter panel allows you to narrow down artifacts based on multiple criteria:</p> Filter Type Description Options Pipeline Name Filter by specific pipeline Dropdown of all available pipelines Artifact Type Filter by artifact category Dataset, Model, Metrics Custom Properties Filter by user-defined metadata Based on tracked properties Time Range Filter by creation date Date range picker <p>Usage: 1. Select a pipeline from the dropdown to view its artifacts 2. Choose artifact type (Dataset/Model/Metrics) from tabs 3. Use search box for full-text search across artifact names and properties</p>"},{"location":"ui/artifacts/#2-artifacts-table","title":"2. Artifacts Table","text":"<p>The main table displays artifacts with the following columns:</p> Column Description Sortable Name Artifact name and identifier \u2713 Type Artifact type (Dataset/Model/Metrics) \u2713 URI Artifact location/path \u2717 Pipeline Associated pipeline name \u2713 Stage Pipeline stage that created it \u2713 Created Timestamp of creation \u2713 Custom Properties User-defined metadata \u2717 <p>Interactions: - Click on artifact name: Opens detailed view with full metadata - Click column headers: Sort by that column (ascending/descending) - Pagination controls: Navigate through large artifact lists</p>"},{"location":"ui/artifacts/#3-artifact-details-view","title":"3. Artifact Details View","text":"<p>Clicking on an artifact name opens a detailed panel showing:</p>"},{"location":"ui/artifacts/#basic-information","title":"Basic Information","text":"<ul> <li>Artifact ID: Unique identifier in MLMD</li> <li>Name: Human-readable artifact name</li> <li>Type: Dataset, Model, or Metrics</li> <li>URI: Full path to artifact location</li> <li>Content Hash: SHA256 hash for version identification</li> <li>Created Time: Timestamp of artifact creation</li> <li>Last Updated: Timestamp of last modification</li> </ul>"},{"location":"ui/artifacts/#pipeline-context","title":"Pipeline Context","text":"<ul> <li>Pipeline Name: Parent pipeline</li> <li>Stage/Context: Pipeline stage that produced this artifact</li> <li>Execution ID: Specific execution that created it</li> <li>Git Commit: Code version at creation time</li> </ul>"},{"location":"ui/artifacts/#custom-properties","title":"Custom Properties","text":"<p>User-defined metadata tracked via CMF API: <pre><code># Example: Properties visible in GUI\nartifact.custom_properties = {\n    \"framework\": \"tensorflow\",\n    \"version\": \"2.10.0\",\n    \"accuracy\": \"0.95\",\n    \"dataset_size\": \"10000\",\n    \"tags\": \"production,v1.2\"\n}\n</code></pre></p>"},{"location":"ui/artifacts/#version-history","title":"Version History","text":"<p>For artifacts with multiple versions: - List of all versions with timestamps - Content hash for each version - Associated executions for each version - Diff view showing property changes between versions</p>"},{"location":"ui/artifacts/#using-the-artifacts-page","title":"Using the Artifacts Page","text":""},{"location":"ui/artifacts/#example-1-find-all-models-from-a-pipeline","title":"Example 1: Find All Models from a Pipeline","text":"<ol> <li>Navigate to Artifacts page from the sidebar</li> <li>Select your pipeline from the Pipeline dropdown</li> <li>Click on the Model tab to filter by model artifacts</li> <li>Review the list of all models created by that pipeline</li> <li>Click on a model name to view training parameters and metrics</li> </ol>"},{"location":"ui/artifacts/#example-2-compare-dataset-versions","title":"Example 2: Compare Dataset Versions","text":"<ol> <li>Search for your dataset name in the search box</li> <li>Click on the dataset to open details</li> <li>Scroll to Version History section</li> <li>Compare content hashes and properties across versions</li> <li>Identify which executions used which version</li> </ol>"},{"location":"ui/artifacts/#example-3-track-metrics-over-time","title":"Example 3: Track Metrics Over Time","text":"<ol> <li>Select Metrics artifact type tab</li> <li>Sort by Created column to view chronologically</li> <li>Click on metrics artifacts to view values</li> <li>Compare metrics across different model versions</li> </ol>"},{"location":"ui/artifacts/#api-integration","title":"API Integration","text":""},{"location":"ui/artifacts/#backend-endpoints-used","title":"Backend Endpoints Used","text":"<p>The Artifacts page interacts with these CMF Server endpoints:</p> <pre><code># Get list of pipelines\nGET /display_pipelines\n\n# Get artifact types for a pipeline\nGET /artifact_types?pipeline_name={pipeline}\n\n# Get artifacts with filtering\nGET /display_artifacts?pipeline_name={pipeline}&amp;type={type}\n\n# Get artifact lineage\nGET /artifact-lineage/force-directed-graph/{pipeline}\n</code></pre>"},{"location":"ui/artifacts/#query-example","title":"Query Example","text":"<p>Using <code>CmfQuery</code> to retrieve artifact data programmatically:</p> <pre><code>from cmflib.cmfquery import CmfQuery\n\n# Initialize query object\nquery = CmfQuery(mlmd_path=\"/path/to/mlmd\")\n\n# Get all artifacts by type\ndatasets = query.get_all_artifacts_by_type(\"Dataset\")\nmodels = query.get_all_artifacts_by_type(\"Model\")\nmetrics = query.get_all_artifacts_by_type(\"Metrics\")\n\n# Get artifacts for specific pipeline\npipeline_id = query.get_pipeline_id(\"my-pipeline\")\nartifacts = query.get_all_artifacts_for_pipeline(pipeline_id)\n\n# Get artifact by name\nartifact = query.get_artifact(\"trained_model.pkl\")\nprint(f\"Artifact ID: {artifact['id']}\")\nprint(f\"URI: {artifact['uri']}\")\nprint(f\"Custom Properties: {artifact['custom_properties']}\")\n</code></pre>"},{"location":"ui/artifacts/#advanced-features","title":"Advanced Features","text":""},{"location":"ui/artifacts/#search-functionality","title":"Search Functionality","text":"<p>The search box supports: - Full-text search: Searches across artifact names, URIs, and properties - Wildcards: Use <code>*</code> for pattern matching (e.g., <code>model_*_final</code>) - Property search: Search by custom property values</p>"},{"location":"ui/artifacts/#pagination","title":"Pagination","text":"<p>For large artifact collections: - Items per page: Configurable (default: 50) - Page navigation: First, Previous, Next, Last buttons - Jump to page: Direct page number input - Total count: Displays total number of matching artifacts</p>"},{"location":"ui/artifacts/#export-functionality","title":"Export Functionality","text":"<p>Export artifact metadata: - CSV Export: Download table as CSV file - JSON Export: Download as structured JSON - Filtered Export: Only exports currently filtered artifacts</p>"},{"location":"ui/artifacts/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li>Use Descriptive Names: Name artifacts clearly (e.g., <code>train_dataset_v1.2</code> instead of <code>data.csv</code>)</li> <li>Add Custom Properties: Track important metadata like accuracy, dataset size, framework version</li> <li>Tag Artifacts: Use tags in properties for easy filtering (e.g., <code>tags: production,validated</code>)</li> <li>Regular Cleanup: Archive or remove outdated artifacts to keep lists manageable</li> <li>Version Naming: Use consistent version naming schemes (e.g., <code>v1.0</code>, <code>v1.1</code>, <code>v2.0</code>)</li> </ol>"},{"location":"ui/artifacts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ui/artifacts/#artifacts-not-appearing","title":"Artifacts Not Appearing","text":"<p>Issue: Expected artifacts don't show in the list</p> <p>Solutions: 1. Check pipeline selection - ensure correct pipeline is selected 2. Verify artifact type filter - check if filtering by wrong type 3. Check if metadata was pushed: <code>cmf metadata push -p pipeline-name</code> 4. Refresh the page to reload data from server</p>"},{"location":"ui/artifacts/#slow-loading","title":"Slow Loading","text":"<p>Issue: Artifacts page takes long to load</p> <p>Solutions: 1. Use filters to reduce dataset size 2. Increase pagination size to reduce number of requests 3. Check CMF Server logs for performance issues 4. Consider archiving old artifacts to a separate database</p>"},{"location":"ui/artifacts/#missing-properties","title":"Missing Properties","text":"<p>Issue: Custom properties not visible</p> <p>Solutions: 1. Verify properties were logged during artifact creation:    <pre><code>cmf.log_dataset(\"data.csv\", \"input\", \n                custom_properties={\"size\": \"1000\"})\n</code></pre> 2. Check if properties were pushed to server 3. Ensure property keys don't contain special characters</p>"},{"location":"ui/artifacts/#related-pages","title":"Related Pages","text":"<ul> <li>Executions Page - View pipeline runs that created these artifacts</li> <li>Lineage Page - Visualize artifact dependencies and data flow</li> <li>CMF Client Commands - CLI for artifact management</li> <li>Installation &amp; Setup - Set up CMF Server and GUI</li> </ul>"},{"location":"ui/executions/","title":"Executions Page","text":"<p>The Executions page provides detailed insights into pipeline runs, showing when stages executed, what parameters were used, and what artifacts were produced. This page is essential for debugging pipeline issues, comparing runs, and understanding execution history.</p>"},{"location":"ui/executions/#overview","title":"Overview","text":"<p>An Execution represents a single run of a pipeline stage. Each execution captures: - Configuration: Parameters and environment settings - Code Version: Git commit that was executed - Artifacts: Input and output artifacts - Metadata: Runtime information and custom properties - Lineage: Relationships to other executions and artifacts</p> <pre><code>graph LR\n    subgraph \"Execution Lifecycle\"\n        START[\"Pipeline Stage&lt;br/&gt;Triggered\"]\n        EXEC[\"Execution&lt;br/&gt;Created\"]\n        TRACK[\"Metadata&lt;br/&gt;Tracked\"]\n        ARTIFACTS[\"Artifacts&lt;br/&gt;Logged\"]\n        COMPLETE[\"Execution&lt;br/&gt;Completed\"]\n    end\n\n    START --&gt; EXEC\n    EXEC --&gt; TRACK\n    TRACK --&gt; ARTIFACTS\n    ARTIFACTS --&gt; COMPLETE\n\n    style EXEC fill:#e1f5ff\n    style COMPLETE fill:#c8e6c9</code></pre>"},{"location":"ui/executions/#page-features","title":"Page Features","text":""},{"location":"ui/executions/#1-execution-filters","title":"1. Execution Filters","text":"<p>Filter executions to find specific runs:</p> Filter Description Usage Pipeline Name Select specific pipeline Dropdown with all pipelines Execution Type Filter by stage name e.g., \"train\", \"test\", \"prepare\" Date Range Filter by execution time Date picker for start/end dates Status Filter by completion status Running, Completed, Failed Git Branch Filter by code branch Dropdown of tracked branches"},{"location":"ui/executions/#2-executions-table","title":"2. Executions Table","text":"<p>The main table displays execution runs with these columns:</p> Column Description Information Execution ID Unique identifier Numeric ID from MLMD Name Execution name Stage name + unique suffix Pipeline Parent pipeline Pipeline name Context/Stage Pipeline stage e.g., \"train\", \"evaluate\" Status Execution state Running/Completed/Failed Started Start timestamp When execution began Duration Execution time Time taken to complete Git Commit Code version Short commit hash Actions Quick actions View details, View lineage <p>Table Interactions: - Click execution name: Opens detailed execution view - Sort columns: Click headers to sort by that field - Multi-select: Select multiple executions for comparison - Pagination: Navigate through execution history</p>"},{"location":"ui/executions/#3-execution-details-view","title":"3. Execution Details View","text":"<p>Clicking on an execution opens a comprehensive details panel:</p>"},{"location":"ui/executions/#metadata-section","title":"Metadata Section","text":"<pre><code>Execution Information:\n  ID: 42\n  Name: train_model_a3f9\n  Type: Train\n  Pipeline: mnist-classifier\n  Context: training-stage\n  Status: Completed\n\nTiming:\n  Created: 2026-01-21 14:23:10 UTC\n  Started: 2026-01-21 14:23:15 UTC\n  Completed: 2026-01-21 14:28:43 UTC\n  Duration: 5m 28s\n</code></pre>"},{"location":"ui/executions/#git-information","title":"Git Information","text":"<pre><code>Code Version:\n  Commit Hash: a3f9e2b7\n  Branch: feature/improved-model\n  Repository: https://github.com/user/ml-project\n  Author: john.doe@example.com\n  Commit Message: \"Improved model architecture\"\n</code></pre>"},{"location":"ui/executions/#execution-properties","title":"Execution Properties","text":"<p>Custom properties logged during execution:</p> <pre><code># Properties visible in GUI\nExecution Properties:\n  learning_rate: 0.001\n  batch_size: 32\n  epochs: 50\n  optimizer: adam\n  python_version: 3.10.0\n  cuda_version: 11.7\n  gpu_used: Tesla V100\n</code></pre>"},{"location":"ui/executions/#input-artifacts","title":"Input Artifacts","text":"<p>List of artifacts consumed by this execution:</p> Artifact Name Type Version URI train_dataset.csv Dataset v1.2 s3://bucket/data/train.csv val_dataset.csv Dataset v1.2 s3://bucket/data/val.csv config.json Metrics v1.0 /local/config.json"},{"location":"ui/executions/#output-artifacts","title":"Output Artifacts","text":"<p>List of artifacts produced by this execution:</p> Artifact Name Type Version URI trained_model.pkl Model v2.3 s3://bucket/models/model.pkl training_metrics.json Metrics v2.3 s3://bucket/metrics.json model_checkpoint.h5 Model v2.3 s3://bucket/checkpoints/ckpt.h5"},{"location":"ui/executions/#events-timeline","title":"Events Timeline","text":"<p>Chronological list of events during execution:</p> <pre><code>gantt\n    title Execution Timeline\n    dateFormat HH:mm:ss\n    section Data Loading\n    Load training data    :14:23:15, 30s\n    Load validation data  :14:23:45, 15s\n    section Training\n    Initialize model      :14:24:00, 10s\n    Training epochs       :14:24:10, 4m\n    section Evaluation\n    Validate model        :14:28:10, 20s\n    Save artifacts        :14:28:30, 13s</code></pre>"},{"location":"ui/executions/#using-the-executions-page","title":"Using the Executions Page","text":""},{"location":"ui/executions/#example-1-debug-a-failed-execution","title":"Example 1: Debug a Failed Execution","text":"<ol> <li>Navigate to Executions page</li> <li>Filter by Status: \"Failed\"</li> <li>Select the most recent failed execution</li> <li>Review Execution Properties for parameter issues</li> <li>Check Input Artifacts to verify data availability</li> <li>Examine Git Commit to identify code changes</li> <li>Compare with previous successful execution</li> </ol>"},{"location":"ui/executions/#example-2-compare-training-runs","title":"Example 2: Compare Training Runs","text":"<ol> <li>Select pipeline from dropdown</li> <li>Filter by Execution Type: \"train\"</li> <li>Select multiple training executions (use checkboxes)</li> <li>Click Compare button</li> <li>View side-by-side comparison of:</li> <li>Hyperparameters</li> <li>Training metrics</li> <li>Output models</li> <li>Execution duration</li> </ol>"},{"location":"ui/executions/#example-3-track-experiment-history","title":"Example 3: Track Experiment History","text":"<ol> <li>Select your experiment pipeline</li> <li>Sort by Started column (descending) for latest first</li> <li>Review each execution's properties to see parameter variations</li> <li>Click on execution to view detailed metrics</li> <li>Use View Lineage action to see data flow</li> </ol>"},{"location":"ui/executions/#execution-states","title":"Execution States","text":""},{"location":"ui/executions/#status-indicators","title":"Status Indicators","text":"Status Icon Meaning Actions Available Running \ud83d\udd04 Execution in progress View logs, Monitor Completed \u2705 Successfully finished View results, Compare Failed \u274c Execution failed Debug, Retry, View logs Cached \ud83d\udcbe Reused from cache View original, Use results Pending \u23f3 Waiting to start Cancel, View queue"},{"location":"ui/executions/#status-transitions","title":"Status Transitions","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Pending\n    Pending --&gt; Running : Start Execution\n    Running --&gt; Completed : Success\n    Running --&gt; Failed : Error\n    Running --&gt; Cached : Cache Hit\n    Failed --&gt; Running : Retry\n    Cached --&gt; [*]\n    Completed --&gt; [*]\n    Failed --&gt; [*]</code></pre>"},{"location":"ui/executions/#advanced-features","title":"Advanced Features","text":""},{"location":"ui/executions/#execution-comparison","title":"Execution Comparison","text":"<p>Compare multiple executions side-by-side:</p> <ol> <li>Select executions: Check boxes next to 2-5 executions</li> <li>Click Compare: Button appears when multiple selected</li> <li>View differences:</li> <li>Parameter differences highlighted</li> <li>Metric deltas calculated</li> <li>Performance comparison charts</li> <li>Artifact version differences</li> </ol>"},{"location":"ui/executions/#execution-search","title":"Execution Search","text":"<p>Advanced search capabilities:</p> <pre><code># Search by property\nproperty:learning_rate=0.001\n\n# Search by date range\ndate:2026-01-01 TO 2026-01-31\n\n# Search by git commit\ncommit:a3f9e2b7\n\n# Combined search\npipeline:mnist AND status:completed AND property:epochs&gt;10\n</code></pre>"},{"location":"ui/executions/#export-execution-data","title":"Export Execution Data","text":"<p>Export options: - CSV Export: Table data with properties - JSON Export: Complete execution metadata - Report Generation: PDF report with charts - Batch Export: Export multiple executions at once</p>"},{"location":"ui/executions/#api-integration","title":"API Integration","text":""},{"location":"ui/executions/#backend-endpoints","title":"Backend Endpoints","text":"<p>Executions page uses these CMF Server endpoints:</p> <pre><code># Get all executions for a pipeline\nGET /execution_types?pipeline_name={pipeline}\n\n# Get execution details\nGET /display_executions?pipeline_name={pipeline}&amp;type={type}\n\n# Get execution lineage\nGET /execution-lineage/force-directed-graph/{pipeline}/{uuid}\n</code></pre>"},{"location":"ui/executions/#programmatic-access","title":"Programmatic Access","text":"<p>Query executions using CmfQuery:</p> <pre><code>from cmflib.cmfquery import CmfQuery\n\n# Initialize query\nquery = CmfQuery(mlmd_path=\"/path/to/mlmd\")\n\n# Get all executions for a pipeline\npipeline_id = query.get_pipeline_id(\"my-pipeline\")\nexecutions = query.get_all_executions_in_pipeline(pipeline_id)\n\n# Get executions by stage\nstage_id = query.get_pipeline_stage(\"my-pipeline\", \"train\")\ntrain_executions = query.get_all_executions_in_stage(stage_id)\n\n# Get execution by ID\nexecution = query.get_execution_by_id(42)\nprint(f\"Execution: {execution['name']}\")\nprint(f\"Status: {execution['status']}\")\nprint(f\"Properties: {execution['custom_properties']}\")\n\n# Get artifacts for execution\ninput_artifacts = query.get_all_artifacts_for_execution(42, \"INPUT\")\noutput_artifacts = query.get_all_artifacts_for_execution(42, \"OUTPUT\")\n</code></pre>"},{"location":"ui/executions/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"ui/executions/#execution-metrics-dashboard","title":"Execution Metrics Dashboard","text":"<p>The executions page can display aggregate metrics:</p> Metric Description Usage Success Rate % of successful executions Track pipeline reliability Avg Duration Average execution time Identify performance trends Failure Rate % of failed executions Monitor pipeline health Throughput Executions per day/hour Capacity planning"},{"location":"ui/executions/#alerts-and-notifications","title":"Alerts and Notifications","text":"<p>Set up alerts for: - Long Running Executions: Alert if exceeds expected duration - Failed Executions: Immediate notification on failures - Property Thresholds: Alert when metrics exceed limits - Resource Usage: Warning on high memory/CPU usage</p>"},{"location":"ui/executions/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li>Use Descriptive Names: Name executions clearly (e.g., <code>train_model_experiment_v3</code>)</li> <li>Log Parameters: Always log hyperparameters and configuration</li> <li>Track Git Commits: Ensure git commit is captured for reproducibility</li> <li>Add Custom Properties: Log environment details (GPU type, Python version)</li> <li>Regular Cleanup: Archive old executions to maintain performance</li> <li>Document Failures: Add notes to failed executions explaining issues</li> <li>Tag Important Runs: Use properties to tag production or baseline runs</li> </ol>"},{"location":"ui/executions/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ui/executions/#executions-not-appearing","title":"Executions Not Appearing","text":"<p>Issue: Expected executions don't show up</p> <p>Solutions: 1. Verify pipeline name is correct 2. Check if metadata was pushed: <code>cmf metadata push -p pipeline-name</code> 3. Refresh the page 4. Check execution type filter 5. Verify date range filter isn't excluding executions</p>"},{"location":"ui/executions/#missing-execution-details","title":"Missing Execution Details","text":"<p>Issue: Execution details are incomplete</p> <p>Solutions: 1. Ensure <code>cmf.create_execution()</code> was called 2. Verify properties were logged:    <pre><code>execution.log_execution_metrics_from_dict({\n    \"learning_rate\": 0.001,\n    \"batch_size\": 32\n})\n</code></pre> 3. Check if artifacts were properly logged 4. Ensure execution was finalized before querying</p>"},{"location":"ui/executions/#performance-issues","title":"Performance Issues","text":"<p>Issue: Page loads slowly with many executions</p> <p>Solutions: 1. Use filters to reduce dataset 2. Increase pagination size 3. Archive old executions 4. Enable database indexing on frequently queried fields 5. Consider database optimization</p>"},{"location":"ui/executions/#related-pages","title":"Related Pages","text":"<ul> <li>Artifacts Page - View artifacts produced by executions</li> <li>Lineage Page - Visualize execution flow and dependencies</li> <li>CMF Client Commands - CLI for execution management</li> <li>CMFLib API - Programmatic execution tracking</li> </ul>"},{"location":"ui/lineage/","title":"Lineage Visualization Page","text":"<p>The Lineage page provides interactive visualizations of data flow and dependencies in your ML pipelines. It helps you understand how artifacts and executions are connected, trace data provenance, and analyze pipeline structure.</p>"},{"location":"ui/lineage/#overview","title":"Overview","text":"<p>Lineage tracking captures the relationships between: - Artifacts: Datasets, models, and metrics - Executions: Pipeline stage runs - Data Flow: How data moves through pipeline stages - Dependencies: Which artifacts depend on which executions</p> <pre><code>graph LR\n    subgraph \"Lineage Concepts\"\n        DATA1[\"Dataset&lt;br/&gt;raw_data.csv\"]\n        EXEC1[\"Execution&lt;br/&gt;preprocess\"]\n        DATA2[\"Dataset&lt;br/&gt;clean_data.csv\"]\n        EXEC2[\"Execution&lt;br/&gt;train\"]\n        MODEL[\"Model&lt;br/&gt;trained_model.pkl\"]\n        EXEC3[\"Execution&lt;br/&gt;evaluate\"]\n        METRICS[\"Metrics&lt;br/&gt;accuracy.json\"]\n    end\n\n    DATA1 --&gt;|\"input\"| EXEC1\n    EXEC1 --&gt;|\"output\"| DATA2\n    DATA2 --&gt;|\"input\"| EXEC2\n    EXEC2 --&gt;|\"output\"| MODEL\n    MODEL --&gt;|\"input\"| EXEC3\n    EXEC3 --&gt;|\"output\"| METRICS\n\n    style DATA1 fill:#e3f2fd\n    style DATA2 fill:#e3f2fd\n    style MODEL fill:#fff3e0\n    style METRICS fill:#f3e5f5\n    style EXEC1 fill:#c8e6c9\n    style EXEC2 fill:#c8e6c9\n    style EXEC3 fill:#c8e6c9</code></pre>"},{"location":"ui/lineage/#visualization-types","title":"Visualization Types","text":"<p>The Lineage page offers four different visualization modes:</p>"},{"location":"ui/lineage/#1-artifact-tree","title":"1. Artifact Tree","text":"<p>Purpose: Hierarchical view of artifact dependencies</p> <p>Use Cases: - Understand data transformation pipeline - Trace dataset lineage from raw to final - Identify reused artifacts across stages</p> <p>Features: - Tree layout showing parent-child relationships - Color-coded by artifact type (Dataset/Model/Metrics) - Expandable/collapsible nodes - Hover for artifact details</p> <pre><code>graph TB\n    ROOT[\"raw_data.csv&lt;br/&gt;(Dataset)\"]\n    CLEAN[\"cleaned_data.csv&lt;br/&gt;(Dataset)\"]\n    TRAIN[\"train_split.csv&lt;br/&gt;(Dataset)\"]\n    TEST[\"test_split.csv&lt;br/&gt;(Dataset)\"]\n    MODEL[\"model_v1.pkl&lt;br/&gt;(Model)\"]\n    METRICS[\"eval_metrics.json&lt;br/&gt;(Metrics)\"]\n\n    ROOT --&gt; CLEAN\n    CLEAN --&gt; TRAIN\n    CLEAN --&gt; TEST\n    TRAIN --&gt; MODEL\n    MODEL --&gt; METRICS\n    TEST --&gt; METRICS\n\n    style ROOT fill:#e3f2fd\n    style CLEAN fill:#e3f2fd\n    style TRAIN fill:#e3f2fd\n    style TEST fill:#e3f2fd\n    style MODEL fill:#fff3e0\n    style METRICS fill:#f3e5f5</code></pre>"},{"location":"ui/lineage/#2-execution-tree","title":"2. Execution Tree","text":"<p>Purpose: Hierarchical view of execution dependencies</p> <p>Use Cases: - Understand pipeline execution flow - Debug pipeline stage ordering - Identify parallel vs sequential stages</p> <p>Features: - Select specific execution type from dropdown - Shows execution order and dependencies - Git commit info on hover - Execution status indicators</p> <p>Example View: <pre><code>pipeline_root\n\u251c\u2500\u2500 download_data\n\u2502   \u2514\u2500\u2500 outputs: raw_data.csv\n\u251c\u2500\u2500 preprocess\n\u2502   \u251c\u2500\u2500 inputs: raw_data.csv\n\u2502   \u2514\u2500\u2500 outputs: clean_data.csv\n\u251c\u2500\u2500 split_data\n\u2502   \u251c\u2500\u2500 inputs: clean_data.csv\n\u2502   \u2514\u2500\u2500 outputs: train.csv, test.csv\n\u251c\u2500\u2500 train_model (parallel)\n\u2502   \u251c\u2500\u2500 inputs: train.csv\n\u2502   \u2514\u2500\u2500 outputs: model.pkl\n\u2514\u2500\u2500 evaluate (parallel)\n    \u251c\u2500\u2500 inputs: model.pkl, test.csv\n    \u2514\u2500\u2500 outputs: metrics.json\n</code></pre></p>"},{"location":"ui/lineage/#3-artifact-execution-tree","title":"3. Artifact-Execution Tree","text":"<p>Purpose: Combined view showing both artifacts and executions</p> <p>Use Cases: - Complete end-to-end pipeline visualization - Understand which execution created which artifact - Trace full data lineage with transformations</p> <p>Features: - Alternating artifact and execution nodes - Shows input/output relationships - Complete provenance trail - Filtered by pipeline</p> <p>Visualization: <pre><code>graph TB\n    A1[\"\ud83d\udce6 raw_data.csv\"]\n    E1[\"\u2699\ufe0f preprocess\"]\n    A2[\"\ud83d\udce6 cleaned_data.csv\"]\n    E2[\"\u2699\ufe0f train\"]\n    A3[\"\ud83e\udd16 model.pkl\"]\n    E3[\"\u2699\ufe0f evaluate\"]\n    A4[\"\ud83d\udcca metrics.json\"]\n\n    A1 --&gt;|\"INPUT\"| E1\n    E1 --&gt;|\"OUTPUT\"| A2\n    A2 --&gt;|\"INPUT\"| E2\n    E2 --&gt;|\"OUTPUT\"| A3\n    A3 --&gt;|\"INPUT\"| E3\n    E3 --&gt;|\"OUTPUT\"| A4\n\n    style A1 fill:#e3f2fd\n    style A2 fill:#e3f2fd\n    style A3 fill:#fff3e0\n    style A4 fill:#f3e5f5\n    style E1 fill:#c8e6c9\n    style E2 fill:#c8e6c9\n    style E3 fill:#c8e6c9</code></pre></p>"},{"location":"ui/lineage/#4-force-directed-graph-legacy","title":"4. Force-Directed Graph (Legacy)","text":"<p>Purpose: Network graph showing all relationships</p> <p>Use Cases: - Explore complex pipeline relationships - Identify data sharing between stages - Find circular dependencies</p> <p>Features: - Interactive physics-based layout - Zoom and pan controls - Node dragging for custom layout - Multiple artifacts and executions visible</p>"},{"location":"ui/lineage/#page-interface","title":"Page Interface","text":""},{"location":"ui/lineage/#navigation-controls","title":"Navigation Controls","text":"<ol> <li>Pipeline Selector: Dropdown to choose which pipeline to visualize</li> <li>Lineage Type Tabs: Switch between visualization modes</li> <li>Artifact Tree</li> <li>Execution Tree</li> <li>Artifact-Execution Tree</li> <li>Execution Dropdown: (for Execution Tree) Select specific execution</li> <li>Zoom Controls: Zoom in/out of visualization</li> <li>Reset View: Reset zoom and pan to default</li> <li>Export: Save visualization as PNG or SVG</li> </ol>"},{"location":"ui/lineage/#visualization-controls","title":"Visualization Controls","text":"Control Function Shortcut Pan Click and drag background Mouse drag Zoom Scroll to zoom in/out Mouse wheel Node Click Show node details Left click Node Hover Quick preview Hover Reset View Reset to default view Double click Select Node Highlight path Click + Ctrl"},{"location":"ui/lineage/#node-information","title":"Node Information","text":"<p>Artifact Nodes show: - Artifact name - Type icon (\ud83d\udce6 Dataset, \ud83e\udd16 Model, \ud83d\udcca Metrics) - Version/hash (shortened) - Creation timestamp</p> <p>Execution Nodes show: - Execution name - Status icon (\u2705 Completed, \u274c Failed, \ud83d\udd04 Running) - Duration - Git commit (short hash)</p>"},{"location":"ui/lineage/#using-the-lineage-page","title":"Using the Lineage Page","text":""},{"location":"ui/lineage/#example-1-trace-data-provenance","title":"Example 1: Trace Data Provenance","text":"<p>Goal: Understand where a specific model's training data came from</p> <ol> <li>Navigate to Lineage page</li> <li>Select your pipeline from dropdown</li> <li>Choose Artifact Tree tab</li> <li>Find your trained model in the tree</li> <li>Trace backwards to see:</li> <li>Training dataset used</li> <li>Preprocessing steps applied</li> <li>Original raw data source</li> <li>Click on artifacts to see version details</li> </ol>"},{"location":"ui/lineage/#example-2-debug-pipeline-execution-order","title":"Example 2: Debug Pipeline Execution Order","text":"<p>Goal: Verify stages executed in correct sequence</p> <ol> <li>Select Execution Tree tab</li> <li>Choose the execution type from dropdown</li> <li>View the tree structure showing:</li> <li>Which stages ran first</li> <li>Which stages ran in parallel</li> <li>Dependencies between stages</li> <li>Check timestamps to verify timing</li> <li>Identify any out-of-order executions</li> </ol>"},{"location":"ui/lineage/#example-3-analyze-full-pipeline-flow","title":"Example 3: Analyze Full Pipeline Flow","text":"<p>Goal: Get complete picture of data flow through pipeline</p> <ol> <li>Select Artifact-Execution Tree tab</li> <li>View the alternating artifact \u2192 execution \u2192 artifact pattern</li> <li>Trace a specific data path:</li> <li>Start from input dataset</li> <li>Follow through each transformation</li> <li>End at final output (model/metrics)</li> <li>Hover on nodes to see details</li> <li>Click to navigate to artifact or execution page</li> </ol>"},{"location":"ui/lineage/#example-4-find-reused-artifacts","title":"Example 4: Find Reused Artifacts","text":"<p>Goal: Identify which artifacts are used by multiple executions</p> <ol> <li>Use Artifact Tree visualization</li> <li>Look for artifacts with multiple outgoing edges</li> <li>These artifacts are inputs to multiple stages</li> <li>Useful for understanding data sharing patterns</li> <li>Can help identify opportunities for caching</li> </ol>"},{"location":"ui/lineage/#advanced-features","title":"Advanced Features","text":""},{"location":"ui/lineage/#path-highlighting","title":"Path Highlighting","text":"<p>Click on a node to highlight the full path: - Upstream: Shows all ancestors (sources) - Downstream: Shows all descendants (consumers) - Complete Path: Full lineage from source to current node</p>"},{"location":"ui/lineage/#filtering-options","title":"Filtering Options","text":"<p>Filter the lineage graph: - By Artifact Type: Show only Datasets, Models, or Metrics - By Time Range: Show only recent executions - By Status: Show only completed/failed executions - By Branch: Filter by Git branch</p>"},{"location":"ui/lineage/#search-in-graph","title":"Search in Graph","text":"<p>Search for specific nodes: <pre><code>Search by name: \"train_data\"\nSearch by type: type:model\nSearch by property: property:accuracy&gt;0.9\nSearch by date: created:2026-01-21\n</code></pre></p>"},{"location":"ui/lineage/#graph-layouts","title":"Graph Layouts","text":"<p>Choose different layout algorithms: - Tree (Hierarchical): Top-down or left-right tree - Force-Directed: Physics-based natural layout - Radial: Circular layout with center node - Layered: Organized by pipeline stage layers</p>"},{"location":"ui/lineage/#api-endpoints","title":"API Endpoints","text":"<p>The Lineage page uses these CMF Server endpoints:</p> <pre><code># Artifact lineage (force-directed graph)\nGET /artifact-lineage/force-directed-graph/{pipeline}\n\n# Artifact tree (hierarchical)\nGET /artifact-lineage/tangled-tree/{pipeline}\n\n# Execution types list\nGET /list-of-executions/{pipeline}\n\n# Execution lineage\nGET /execution-lineage/force-directed-graph/{pipeline}/{uuid}\n\n# Execution tree\nGET /execution-lineage/tangled-tree/{uuid}/{pipeline}\n\n# Combined artifact-execution tree\nGET /artifact-execution-lineage/tangled-tree/{pipeline}\n</code></pre>"},{"location":"ui/lineage/#programmatic-lineage-queries","title":"Programmatic Lineage Queries","text":"<p>Query lineage using CmfQuery:</p> <pre><code>from cmflib.cmfquery import CmfQuery\n\n# Initialize query\nquery = CmfQuery(mlmd_path=\"/path/to/mlmd\")\n\n# Get all artifacts in a lineage\nartifact = query.get_artifact(\"trained_model.pkl\")\nlineage = query.get_artifact_lineage(artifact['id'])\n\n# Get upstream artifacts (inputs)\nupstream = lineage['upstream_artifacts']\nfor art in upstream:\n    print(f\"Input: {art['name']} (Type: {art['type']})\")\n\n# Get downstream artifacts (outputs)\ndownstream = lineage['downstream_artifacts']\nfor art in downstream:\n    print(f\"Output: {art['name']} (Type: {art['type']})\")\n\n# Get execution lineage\nexecution_id = 42\nexec_lineage = query.get_execution_lineage(execution_id)\nprint(f\"Input artifacts: {exec_lineage['inputs']}\")\nprint(f\"Output artifacts: {exec_lineage['outputs']}\")\n</code></pre>"},{"location":"ui/lineage/#lineage-data-structure","title":"Lineage Data Structure","text":"<p>The lineage visualizations use this data structure:</p> <pre><code>{\n  \"nodes\": [\n    {\n      \"id\": \"a_123\",\n      \"name\": \"artifact_name_train_data.csv\",\n      \"type\": \"artifact\",\n      \"artifact_type\": \"Dataset\",\n      \"uri\": \"s3://bucket/data/train.csv\",\n      \"created_at\": \"2026-01-21T10:30:00Z\"\n    },\n    {\n      \"id\": \"e_456\",\n      \"name\": \"execution_name_train_model\",\n      \"type\": \"execution\",\n      \"status\": \"completed\",\n      \"git_commit\": \"a3f9e2b7\",\n      \"duration\": 328\n    }\n  ],\n  \"links\": [\n    {\n      \"source\": \"a_123\",\n      \"target\": \"e_456\",\n      \"type\": \"INPUT\"\n    },\n    {\n      \"source\": \"e_456\",\n      \"target\": \"a_789\",\n      \"type\": \"OUTPUT\"\n    }\n  ]\n}\n</code></pre>"},{"location":"ui/lineage/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li>Start with Artifact Tree: Easiest to understand data flow</li> <li>Use Execution Tree for Debugging: Shows actual execution order</li> <li>Full Tree for Documentation: Artifact-Execution tree shows complete story</li> <li>Export Visualizations: Save lineage graphs for documentation</li> <li>Regular Review: Check lineage after pipeline changes</li> <li>Identify Bottlenecks: Long execution times visible in lineage</li> <li>Reusability Analysis: Find commonly reused artifacts</li> <li>Version Tracking: Use lineage to track artifact versions</li> </ol>"},{"location":"ui/lineage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ui/lineage/#empty-lineage-graph","title":"Empty Lineage Graph","text":"<p>Issue: No nodes appear in visualization</p> <p>Solutions: 1. Verify pipeline has executions 2. Check if metadata was pushed to server 3. Ensure artifacts were logged during execution 4. Refresh the page 5. Try different lineage type</p>"},{"location":"ui/lineage/#incomplete-lineage","title":"Incomplete Lineage","text":"<p>Issue: Some connections are missing</p> <p>Solutions: 1. Ensure all artifacts are logged:    <pre><code>cmf.log_dataset(\"data.csv\", \"input\")\ncmf.log_dataset(\"output.csv\", \"output\")\n</code></pre> 2. Verify execution completion 3. Check for errors in metadata push 4. Ensure proper input/output event logging</p>"},{"location":"ui/lineage/#performance-issues","title":"Performance Issues","text":"<p>Issue: Large graphs are slow to render</p> <p>Solutions: 1. Use filters to reduce complexity 2. Focus on specific pipeline stages 3. Use Tree view instead of Force-Directed 4. Consider breaking large pipelines into sub-pipelines 5. Increase browser memory allocation</p>"},{"location":"ui/lineage/#overlapping-nodes","title":"Overlapping Nodes","text":"<p>Issue: Nodes overlap and are hard to read</p> <p>Solutions: 1. Use zoom controls to spread out graph 2. Manually drag nodes to reposition 3. Try different layout algorithm 4. Export as SVG and edit externally 5. Increase visualization area size</p>"},{"location":"ui/lineage/#related-pages","title":"Related Pages","text":"<ul> <li>Artifacts Page - Detailed artifact information</li> <li>Executions Page - Execution details and logs</li> <li>CMF Client Commands - CLI for metadata management</li> <li>Installation &amp; Setup - Set up CMF Server</li> </ul>"},{"location":"ui/lineage/#additional-resources","title":"Additional Resources","text":""},{"location":"ui/lineage/#understanding-lineage-concepts","title":"Understanding Lineage Concepts","text":"<ul> <li>Provenance: History of an artifact's creation and transformations</li> <li>Upstream: Artifacts and executions that contributed to current node</li> <li>Downstream: Artifacts and executions that depend on current node</li> <li>Lineage Graph: Directed acyclic graph (DAG) of dependencies</li> </ul>"},{"location":"ui/lineage/#best-practices-for-lineage-tracking","title":"Best Practices for Lineage Tracking","text":"<ol> <li> <p>Always log inputs and outputs:    <pre><code># Log input\ncmf.log_dataset(\"input.csv\", \"input\")\n\n# Your processing logic\n\n# Log output\ncmf.log_dataset(\"output.csv\", \"output\")\n</code></pre></p> </li> <li> <p>Use descriptive names: Makes lineage graphs easier to understand</p> </li> <li> <p>Track all transformations: Don't skip intermediate artifacts</p> </li> <li> <p>Version consistently: Use semantic versioning for clarity</p> </li> <li> <p>Document pipelines: Add descriptions to help interpret lineage</p> </li> </ol>"}]}