{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started with cmf - NEEDS TO BE UPDATED","text":""},{"location":"#purpose-and-scope","title":"Purpose and Scope","text":"<p>This document provides a comprehensive overview of the Common Metadata Framework (CMF), which implements a system for collecting, storing, and querying metadata associated with Machine Learning (ML) pipelines. CMF adopts a data-first approach where all artifacts (datasets, ML models, and performance metrics) are versioned and identified by their content hash, enabling distributed metadata tracking and collaboration across ML teams.</p> <p>For detailed API documentation, see Core Library (cmflib). For server deployment instructions, see Installation &amp; Setup. For web user interface details, see cmf-gui.</p>"},{"location":"#system-architecture","title":"System Architecture","text":"<p>CMF is designed as a distributed system that enables ML teams to track pipeline metadata locally and synchronize with a central server. The framework automatically tracks code versions, data artifacts, and execution metadata to provide end-to-end traceability of ML experiments.</p> <p>Common Metadata Framework (CMF) has the following components:</p> <ul> <li>Metadata Library exposes APIs to track pipeline metadata. It also provides APIs to query the stored metadata.</li> <li>cmf-client interacts with the cmf-server to pull or push metadata.</li> <li>cmf-server with GUI interacts with remote cmf-clients and merges the metadata transferred by each   client. This server also provides a GUI that can render the stored metadata.</li> <li>Central Artifact Repositories host the code and data.</li> </ul> <pre><code>graph TB\n    subgraph \"Local Development Environment\"\n        CMF_CLIENT[\"**Metadata Library**&lt;br/&gt;cmflib.cmf.Cmf&lt;br/&gt;Main API Class\"]\n        CLI_TOOLS[\"**cmf-client**&lt;br/&gt;CLI Commands&lt;br/&gt;cmf init, push, pull\"]\n        LOCAL_MLMD[(\"Local MLMD&lt;br/&gt;SQLite Database\")]\n        DVC_GIT[\"DVC + Git&lt;br/&gt;Artifact Versioning\"]\n        NEO4J[(\"Neo4j&lt;br/&gt;Graph Database\")]\n    end\n\n    subgraph \"Central Infrastructure\"\n        CMF_SERVER[\"**cmf-server**&lt;br/&gt;FastAPI Application\"]\n        CENTRAL_MLMD[(\"PostgreSQL&lt;br/&gt;Central Metadata\")]\n        ARTIFACT_STORAGE[(\"Artifact Storage&lt;br/&gt;MinIO/S3/SSH\")]\n    end\n\n    subgraph \"Web Interface\"\n        REACT_UI[\"React Application&lt;br/&gt;Port 3000\"]\n        LINEAGE_VIZ[\"D3.js Lineage&lt;br/&gt;Visualization\"]\n        TENSORBOARD[\"TensorBoard&lt;br/&gt;Port 6006\"]\n    end\n\n    CMF_CLIENT --&gt; LOCAL_MLMD\n    CMF_CLIENT --&gt; DVC_GIT\n    CMF_CLIENT --&gt; NEO4J\n    CLI_TOOLS --&gt; CMF_SERVER\n    CMF_SERVER --&gt; CENTRAL_MLMD\n    DVC_GIT --&gt; ARTIFACT_STORAGE\n    REACT_UI --&gt; CMF_SERVER\n    REACT_UI --&gt; LINEAGE_VIZ\n    CMF_SERVER --&gt; TENSORBOARD</code></pre>"},{"location":"#core-abstractions","title":"Core Abstractions","text":"<p>CMF uses three primary abstractions to model ML pipeline metadata:</p> Abstraction Purpose Implementation Pipeline Groups related stages and executions Identified by name in <code>cmflib.cmf.Cmf</code> constructor Context Represents a stage type (e.g., \"train\", \"test\") Created via <code>create_context()</code> method Execution Represents a specific run of a stage Created via <code>create_execution()</code> method <pre><code>graph LR\n    PIPELINE[\"Pipeline&lt;br/&gt;'mnist_experiment'\"] --&gt; CONTEXT1[\"Context&lt;br/&gt;'download'\"]\n    PIPELINE --&gt; CONTEXT2[\"Context&lt;br/&gt;'train'\"]\n    PIPELINE --&gt; CONTEXT3[\"Context&lt;br/&gt;'test'\"]\n\n    CONTEXT1 --&gt; EXEC1[\"Execution&lt;br/&gt;'download_data'\"]\n    CONTEXT2 --&gt; EXEC2[\"Execution&lt;br/&gt;'train_model'\"]\n    CONTEXT3 --&gt; EXEC3[\"Execution&lt;br/&gt;'evaluate_model'\"]\n\n    EXEC1 --&gt; DATASET1[\"Dataset&lt;br/&gt;'raw_data.csv'\"]\n    EXEC2 --&gt; MODEL1[\"Model&lt;br/&gt;'trained_model.pkl'\"]\n    EXEC3 --&gt; METRICS1[\"Metrics&lt;br/&gt;'accuracy: 0.95'\"]</code></pre>"},{"location":"#component-architecture","title":"Component Architecture","text":""},{"location":"#cmf-library-cmflib","title":"CMF Library (<code>cmflib</code>)","text":"<p>The <code>cmflib</code> package provides the primary API for metadata tracking through the <code>Cmf</code> class and supporting modules:</p> <pre><code>graph TB\n    subgraph \"cmflib Package\"\n        CMF_CLASS[\"cmf.Cmf&lt;br/&gt;Main API Class\"]\n        METADATA_HELPER[\"metadata_helper.py&lt;br/&gt;MLMD Integration\"]\n        CMF_MERGER[\"cmf_merger.py&lt;br/&gt;Push/Pull Operations\"]\n        CMFQUERY[\"cmfquery.py&lt;br/&gt;Query Interface\"]\n        DATASLICE[\"dataslice.py&lt;br/&gt;Data Subset Tracking\"]\n    end\n\n    subgraph \"External Dependencies\"\n        MLMD[(\"ML Metadata&lt;br/&gt;SQLite/PostgreSQL\")]\n        DVC_SYSTEM[\"DVC&lt;br/&gt;Data Version Control\"]\n        GIT_SYSTEM[\"Git&lt;br/&gt;Code Version Control\"]\n        NEO4J_DB[(\"Neo4j&lt;br/&gt;Graph Database\")]\n    end\n\n    CMF_CLASS --&gt; METADATA_HELPER\n    CMF_CLASS --&gt; CMF_MERGER\n    CMF_CLASS --&gt; DATASLICE\n    METADATA_HELPER --&gt; MLMD\n    CMF_CLASS --&gt; DVC_SYSTEM\n    CMF_CLASS --&gt; GIT_SYSTEM\n    CMF_CLASS --&gt; NEO4J_DB\n    CMF_MERGER --&gt; CMFQUERY</code></pre>"},{"location":"#server-and-web-components","title":"Server and Web Components","text":"<p>The CMF server provides centralized metadata storage and a web interface for exploring ML pipeline lineage:</p> <pre><code>graph TB\n    subgraph \"cmf-server\"\n        FASTAPI_SERVER[\"FastAPI Server&lt;br/&gt;Port 8080\"]\n        GET_DATA[\"get_data.py&lt;br/&gt;Data Access Layer\"]\n        LINEAGE_QUERY[\"Lineage Query&lt;br/&gt;D3 Visualization\"]\n    end\n\n    subgraph \"UI Components\"\n        REACT_APP[\"React Application&lt;br/&gt;ui/ directory\"]\n        ARTIFACTS_PAGE[\"Artifacts Page&lt;br/&gt;Browse Datasets/Models\"]\n        EXECUTIONS_PAGE[\"Executions Page&lt;br/&gt;Browse Pipeline Runs\"]\n        LINEAGE_PAGE[\"Lineage Visualization&lt;br/&gt;D3.js Graphs\"]\n    end\n\n    subgraph \"Storage Layer\"\n        POSTGRES[(\"PostgreSQL&lt;br/&gt;Central MLMD\")]\n        TENSORBOARD_LOGS[(\"TensorBoard Logs&lt;br/&gt;Training Metrics\")]\n    end\n\n    FASTAPI_SERVER --&gt; GET_DATA\n    FASTAPI_SERVER --&gt; LINEAGE_QUERY\n    REACT_APP --&gt; FASTAPI_SERVER\n    REACT_APP --&gt; ARTIFACTS_PAGE\n    REACT_APP --&gt; EXECUTIONS_PAGE\n    REACT_APP --&gt; LINEAGE_PAGE\n    GET_DATA --&gt; POSTGRES\n    FASTAPI_SERVER --&gt; TENSORBOARD_LOGS</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#distributed-metadata-tracking","title":"Distributed Metadata Tracking","text":"<p>CMF enables distributed teams to work independently while maintaining consistent metadata through content-addressable artifacts and Git-like synchronization:</p> <ul> <li>Local Development: Each developer works with a local MLMD database</li> <li>Content Hashing: All artifacts are identified by their content hash for universal identification</li> <li>Synchronization: <code>cmf metadata push/pull</code> commands sync with central server</li> <li>Artifact Storage: Support for MinIO, Amazon S3, SSH, and local storage backends</li> </ul>"},{"location":"#automatic-version-tracking","title":"Automatic Version Tracking","text":"<p>CMF automatically captures:</p> <ul> <li>Code Version: Git commit IDs for reproducibility</li> <li>Data Version: DVC-managed artifact content hashes</li> <li>Environment: Execution parameters and custom properties</li> <li>Lineage: Input/output relationships between executions</li> </ul>"},{"location":"#query-and-visualization","title":"Query and Visualization","text":"<p>The system provides multiple interfaces for exploring metadata:</p> <ul> <li>Programmatic: <code>CmfQuery</code> class for custom queries</li> <li>Web UI: React-based interface for browsing artifacts and executions</li> <li>Lineage Graphs: D3.js visualizations showing data flow between pipeline stages</li> <li>TensorBoard Integration: Training metrics visualization</li> </ul>"},{"location":"_src/","title":"CMF docs development resources","text":"<p>This directory contains files that are used to create some content for the CMF documentation. This process is not automated yet. Files in this directory are not supposed to be referenced from documentation pages.</p> <p>It also should not be required to automatically redeploy documentation (e.g., with GitHub actions) when documentation files change only in this particular directory.</p> <ul> <li>The diagrams.drawio file is created with PyCharm's    Diagram.NET plugin. It contains a number of diagrams used in the documentation. Now,   to update those diagrams, use this file to edit them, them take a screenshot, edit with some editor, and then    overwrite corresponding files (e.g., ML Pipeline Definition) used on the main page.</li> </ul>"},{"location":"api/public/API/","title":"Complete API Documentation","text":""},{"location":"api/public/API/#logging-apis-needs-to-be-updated","title":"Logging API'S  NEEDS TO BE UPDATED","text":""},{"location":"api/public/API/#1-library-init-call-cmf","title":"1. Library init call - Cmf()","text":"<p>This calls initiates the library and also creates a pipeline object with the name provided. Arguments to be passed CMF: <pre><code>cmf = cmf.Cmf(filename=\"mlmd\", pipeline_name=\"Test-env\")\n\n# Returns a Context object of mlmd.proto.Context\n</code></pre></p> Argument Type Description filename String Path to the sqlite file to store the metadata pipeline_name String Name to uniquely identify the pipeline. Note that name is the unique identification for a pipeline. If a pipeline already exists with the same name, the existing pipeline object is reused custom_properties Dictionary (Optional) Additional properties of the pipeline that needs to be stored graph Bool (Optional) If set to true, the library also stores the relationships in the provided graph database. Following environment variables should be set: NEO4J_URI, NEO4J_USER_NAME, NEO4J_PASSWD <p>Return Object: <code>mlmd.proto.Context</code></p> Attribute Type Description create_time_since_epoch int64 Creation timestamp custom_properties repeated CustomPropertiesEntry Custom properties id int64 Unique identifier last_update_time_since_epoch int64 Last update timestamp name string Context name properties repeated PropertiesEntry Properties type string Context type type_id int64 Type identifier"},{"location":"api/public/API/#2-create_context-creates-a-stage-with-properties","title":"2. create_context - Creates a Stage with properties","text":"<p>A pipeline may include multiple stages. A unique name should be provided for every Stage in a pipeline. Arguments to be passed CMF: <pre><code>context = cmf.create_context(pipeline_stage=\"Prepare\", custom_properties={\"user-metadata1\":\"metadata_value\"})\n</code></pre></p> Argument Type Description pipeline_stage String Name of the pipeline Stage custom_properties Dictionary (Optional) Key value pairs of additional properties of the stage that needs to be stored <p>Return Object: <code>mlmd.proto.Context</code></p> Attribute Type Description create_time_since_epoch int64 Creation timestamp custom_properties repeated CustomPropertiesEntry Custom properties id int64 Unique identifier last_update_time_since_epoch int64 Last update timestamp name string Context name properties repeated PropertiesEntry Properties type string Context type type_id int64 Type identifier"},{"location":"api/public/API/#3-create_execution-creates-an-execution-with-properties","title":"3. create_execution - Creates an Execution with properties","text":"<p>A stage can have multiple executions. A unique name should ne provided for exery execution.  Properties of the execution can be paased as key value pairs in the custom properties. Eg: The hyper parameters used for the execution can be passed. <pre><code>execution = cmf.create_execution(execution_type=\"Prepare\",\n                                 custom_properties={\"Split\": split, \"Seed\": seed})\n\n# execution_type: String - Name of the execution\n# custom_properties: Dictionary (Optional Parameter)\n# Returns: Execution object of type mlmd.proto.Execution\n</code></pre></p> Argument Type Description execution_type String Name of the execution custom_properties Dictionary (Optional) Additional properties for the execution <p>Return Object: <code>mlmd.proto.Execution</code></p> Attribute Type Description create_time_since_epoch int64 Creation timestamp custom_properties repeated CustomPropertiesEntry Custom properties id int64 Unique identifier last_known_state State Last known execution state last_update_time_since_epoch int64 Last update timestamp name string Execution name properties repeated PropertiesEntry Properties (Git_Repo, Context_Type, Git_Start_Commit, Pipeline_Type, Context_ID, Git_End_Commit, Execution Command, Pipeline_id) type string Execution type type_id int64 Type identifier"},{"location":"api/public/API/#4-log_dataset-logs-a-dataset-and-its-properties","title":"4. log_dataset - Logs a Dataset and its properties","text":"<p>Tracks a Dataset and its version. The version of the  dataset is automatically obtained from the versioning software(DVC) and tracked as a metadata.  <pre><code>artifact = cmf.log_dataset(\"/repo/data.xml\", \"input\", custom_properties={\"Source\": \"kaggle\"})\n</code></pre></p> Argument Type Description url String The path to the dataset event String Takes arguments INPUT or OUTPUT custom_properties Dictionary The Dataset properties <p>Return Object: <code>mlmd.proto.Artifact</code></p> Attribute Type Description create_time_since_epoch int64 Creation timestamp custom_properties repeated CustomPropertiesEntry Custom properties id int64 Unique identifier last_update_time_since_epoch int64 Last update timestamp name string Artifact name properties repeated PropertiesEntry Properties (Commit, Git_Repo) state State Artifact state type string Artifact type type_id int64 Type identifier uri string Artifact URI"},{"location":"api/public/API/#5-log_model-logs-a-model-and-its-properties","title":"5. log_model - Logs a model and its properties.","text":"<pre><code>cmf.log_model(path=\"path/to/model.pkl\",\n              event=\"output\",\n              model_framework=\"SKlearn\",\n              model_type=\"RandomForestClassifier\",\n              model_name=\"RandomForestClassifier:default\")\n\n# Returns an Artifact object of type mlmd.proto.Artifact\n</code></pre> Argument Type Description path String Path to the model file event String Takes arguments INPUT or OUTPUT model_framework String Framework used to create model model_type String Type of Model Algorithm used model_name String Name of the Algorithm used custom_properties Dictionary The model properties <p>Return Object: <code>mlmd.proto.Artifact</code></p> Attribute Type Description create_time_since_epoch int64 Creation timestamp custom_properties repeated CustomPropertiesEntry Custom properties id int64 Unique identifier last_update_time_since_epoch int64 Last update timestamp name string Artifact name properties repeated PropertiesEntry Properties (commit, model_framework, model_type, model_name) state State Artifact state type string Artifact type type_id int64 Type identifier uri string Artifact URI"},{"location":"api/public/API/#6-log_execution_metrics-logs-the-metrics-for-the-execution","title":"6. log_execution_metrics Logs the metrics for the execution","text":"<pre><code>cmf.log_execution_metrics(metrics_name=\"Training_Metrics\", custom_properties={\"auc\": auc, \"loss\": loss})\n</code></pre> Arguments metrics_name String Name to identify the metrics custom_properties Dictionary Metrics"},{"location":"api/public/API/#7-log_metrics-logs-the-per-step-metrics-for-fine-grained-tracking","title":"7. log_metrics Logs the per Step metrics for fine grained tracking","text":"<p>The metrics provided is stored in a parquet file. The commit_metrics call add the parquet file in the version control framework. The metrics written in the parquet file can be retrieved using the read_metrics call <pre><code># Can be called at every epoch or every step in the training.\n# This is logged to a parquet file and committed at the commit stage.\nwhile True:  # Inside training loop\n    metawriter.log_metric(\"training_metrics\", {\"loss\": loss})\nmetawriter.commit_metrics(\"training_metrics\")\n</code></pre></p> Arguments for log_metric metrics_name String Name to identify the metrics custom_properties Dictionary Metrics Arguments for commit_metrics metrics_name String Name to identify the metrics"},{"location":"api/public/API/#8-create_dataslice","title":"8. create_dataslice","text":"<p>This helps to track a subset of the data. Currently supported only for file abstractions.  For eg- Accuracy of the model for a slice of data(gender, ethnicity etc) <pre><code>dataslice = cmf.create_dataslice(\"slice-a\")\n</code></pre></p> Arguments for create_dataslice name String Name to identify the dataslice Returns a Dataslice object"},{"location":"api/public/API/#9-add_data-adds-data-to-a-dataslice","title":"9. add_data Adds data to a dataslice.","text":"<p>Currently supported only for file abstractions. Pre condition - The parent folder, containing the file should already be versioned.  <pre><code>dataslice.add_data(\"data/raw_data/\" + str(j) + \".xml\")\n</code></pre></p> Arguments name String Name to identify the file to be added to the dataslice"},{"location":"api/public/API/#10-dataslice-commit-commits-the-created-dataslice","title":"10. Dataslice Commit - Commits the created dataslice","text":"<p>The created dataslice is versioned and added to underneath data versioning softwarre <pre><code>dataslice.commit()\n</code></pre></p>"},{"location":"api/public/cmf/","title":"cmflib.cmf","text":""},{"location":"api/public/cmf/#cmflib.cmf.Cmf","title":"<code>cmflib.cmf.Cmf(filepath='mlmd', pipeline_name='', custom_properties=None, graph=False, is_server=False)</code>","text":"<p>This class provides methods to log metadata for distributed AI pipelines. The class instance creates an ML metadata store to store the metadata. It creates a driver to store nodes and its relationships to neo4j. The user has to provide the name of the pipeline, that needs to be recorded with CMF.</p> <pre><code>cmflib.cmf.Cmf(\n    filepath=\"mlmd\",\n    pipeline_name=\"test_pipeline\",\n    custom_properties={\"owner\": \"user_a\"},\n    graph=False\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path  to the sqlite file to store the metadata</p> <code>'mlmd'</code> <code>pipeline_name</code> <code>str</code> <p>Name to uniquely identify the pipeline. Note that name is the unique identifier for a pipeline. If a pipeline already exist with the same name, the existing pipeline object is reused.</p> <code>''</code> <code>custom_properties</code> <code>Optional[Dict]</code> <p>Additional properties of the pipeline that needs to be stored.</p> <code>None</code> <code>graph</code> <code>bool</code> <p>If set to true, the libray also stores the relationships in the provided graph database.</p> <code>False</code> <p>The following variables should be set: <code>neo4j_uri</code> (graph server URI), <code>neo4j_user</code> (user name) and <code>neo4j_password</code> (user password), e.g.: <pre><code>cmf init local --path /home/user/local-storage --git-remote-url https://github.com/XXX/exprepo.git --neo4j-user neo4j --neo4j-password neo4j\n                        --neo4j-uri bolt://localhost:7687\n</code></pre></p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.create_context","title":"<code>create_context(pipeline_stage, custom_properties=None)</code>","text":"<p>Create's a  context(stage). Every call creates a unique pipeline stage. Updates Pipeline_stage name.</p> <pre><code>#Create context\n# Import CMF\nfrom cmflib.cmf import Cmf\nfrom ml_metadata.proto import metadata_store_pb2 as mlpb\n# Create CMF logger\ncmf = Cmf(filepath=\"mlmd\", pipeline_name=\"test_pipeline\")\n# Create context\ncontext: mlmd.proto.Context = cmf.create_context(\n    pipeline_stage=\"prepare\",\n    custom_properties ={\"user-metadata1\": \"metadata_value\"}\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_stage</code> <code>str</code> <p>Name of the Stage.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Developers can provide key value pairs with additional properties of the execution that need to be stored.</p> <code>None</code> <p>Returns:</p> Type Description <code>Context</code> <p>Context object from ML Metadata library associated with the new context for this stage.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.create_execution","title":"<code>create_execution(execution_type, custom_properties=None, cmd=None, create_new_execution=True)</code>","text":"<p>Create execution. Every call creates a unique execution. Execution can only be created within a context, so create_context must be called first.</p> <pre><code># Import CMF\nfrom cmflib.cmf import Cmf\nfrom ml_metadata.proto import metadata_store_pb2 as mlpb\n# Create CMF logger\ncmf = Cmf(filepath=\"mlmd\", pipeline_name=\"test_pipeline\")\n# Create or reuse context for this stage\ncontext: mlmd.proto.Context = cmf.create_context(\n    pipeline_stage=\"prepare\",\n    custom_properties ={\"user-metadata1\": \"metadata_value\"}\n)\n# Create a new execution for this stage run\nexecution: mlmd.proto.Execution = cmf.create_execution(\n    execution_type=\"Prepare\",\n    custom_properties = {\"split\": split, \"seed\": seed}\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>execution_type</code> <code>str</code> <p>Type of the execution.(when create_new_execution is False, this is the name of execution)</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Developers can provide key value pairs with additional properties of the execution that need to be stored.</p> <code>None</code> <code>cmd</code> <code>Optional[str]</code> <p>command used to run this execution.</p> <code>None</code> <code>create_new_execution</code> <code>bool</code> <p>bool = True, This can be used by advanced users to re-use executions This is applicable, when working with framework code like mmdet, pytorch lightning etc, where the custom call-backs are used to log metrics. if create_new_execution is True(Default), execution_type parameter will be used as the name of the execution type. if create_new_execution is False, if existing execution exist with the same name as execution_type. it will be reused. Only executions created with  create_new_execution as False will have \"name\" as a property.</p> <code>True</code> <p>Returns:</p> Type Description <code>Execution</code> <p>Execution object from ML Metadata library associated with the new execution for this stage.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.update_execution","title":"<code>update_execution(execution_id, custom_properties=None)</code>","text":"<p>Updates an existing execution. The custom properties can be updated after creation of the execution. The new custom properties is merged with earlier custom properties.</p> <pre><code># Import CMF\nfrom cmflib.cmf import Cmf\nfrom ml_metadata.proto import metadata_store_pb2 as mlpb\n# Create CMF logger\ncmf = Cmf(filepath=\"mlmd\", pipeline_name=\"test_pipeline\")\n# Update a execution\nexecution: mlmd.proto.Execution = cmf.update_execution(\n    execution_id=8,\n    custom_properties = {\"split\": split, \"seed\": seed}\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>execution_id</code> <code>int</code> <p>id of the execution.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Developers can provide key value pairs with additional properties of the execution that need to be updated.</p> <code>None</code> <p>Returns:</p> Type Description <code>Execution</code> <p>Execution object from ML Metadata library associated with the updated execution for this stage.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_dataset","title":"<code>log_dataset(url, event, custom_properties=None, label=None, label_properties=None, external=False)</code>","text":"<p>Logs a dataset as artifact. This call adds the dataset to dvc. The dvc metadata file created (.dvc) will be added to git and committed. The version of the  dataset is automatically obtained from the versioning software(DVC) and tracked as a metadata.</p> <pre><code>artifact: mlmd.proto.Artifact = cmf.log_dataset(\n    url=\"/repo/data.xml\",\n    event=\"input\",\n    custom_properties={\"source\":\"kaggle\"},\n    label=artifacts/labels.csv,\n    label_properties={\"user\":\"Ron\"}\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The path to the dataset.</p> required <code>event</code> <code>str</code> <p>Takes arguments <code>INPUT</code> OR <code>OUTPUT</code>.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Dataset properties (key/value pairs).</p> <code>None</code> <code>label</code> <code>Optional[str]</code> <p>Labels are usually .csv files containing information regarding the dataset.</p> <code>None</code> <code>label_properties</code> <code>Optional[Dict]</code> <p>Custom properties for a label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>Artifact object from ML Metadata library associated with the new dataset artifact.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_model","title":"<code>log_model(path, event, model_framework='Default', model_type='Default', model_name='Default', custom_properties=None)</code>","text":"<p>Logs a model. The model is added to dvc and the metadata file (.dvc) gets committed to git.</p> <pre><code>artifact: mlmd.proto.Artifact= cmf.log_model(\n    path=\"path/to/model.pkl\",\n    event=\"output\",\n    model_framework=\"SKlearn\",\n    model_type=\"RandomForestClassifier\",\n    model_name=\"RandomForestClassifier:default\"\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the model file.</p> required <code>event</code> <code>str</code> <p>Takes arguments <code>INPUT</code> OR <code>OUTPUT</code>.</p> required <code>model_framework</code> <code>str</code> <p>Framework used to create the model.</p> <code>'Default'</code> <code>model_type</code> <code>str</code> <p>Type of model algorithm used.</p> <code>'Default'</code> <code>model_name</code> <code>str</code> <p>Name of the algorithm used.</p> <code>'Default'</code> <code>custom_properties</code> <code>Optional[Dict]</code> <p>The model properties.</p> <code>None</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>Artifact object from ML Metadata library associated with the new model artifact.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_execution_metrics","title":"<code>log_execution_metrics(metrics_name, custom_properties=None)</code>","text":"<p>Log the metadata associated with the execution (coarse-grained tracking). It is stored as a metrics artifact. This does not have a backing physical file, unlike other artifacts that we have.</p> <pre><code>exec_metrics: mlpb.Artifact = cmf.log_execution_metrics(\n    metrics_name=\"Training_Metrics\",\n    {\"auc\": auc, \"loss\": loss}\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>metrics_name</code> <code>str</code> <p>Name to identify the metrics.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Dictionary with metric values.</p> <code>None</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>Artifact object from ML Metadata library associated with the new coarse-grained metrics artifact.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_metric","title":"<code>log_metric(metrics_name, custom_properties=None)</code>","text":"<p>Stores the fine-grained (per step or per epoch) metrics to memory. The metrics provided are stored in a parquet file. The <code>commit_metrics</code> call add the parquet file in the version control framework. The metrics written in the parquet file can be retrieved using the <code>read_metrics</code> call.</p> <pre><code># Can be called at every epoch or every step in the training. This is logged to a parquet file and committed\n# at the commit stage.\n# Inside training loop\nwhile True:\n        cmf.log_metric(\"training_metrics\", {\"train_loss\": train_loss})\ncmf.commit_metrics(\"training_metrics\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>metrics_name</code> <code>str</code> <p>Name to identify the metrics.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Dictionary with metrics.</p> <code>None</code>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.create_dataslice","title":"<code>create_dataslice(name)</code>","text":"<p>Creates a dataslice object. Once created, users can add data instances to this data slice with add_data method. Users are also responsible for committing data slices by calling the commit method.</p> <pre><code>dataslice = cmf.create_dataslice(\"slice-a\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name to identify the dataslice.</p> required <p>Returns:</p> Type Description <code>DataSlice</code> <p>Instance of a newly created DataSlice.</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.update_dataslice","title":"<code>update_dataslice(name, record, custom_properties)</code>","text":"<p>Updates a dataslice record in a Parquet file with the provided custom properties.</p> <pre><code>   dataslice=cmf.update_dataslice(\"dataslice_file.parquet\", \"record_id\", \n   {\"key1\": \"updated_value\"})\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the Parquet file.</p> required <code>record</code> <code>str</code> <p>Identifier of the dataslice record to be updated.</p> required <code>custom_properties</code> <code>Dict</code> <p>Dictionary containing custom properties to update.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_label","title":"<code>log_label(url, dataset_name, custom_properties=None)</code>","text":"<p>Logs a label artifact associated with a dataset.</p> <p>This function checks whether a label artifact (identified by <code>label_hash</code>) already exists in the metadata store. - If the artifact exists, it links it to the current execution and optionally updates its properties and URL. - If the artifact does not exist, it creates a new artifact with the provided properties and links it to the execution context.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The base URL representing the label (e.g., path or storage location).</p> required <code>dataset_name</code> <code>str</code> <p>The name of the associated dataset.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Additional metadata to associate with the artifact. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>mlpb.Artifact: The logged or linked label artifact.</p>"},{"location":"api/public/cmf/#cmflib.cmf","title":"<code>cmflib.cmf</code>","text":"<p>This module contains all the public API for CMF</p>"},{"location":"api/public/cmf/#cmflib.cmf.cmf_init_show","title":"<code>cmf_init_show()</code>","text":"<p>Initializes and shows details of the CMF command. </p> <pre><code>result = cmf_init_show() \n</code></pre> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _cmf_init_show function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.cmf_init","title":"<code>cmf_init(type='', path='', git_remote_url='', cmf_server_url='http://127.0.0.1:80', url='', endpoint_url='', access_key_id='', secret_key='', session_token='', user='', password='', port=0, osdf_path='', osdf_cache='', key_id='', key_path='', key_issuer='', neo4j_user=None, neo4j_password=None, neo4j_uri=None)</code>","text":"<p>Initializes the CMF configuration based on the provided parameters. </p> <pre><code>cmf_init( type=\"local\", \n            path=\"/path/to/re\",\n            git_remote_url=\"https://github.com/hpe-user/experiment-repo.git\",\n            cmf_server_url=\"http://cmf-server:80\",\n            neo4j_user=\"neo4j\",\n            neo4j_password=\"password\",\n            neo4j_uri=\"bolt://localhost:7687\"\n        )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>Required Arguments</code> required <code>- type</code> <p>Type of repository (\"local\", \"minioS3\", \"amazonS3\", \"sshremote\", \"osdfremote\")</p> required <code>- path</code> <p>Path for the local/ssh repository. </p> required <code>- git_remote_url</code> <p>Git remote URL for version control.</p> required <code>- url</code> <p>URL for MinioS3 or AmazonS3.</p> required <code>- endpoint_url</code> <p>Endpoint URL for MinioS3.</p> required <code>- access_key_id</code> <p>Access key ID for MinioS3 or AmazonS3.</p> required <code>- secret_key</code> <p>Secret key for MinioS3 or AmazonS3. </p> required <code>- session_token</code> <p>Session token for AmazonS3.</p> required <code>- user</code> <p>SSH remote username.</p> required <code>- password</code> <p>SSH remote password. </p> required <code>- port</code> <p>SSH remote port.</p> required <code>- osdf_path</code> <p>OSDF Origin Path.</p> required <code>- osdf_cache</code> <p>OSDF Cache Path (Optional).</p> required <code>- key_id</code> <p>OSDF Key ID.</p> required <code>- key_path</code> <p>OSDF Private Key Path.</p> required <code>- key_issuer</code> <p>OSDF Key Issuer URL.</p> required <code>Default Arguments</code> required <code>- cmf_server_url</code> <p>CMF server URL. - this is interesting as this has default value.</p> required <code>Optional Arguments</code> required <code>- neo4j_user</code> <p>Neo4j database username.</p> required <code>- neo4j_password</code> <p>Neo4j database password.</p> required <code>- neo4j_uri</code> <p>Neo4j database URI.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Output based on the initialized repository type.</p>"},{"location":"api/public/cmf/#cmflib.cmf.metadata_push","title":"<code>metadata_push(pipeline_name, file_name='./mlmd', tensorboard_path=None, execution_uuid=None)</code>","text":"<p>Pushes metadata file to CMF-server.</p> <pre><code>result = metadata_push(\"example_pipeline\", \"mlmd_file\", \"eg_execution_uuid\", \"tensorboard_log\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline.</p> required <code>file_name</code> <code>str</code> <p>Specify input metadata file name.</p> <code>'./mlmd'</code> <code>execution_uuid</code> <code>Optional[str]</code> <p>Optional execution UUID.</p> <code>None</code> <code>tensorboard_path</code> <code>Optional[str]</code> <p>Path to tensorboard logs.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Response output from the _metadata_push function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.metadata_pull","title":"<code>metadata_pull(pipeline_name, file_name='./mlmd', execution_uuid=None)</code>","text":"<p>Pulls metadata file from CMF-server. </p> <pre><code>result = metadata_pull(\"example_pipeline\", \"./mlmd_directory\", \"eg_execution_uuid\") \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline. </p> required <code>file_name</code> <code>str</code> <p>Specify output metadata file name.</p> <code>'./mlmd'</code> <code>execution_uuid</code> <code>Optional[str]</code> <p>Optional execution UUID. </p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Message from the _metadata_pull function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.metadata_export","title":"<code>metadata_export(pipeline_name, json_file_name=None, file_name='./mlmd')</code>","text":"<p>Export local mlmd's metadata in json format to a json file. </p> <pre><code>result = metadata_export(\"example_pipeline\", \"./jsonfile\", \"./mlmd_directory\") \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline. </p> required <code>json_file_name</code> <code>Optional[str]</code> <p>File path of json file. </p> <code>None</code> <code>file_name</code> <code>str</code> <p>Specify input metadata file name. </p> <code>'./mlmd'</code> <p>Returns:</p> Type Description <code>str</code> <p>Message from the _metadata_export function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.artifact_pull","title":"<code>artifact_pull(pipeline_name, file_name='./mlmd', artifact_name=None)</code>","text":"<p>Pulls artifacts from the initialized repository.</p> <pre><code>result = artifact_pull(\"example_pipeline\", \"./mlmd_directory\", \"artifact_name)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline.</p> required <code>file_name</code> <code>str</code> <p>Specify input metadata file name.</p> <code>'./mlmd'</code> <code>artifact_name</code> <code>Optional[str]</code> <p>Name of the artifact</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _artifact_pull function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.artifact_push","title":"<code>artifact_push(pipeline_name, filepath='./mlmd', jobs=32)</code>","text":"<p>Pushes artifacts to the initialized repository.</p> <pre><code>result = artifact_push(\"example_pipeline\", \"./mlmd_directory\", \"32\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline. </p> required <code>filepath</code> <code>str</code> <p>Path to store the artifact. </p> <code>'./mlmd'</code> <code>jobs</code> <code>int</code> <p>Number of jobs to use for pushing artifacts.</p> <code>32</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _artifact_push function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.artifact_list","title":"<code>artifact_list(pipeline_name, file_name='./mlmd', artifact_name=None)</code>","text":"<p>Displays artifacts from the input metadata file with a few properties in a 7-column table, limited to 20 records per page.</p> <pre><code>result = _artifact_list(\"example_pipeline\", \"./mlmd_directory\", \"example_artifact_name\") \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline. </p> required <code>file_name</code> <code>str</code> <p>Specify input metadata file name. </p> <code>'./mlmd'</code> <code>artifact_name</code> <code>Optional[str]</code> <p>Artifacts for particular artifact name.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _artifact_list function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.pipeline_list","title":"<code>pipeline_list(file_name='./mlmd')</code>","text":"<p>Display a list of pipeline name(s) from the available input metadata file.</p> <pre><code>result = _pipeline_list(\"./mlmd_directory\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Specify input metadata file name.</p> <code>'./mlmd'</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _pipeline_list function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.execution_list","title":"<code>execution_list(pipeline_name, file_name='./mlmd', execution_uuid=None)</code>","text":"<p>Displays executions from the input metadata file with a few properties in a 7-column table, limited to 20 records per page.</p> <pre><code>result = _execution_list(\"example_pipeline\", \"./mlmd_directory\", \"example_execution_uuid\") \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline. </p> required <code>file_name</code> <code>str</code> <p>Specify input metadata file name.</p> <code>'./mlmd'</code> <code>execution_uuid</code> <code>Optional[str]</code> <p>Specify the execution uuid to retrieve execution.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _execution_list function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.repo_push","title":"<code>repo_push(pipeline_name, filepath='./mlmd', tensorboard_path=None, execution_uuid=None, jobs=32)</code>","text":"<p>Push artifacts, metadata files, and source code to the user's artifact repository, cmf-server, and git respectively.</p> <pre><code>result = _repo_push(\"example_pipeline\", \"./mlmd_directory\", \"example_execution_uuid\", \"./tensorboard_path\", 32) \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline. </p> required <code>filepath</code> <code>str</code> <p>Specify input metadata file path.</p> <code>'./mlmd'</code> <code>execution_uuid</code> <code>Optional[str]</code> <p>Specify execution uuid.</p> <code>None</code> <code>tensorboard_path</code> <code>Optional[str]</code> <p>Path to tensorboard logs.</p> <code>None</code> <code>jobs</code> <code>int</code> <p>Number of jobs to use for pushing artifacts.</p> <code>32</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _repo_push function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.repo_pull","title":"<code>repo_pull(pipeline_name, file_name='./mlmd', execution_uuid=None)</code>","text":"<p>Pull artifacts, metadata files, and source code from the user's artifact repository, cmf-server, and git respectively.</p> <pre><code>result = _repo_pull(\"example_pipeline\", \"./mlmd_directory\", \"example_execution_uuid\") \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline. </p> required <code>file_name</code> <p>Specify output metadata file name.</p> <code>'./mlmd'</code> <code>execution_uuid</code> <code>Optional[str]</code> <p>Specify execution uuid.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _repo_pull function.</p>"},{"location":"api/public/cmf/#cmflib.cmf.dvc_ingest","title":"<code>dvc_ingest(file_name='./mlmd')</code>","text":"<p>Ingests metadata from the dvc.lock file into the CMF.      If an existing MLMD file is provided, it merges and updates execution metadata      based on matching commands, or creates new executions if none exist.</p> <pre><code>result = _dvc_ingest(\"./mlmd_directory\") \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Specify input metadata file name.</p> <code>'./mlmd'</code> <p>Returns:</p> Type Description <code>str</code> <p>Output from the _dvc_ingest function.</p>"},{"location":"api/public/cmf_ray_logger/","title":"cmflib.cmf_ray_logger.CmfRayLogger","text":""},{"location":"api/public/cmf_ray_logger/#cmfraylogger-user-guide","title":"CmfRayLogger User Guide","text":""},{"location":"api/public/cmf_ray_logger/#overview","title":"Overview","text":"<p>The <code>CmfRayLogger</code> class is designed to log Ray Tune metrics for the CMF (Common Metadata Framework). It tracks the performance and outputs of trials during the tuning process, directly linking metrics to stages of your CMF pipeline.</p>"},{"location":"api/public/cmf_ray_logger/#requirements","title":"Requirements","text":"<ul> <li>Ensure both <code>cmf</code> and <code>raytune</code> are installed on your system to use the <code>CmfRayLogger</code>.</li> </ul>"},{"location":"api/public/cmf_ray_logger/#installation","title":"Installation","text":"<p>To use <code>CmfRayLogger</code>, import it in your Python script:</p> <pre><code>from cmflib import cmf_ray_logger\n</code></pre>"},{"location":"api/public/cmf_ray_logger/#usage","title":"Usage","text":""},{"location":"api/public/cmf_ray_logger/#initialization","title":"Initialization","text":"<p>Create an instance of CmfRayLogger by providing the following parameters:</p> <ul> <li>pipeline_name: A string representing the name of the CMF pipeline.</li> <li>file_path: The file path to the metadata file associated with the CMF pipeline.</li> <li>pipeline_stage: The name of the current stage of the CMF pipeline.</li> <li>data_dir (optional): A directory path where trial data should be logged. If the path is within the CMF directory, it should be relative. If it is outside, it must be an absolute path. Default vale is <code>None</code>.</li> </ul> <p>Example of instantiation: <pre><code>logger = cmf_ray_logger.CmfRayLogger(pipeline_name, file_path, pipeline_stage. data_dir)\n</code></pre> Here, the <code>data_dir</code> argument is used to log the dataset at the start of each trial. Ensure that this path is relative if within the CMF directory and absolute if external to the CMF directory.</p>"},{"location":"api/public/cmf_ray_logger/#integration-with-ray-tune","title":"Integration with Ray Tune","text":"<p>After initializing the logger, it should be passed to Ray Tune\u2019s <code>tune.run</code> method via the <code>callbacks</code> parameter. This setup allows <code>CmfRayLogger</code> to log metrics for each trial based on the pipeline configuration and trial execution details.</p> <pre><code>from ray import tune\n\n# Example configuration for Ray Tune\nconfig = {\n    # Your configuration details\n}\n\ntune.run(\n    &lt;your_trainable&gt;,\n    config=config,\n    callbacks=[logger]\n)\n</code></pre>"},{"location":"api/public/cmf_ray_logger/#model-logging","title":"Model Logging","text":"<p><code>CmfRayLogger</code> can now log the model during trials. To enable this, the <code>train.report</code> method must include a special key: <code>\"model_path\"</code>. The value of <code>\"model_path\"</code> should be a relative path pointing to the saved model within the CMF directory.</p> <p>Important: Ensure that the <code>\"model_path\"</code> is relative, as the DVC wrapper expects all paths nested within the CMF directory to be relative. <pre><code>train.report({\n    \"accuracy\": 0.95,\n    \"loss\": 0.05,\n    \"model_path\": \"models/example_model.pth\"\n})\n</code></pre></p>"},{"location":"api/public/cmf_ray_logger/#output","title":"Output","text":"<p>During each trial, <code>CmfRayLogger</code> will automatically create a CMF object with attributes set as <code>pipeline_name</code>, <code>pipeline_stage</code>, and the CMF execution as <code>trial_id</code>. It captures the trial's output and logs it under the metric key <code>'Output'</code>. Additionally, it logs the dataset at the start of each trial (if data_dir is specified) and logs the model based on the <code>\"model_path\"</code> key in <code>train.report</code>.</p>"},{"location":"api/public/cmf_ray_logger/#example","title":"Example","text":"<p>Here is a complete example of how to use <code>CmfRayLogger</code> with Ray Tune:</p> <pre><code>from cmflib import cmf_ray_logger\nfrom ray import tune\n\n# Initialize the logger\nlogger = cmf_ray_logger.CmfRayLogger(\"ExamplePipeline\", \"/path/to/metadata.json\", \"Stage1\", \"path/to/data_dir\")\n\n# Configuration for tuning\nconfig = {\n    # Configuration details\n}\n\n# Execute the tuning process\ntune.run(\n    &lt;your_trainable&gt;,\n    config=config,\n    callbacks=[logger]\n)\n\n# Reporting within your trainable function\ntrain.report({\n    \"accuracy\": 0.95,\n    \"loss\": 0.05,\n    \"model_path\": \"path/to/models/example_model.pth\"\n})\n</code></pre>"},{"location":"api/public/cmfquery/","title":"cmflib.cmfquery.CmfQuery","text":""},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery","title":"<code>cmflib.cmfquery.CmfQuery(filepath='mlmd', is_server=False)</code>","text":"<p>               Bases: <code>object</code></p> <p>CMF Query communicates with the MLMD database and implements basic search and retrieval functionality.</p> <p>This class has been designed to work with the CMF framework. CMF alters names of pipelines, stages and artifacts in various ways. This means that actual names in the MLMD database will be different from those originally provided by users via CMF API. When methods in this class accept <code>name</code> parameters, it is expected that values of these parameters are fully-qualified names of respective entities.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the MLMD database file.</p> <code>'mlmd'</code>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_pipeline_names","title":"<code>get_pipeline_names()</code>","text":"<p>Return names of all pipelines.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of all pipeline names.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_pipeline_id","title":"<code>get_pipeline_id(pipeline_name)</code>","text":"<p>Return pipeline identifier for the pipeline names <code>pipeline_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Pipeline identifier or -1 if one does not exist.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_pipeline_stages","title":"<code>get_pipeline_stages(pipeline_name)</code>","text":"<p>Return list of pipeline stages for the pipeline with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline for which stages need to be returned. In CMF, there are no different pipelines with the same name.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of stage names associated with the given pipeline.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_exe_in_stage","title":"<code>get_all_exe_in_stage(stage_name)</code>","text":"<p>Return list of all executions for the stage with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Name of the stage. Before stages are recorded in MLMD, they are modified (e.g., pipeline name         will become part of the stage name). So stage names from different pipelines will not collide.</p> required <p>Returns:</p> Type Description <code>List[Execution]</code> <p>List of executions for the given stage.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_executions_by_ids_list","title":"<code>get_all_executions_by_ids_list(exe_ids)</code>","text":"<p>Return executions for given execution ids list as a pandas data frame.</p> <p>Parameters:</p> Name Type Description Default <code>exe_ids</code> <code>List[int]</code> <p>List of execution identifiers.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with all executions for the list of given execution identifiers.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_artifacts_by_context","title":"<code>get_all_artifacts_by_context(pipeline_name)</code>","text":"<p>Return artifacts for given pipeline name as a pandas data frame.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with all artifacts associated with given pipeline name.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_artifacts_by_ids_list","title":"<code>get_all_artifacts_by_ids_list(artifact_ids)</code>","text":"<p>Return all artifacts for the given artifact ids list.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_ids</code> <code>List[int]</code> <p>List of artifact identifiers</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with all artifacts for the given artifact ids list.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_executions_in_stage","title":"<code>get_all_executions_in_stage(stage_name)</code>","text":"<p>Return executions of the given stage as pandas data frame.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage name. See doc strings for the prev method.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with all executions associated with the given stage.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_artifact_df","title":"<code>get_artifact_df(artifact, d=None)</code>","text":"<p>Return artifact's data frame representation.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>MLMD entity representing artifact.</p> required <code>d</code> <code>Optional[Dict]</code> <p>Optional initial content for data frame.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A data frame with the single row containing attributes of this artifact.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_artifacts","title":"<code>get_all_artifacts()</code>","text":"<p>Return names of all artifacts.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of all artifact names.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_artifact","title":"<code>get_artifact(name)</code>","text":"<p>Return artifact's data frame representation using artifact name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Artifact name.</p> required <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>Pandas data frame with one row containing attributes of this artifact.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_artifacts_for_execution","title":"<code>get_all_artifacts_for_execution(execution_id)</code>","text":"<p>Return input and output artifacts for the given execution.</p> <p>Parameters:</p> Name Type Description Default <code>execution_id</code> <code>int</code> <p>Execution identifier.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame containing input and output artifacts for the given execution, one artifact per row.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_artifact_types","title":"<code>get_all_artifact_types()</code>","text":"<p>Return names of all artifact types.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of all artifact types.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_executions_for_artifact","title":"<code>get_all_executions_for_artifact(artifact_name)</code>","text":"<p>Return executions that consumed and produced given artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>Artifact name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pandas data frame containing stage executions, one execution per row.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_one_hop_child_artifacts","title":"<code>get_one_hop_child_artifacts(artifact_name, pipeline_id=None)</code>","text":"<p>Get artifacts produced by executions that consume given artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>Name of an artifact.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Output artifacts of all executions that consumed given artifact.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_child_artifacts","title":"<code>get_all_child_artifacts(artifact_name)</code>","text":"<p>Return all downstream artifacts starting from the given artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>Artifact name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame containing all child artifacts.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_one_hop_parent_artifacts","title":"<code>get_one_hop_parent_artifacts(artifact_name)</code>","text":"<p>Return input artifacts for the execution that produced the given artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>Artifact name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame containing immediate parent artifact of given artifact.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_parent_artifacts","title":"<code>get_all_parent_artifacts(artifact_name)</code>","text":"<p>Return all upstream artifacts.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>Artifact name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame containing all parent artifacts.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_all_parent_executions","title":"<code>get_all_parent_executions(artifact_name)</code>","text":"<p>Return all executions that produced upstream artifacts for the given artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>Artifact name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame containing all parent executions.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.get_metrics","title":"<code>get_metrics(metrics_name)</code>","text":"<p>Return metric data frame.</p> <p>Parameters:</p> Name Type Description Default <code>metrics_name</code> <code>str</code> <p>Metrics name.</p> required <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>Data frame containing all metrics.</p>"},{"location":"api/public/cmfquery/#cmflib.cmfquery.CmfQuery.dumptojson","title":"<code>dumptojson(pipeline_name, exec_uuid=None)</code>","text":"<p>Return JSON-parsable string containing details about the given pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Name of an AI pipelines.</p> required <code>exec_uuid</code> <code>Optional[str]</code> <p>Optional stage execution_uuid - filter stages by this execution_uuid.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Pipeline in JSON format.</p>"},{"location":"api/public/cmfserver/","title":"cmf-server","text":""},{"location":"api/public/cmfserver/#api-reference-needs-to-be-updated","title":"API Reference - NEEDS TO BE UPDATED","text":"<p>cmf-server APIs are organized around FastAPI. They accept and return JSON-encoded request bodies and responses and return standard HTTP response codes.</p>"},{"location":"api/public/cmfserver/#list-of-apis","title":"List of APIs","text":"Method URL Description <code>POST</code> <code>/mlmd_push</code> Pushes JSON-encoded data to the cmf-server. <code>GET</code> <code>/mlmd_pull/{pipeline_name}</code> Retrieves an MLMD file from the cmf-server. <code>GET</code> <code>/executions/{pipeline_name}</code> Retrieves all executions from the cmf-server. <code>GET</code> <code>/list-of-executions/{pipeline_name}</code> Retrieves a list of execution types. <code>GET</code> <code>/execution-lineage/tangled-tree/{uuid}/{pipeline_name}</code> Retrieves a dictionary of nodes and links for a given execution type. <code>GET</code> <code>/artifacts/{pipeline_name}/{type}</code> Retrieves all artifacts of the specified type from the cmf-server. <code>GET</code> <code>/artifact-lineage/tangled-tree/{pipeline_name}</code> Retrieves a nested list of dictionaries with <code>id</code> and <code>parents</code> keys for artifacts. <code>GET</code> <code>/artifact_types</code> Retrieves a list of artifact types. <code>GET</code> <code>/pipelines</code> Retrieves all pipelines present in the MLMD file. <code>POST</code> <code>/tensorboard</code> Uploads TensorBoard logs to the cmf-server. <code>GET</code> <code>/model-card</code> Retrieves model data, input/output artifacts, and executions for a model. <code>GET</code> <code>/artifact-execution-lineage/tangled-tree/{pipeline_name}</code> Retrieves a nested list of dictionaries with <code>id</code> and <code>parents</code> keys for artifacts and executions. <code>POST</code> <code>/python-env</code> Pushes Python environment data to the cmf-server. <code>GET</code> <code>/python-env</code> Retrieves environment data from the <code>/cmf-server/data/env</code> folder."},{"location":"api/public/cmfserver/#http-response-status-codes","title":"HTTP Response Status codes","text":"Code Title Description <code>200</code> <code>OK</code> mlmd is successfully pushed (e.g. when using <code>GET</code>, <code>POST</code>). <code>400</code> <code>Bad request</code> When the cmf-server is not available. <code>500</code> <code>Internal server error</code> When an internal error has happened"},{"location":"api/public/dataslice/","title":"cmflib.cmf.Cmf.DataSlice","text":""},{"location":"api/public/dataslice/#cmflib.cmf.Cmf.DataSlice","title":"<code>cmflib.cmf.Cmf.DataSlice(name, writer)</code>","text":"<p>A data slice represents a named subset of data. It can be used to track performance of an ML model on different slices of the training or testing dataset splits. This can be useful from different perspectives, for instance, to mitigate model bias.</p> <p>Instances of data slices are not meant to be created manually by users. Instead, use Cmf.create_dataslice method.</p>"},{"location":"api/public/dataslice/#cmflib.cmf.Cmf.DataSlice.add_data","title":"<code>add_data(path, custom_properties=None)</code>","text":"<p>Add data to create the dataslice. Currently supported only for file abstractions. Pre-condition - the parent folder, containing the file should already be versioned.</p> <pre><code>dataslice.add_data(f\"data/raw_data/{j}.xml)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Name to identify the file to be added to the dataslice.</p> required <code>custom_properties</code> <code>Optional[Dict]</code> <p>Properties associated with this datum.</p> <code>None</code>"},{"location":"api/public/dataslice/#cmflib.cmf.Cmf.DataSlice.commit","title":"<code>commit(custom_properties=None)</code>","text":"<p>Commit the dataslice. The created dataslice is versioned and added to underneath data versioning software.</p> <pre><code>dataslice.commit()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>custom_properties</code> <code>Optional[Dict]</code> <p>Dictionary to store key value pairs associated with Dataslice</p> <code>None</code> <p>Example {\"mean\":2.5, \"median\":2.6}</p>"},{"location":"architecture/","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>Interactions in data pipelines can be complex. The Different stages in the pipeline, (which may not be next to each other) may have to interact to produce or transform artifacts. </p> <p>As the artifacts navigates and undergo transformations through this pipeline, it can take a complicated path, which might also involve bidirectional movement across these stages.  Also there could be dependencies between the multiple stages, where the metrics produced by a stage could influence the metrics at a subsequent stage.  </p> <p>It is important to track the metadata across a pipeline to provide features like, lineage tracking, provenance and reproducibility.  </p> <p>The tracking of metadata through these complex pipelines have multiple challenges, some of them being,  </p> <ul> <li>Each stage in the pipeline could be executed in a different datacenter or an edge site having intermittent connection to the core datacenter.   </li> <li>Each stage in the pipeline could be possibly managed by different teams.  </li> <li>The artifacts (input or output) needs to be uniquely identified across different sites and across multiple pipelines. </li> </ul> <p>Common metadata framework (CMF) addresses the problems associated with tracking of pipeline metadata from distributed sites and tracks code, data and metadata together for end-to-end traceability.   </p> <p>The framework automatically tracks the code version as one of the metadata for an execution. Additionally the data artifacts are also versioned automatically using a data versioning framework (like DVC) and the metadata regarding the data version is stored along with the code. </p> <p>The framework stores the Git commit id of the metadata file associated with the artifact and content hash of the artifact as metadata. The framework provides APIs to track the hyper parameters and other metadata of pipelines.  Therefore from the metadata stored, users can zero in on the hyper parameters, code version and the artifact version used for the experiment. </p> <p>Identifying the artifacts by content hash allows the framework, to uniquely identify an artifact anywhere in the distributed sites. This enables the metadata from the distributed sites to be precisely merged to a central repository, thereby providing a single global metadata from the distributed sites.   </p> <p>On this backbone, we build the Git like experience for metadata, enabling users to push their local metadata to the remote repository, where it is merged to create the global metadata and pull metadata from the global metadata to the local, to create a local view, which would contain only the metadata of interest. </p> <p>The framework can be used to track various types of pipelines such as data pipelines or AI pipelines. </p> <p> </p>"},{"location":"architecture/advantages/","title":"Advantages","text":"<ol> <li>Tracking of metadata for distributed pipeline, thereby enabling efficient pipeline. </li> <li>Enables tracking of code, data and metadata in a single framework. </li> <li>Provides a git like ease of management for metadata.</li> <li>Provides collaboration across teams.</li> </ol>"},{"location":"architecture/components/","title":"Components","text":""},{"location":"architecture/components/#cmflib","title":"cmflib","text":"<p>The APIs and the abstractions provided by the <code>cmflib</code> enables tracking of pipeline metadata. </p> <p>It tracks the stages in the pipeline, the input and output artifacts at each stage and metrics. </p> <p>The framework allows metrics to be tracked both at coarse and fine grained intervals. It could be a stage metrics, which could be captured at the end of a stage or fine grained metrics which is tracked per step (epoch) or at regular intervals during the execution of the stage. </p> <p>The metadata logged through the APIs are written to a backend relational database. The <code>cmflib</code> also provides APIs to query the metadata stored in the relational database for the users to inspect pipelines.   </p> <p>In addition to explicit tracking through the APIs, <code>cmflib</code> also provides, implicit tracking. The implicit tracking automatically tracks the software version used in the pipelines. The function arguments and function return values can be automatically tracked by adding metadata tracker class decorators on the functions. </p> <p>Before writing the metadata to relational database, the metadata operations are journaled in the metadata journal log. This enables the framework to transfer the local metadata to the central server. </p> <p>All artifacts are versioned with a data versioning framework (for e.g., DVC). The content hash of the artifacts are generated and stored along with the user provided metadata. A special artifact metadata file called a \u201c.dvc\u201d file is created for every artifact (file / folder) which is added to data version management system. The .dvc file contains the content hash of the artifact.  </p> <p>For every new execution, the metadata tracker creates a new branch to track the code. The special metadata file created for artifacts, the \u201c.dvc\u201d file is also committed to GIT and its commit id is tracked as a metadata information.  The artifacts are versioned through the versioning of its metadata file. Whenever there is a change in the artifact, the metadata file is modified to reflect its current content hash, and the file is tracked as a new version of the metadata file.  </p> <p>The metadata tracker automatically tracks the start commit when the cmflib was initialized and creates separate commit for each change in the artifact along the experiment. This helps to track the transformations on the artifacts along the different stages in the pipeline. </p>"},{"location":"architecture/components/#cmf-client","title":"cmf-client","text":"<p>The <code>cmf-client</code> interacts with the metadata server aka <code>cmf-server</code>. It communicates with the <code>cmf-server</code> for the synchronization of metadata.  </p> <p>After the experiment is completed, the user invokes the <code>cmf push</code> command to push the collected metadata to the <code>cmf-server</code>. This transfers the existing metadata journal to the server.  </p> <p>The metadata from the central repository of metadata i.e. <code>cmf-server</code> can be pulled to the local repository, either using the artifacts or using the project as the identifier or both.</p> <p>When artifact is used as the identifier, all metadata associated with the artifacts currently present in the branch of the cloned Git repository is pulled from the central repository to the local repository. The pulled metadata consist of not only the immediate metadata associated with the artifacts, it contains the metadata of all the artifacts in its chain of lineage. </p> <p>When project is used as the identifier, all the metadata associated with the current branch of the pipeline code that is checked out is pulled to the local repository. </p>"},{"location":"architecture/components/#cmf-server","title":"cmf-server","text":"<p>The central server aka <code>cmf-server</code>, exposes REST APIs that can be called from the remote clients called <code>cmf-client</code>. This can help in situations where the connectivity between the core datacenter and the remote client is robust. The remote client calls the APIs exposed by the central server to log the metadata directly to the central metadata repository.  (We don't do this)</p> <p>Where the connectivity with the central server is intermittent, the remote clients log the metadata to the local repository. The journaled metadata is pushed by the remote client to the <code>cmf-server</code>. The central server, will replay the journal and merge the incoming metadata with the metadata already existing in the central repository. The ability to accurately identify the artifacts anywhere using their content hash, makes this merge robust. (this is not making sense)</p>"},{"location":"architecture/components/#central-repositories","title":"Central Repositories","text":"<p>The Common Metadata Framework(CMF) consist of three central repositories for the code, data and metadata. </p>"},{"location":"architecture/components/#central-metadata-repository-does-it-make-sense-to-call-cmf-server-a-central-metadata-repository","title":"Central Metadata repository (Does it make sense to call cmf-server 'a central metadata repository')","text":"<p>Central metadata repository holds the metadata pushed from the distributed sites. It holds metadata about all the different pipelines that was tracked using the common metadata tracker.  The consolidated view of the metadata stored in the central repository, helps the users to learn across various stages in the pipeline executed at different locations. Using the query layer that is pointed to the central repository, the users gets the global view of the metadata which provides them with a deeper understanding of the pipelines and its metadata.  The metadata helps to understand nonobvious results like performance of a dataset with respect to other datasets, Performance of a particular pipeline with respect to other pipelines etc. </p>"},{"location":"architecture/components/#central-artifact-storage-repository","title":"Central Artifact storage repository","text":"<p>Central Artifact storage repository stores all the artifacts related to experiment. The data versioning framework (DVC) stores the artifacts in a content addressable layout. The artifacts are stored inside the folder with name as the first two characters of the content hash and the name of the artifact as the remaining part of the content hash. This helps in efficient retrieval of the artifacts.   </p>"},{"location":"architecture/components/#git-repository","title":"Git Repository","text":"<p>Git repository is used to track the code. Along with the code, the metadata file of the artifacts which contain the content hash of the artifacts are also stored in GIT. The Data versioning framework (dvc) would use these files to retrieve the artifacts from the artifact storage repository. </p>"},{"location":"cmf_client/","title":"Quick start with cmf-client [NEEDS TO BE UPDATED]","text":"<p>Common metadata framework (<code>cmf</code>) has the following components:</p> <ul> <li>Metadata Library: exposes APIs for tracking pipeline metadata and provides endpoints to query stored metadata.</li> <li>cmf-client: handles metadata exchange with the <code>cmf-server</code>, pushes and pulls artifacts from the artifact repository, and syncs code to and from GitHub.</li> <li>cmf-server: interacts with all the remote clients and is responsible for merging the metadata transferred by the <code>cmf-client</code> and manage the consolidated metadata.</li> <li>Central Artifact Repositories: host the code and data.</li> </ul> <p>This tutorial walks you through the process of setting up the <code>cmf-client</code>.</p>"},{"location":"cmf_client/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the setup, ensure the following components are up and running:</p> <ul> <li>cmflib</li> <li>cmf-server</li> </ul> <p>Make sure there are no errors during their startup, as <code>cmf-client</code> depends on both of these components.</p>"},{"location":"cmf_client/#setup-a-cmf-client","title":"Setup a <code>cmf-client</code>","text":"<p><code>cmf-client</code> is a command-line tool that facilitates metadata collaboration between different teams or two team members. It allows users to pull or push metadata from or to the <code>cmf-server</code>.</p> <p>Follow the below-mentioned steps for the end-to-end setup of <code>cmf-client</code>:-</p> <p>Configuration</p> <ol> <li>Create working directory <code>mkdir &lt;workdir&gt;</code></li> <li>Execute <code>cmf init</code> to configure the Data Version Control (DVC) remote directory, Git remote URL, CMF server, and Neo4j. Follow the <code>cmf init</code> for more details.</li> </ol>"},{"location":"cmf_client/#how-to-effectively-use-cmf-client","title":"How to effectively use cmf-client?","text":"<p>Let's assume we are tracking the metadata for a pipeline named <code>Test-env</code> with a MinIO S3 bucket as the artifact repository and a cmf-server.</p> <p>Create a folder <pre><code>mkdir example-folder\n</code></pre></p>"},{"location":"cmf_client/#initialize-cmf","title":"Initialize cmf","text":"<p>CMF initialization is the first and foremost step to use cmf-client commands. This command, in one go, completes the initialization process, making cmf-client user friendly. Execute <code>cmf init</code> in the <code>example-folder</code> directory created in the above step. <pre><code>cmf init minioS3 --url s3://dvc-art --endpoint-url http://x.x.x.x:9000 --access-key-id minioadmin --secret-key minioadmin --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-url http://x.x.x.x:80  --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre></p> <p>Here, \"dvc-art\" is provided as an example bucket name. However, users can change it as needed. If the user chooses to change it, they will need to update the Dockerfile for minioS3 accordingly.</p> <p>Check cmf init minioS3 for more details.</p> <p>Check status of CMF initialization (Optional) <pre><code>cmf init show\n</code></pre> Check cmf init show for more details.</p> <p>Track metadata using cmflib</p> <p>Use Sample projects as a reference to create a new project to track metadata for ML pipelines.</p> <p>More information is available inside Getting Started Tutorial.</p> <p>Before pushing artifacts or metadata, ensure that the cmf server and minioS3 are up and running.</p> <p>Push artifacts</p> <p>Push artifacts in the artifact repo initialized in the Initialize cmf step. <pre><code>cmf artifact push -p 'Test-env'\n</code></pre> Check cmf artifact push for more details.</p> <p>Push metadata to cmf-server <pre><code>cmf metadata push -p 'Test-env'\n</code></pre> Check cmf metadata push for more details.</p>"},{"location":"cmf_client/#cmf-client-with-collaborative-development","title":"cmf-client with collaborative development","text":"<p>In the case of collaborative development, in addition to the above commands, users can follow the commands below to pull metadata and artifacts from a common cmf server and a central artifact repository.</p> <p>Pull metadata from the server</p> <p>Execute <code>cmf metadata pull</code> command in the <code>example_folder</code>. <pre><code>cmf metadata pull -p 'Test-env'\n</code></pre> Check cmf metadata pull for more details.</p> <p>Pull artifacts from the central artifact repo</p> <p>Execute <code>cmf artifact pull</code> command in the <code>example_folder</code>. <pre><code>cmf artifact pull -p 'Test-env'\n</code></pre> Check cmf artifact pull page for more details.</p>"},{"location":"cmf_client/#flow-chart-for-cmf","title":"Flow Chart for cmf","text":"<p> <code>cmf-client</code> is a command-line tool that facilitates metadata collaboration between different teams or two team members. It allows users to pull/push metadata from or to the <code>cmf-server</code> with similar functionalities for artifact repositories and other commands.  ** NEEDS TO BE UPDATED **</p>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/","title":"Getting started with cmf","text":"<p>Common Metadata Framework (CMF) has the following components:</p> <ul> <li>Metadata Library exposes APIs to track pipeline metadata. It also provides APIs to query the stored metadata.</li> <li>cmf-client interacts with the cmf-server to pull or push metadata.</li> <li>cmf-server with GUI interacts with remote cmf-clients and merges the metadata transferred by each   client. This server also provides a GUI that can render the stored metadata.</li> <li>Central Artifact Repositories host the code and data.</li> </ul>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/#setup-a-cmf-client","title":"Setup a cmf-client","text":"<p><code>cmf-client</code> is a tool that facilitates metadata collaboration between different teams and team members. These clients interact with the cmf-server to push/pull metadata.</p> <p>Pre-Requisites</p> <ul> <li>Python 3.9+</li> <li>Git latest version</li> </ul> <p>Install cmf library i.e. cmflib <pre><code>pip install https://github.com/HewlettPackard/cmf\n</code></pre> OR <pre><code>pip install cmflib\n</code></pre> Documentation for more details.</p>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/#install-cmf-server","title":"Install cmf-server","text":"<p>cmf-server is the primary interface for the user to explore and track their ML training runs by browsing the stored metadata. Users can retrieve the saved metadata file and can view the content of the saved metadata file using the UI provided by the cmf-server.</p> <p>Details on how to set up a cmf-server can be found here.</p>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/#simple-example-of-using-the-cmf-client","title":"Simple Example of using the CMF Client","text":"<p>In this example, CMF is used to track the metadata for a pipeline named <code>Test-env</code> which interacts with a MinIO</p> <p>S3 bucket as the artifact repository and a cmf-server.</p> <p>Setup the example directory <pre><code>mkdir example-folder &amp;&amp; cd example-folder\n</code></pre></p>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/#initialize-cmf","title":"Initialize cmf","text":"<p>CMF must be initialized to use cmf-client commands. The following command configures authentication to an S3 bucket and specifies the connection to a CMF server. <pre><code>cmf init minioS3 --url s3://bucket-name --endpoint-url http://localhost:9000 \\\n  --access-key-id minioadmin --secret-key minioadmin --git-remote-url https://github.com/user/experiment-repo.git \\\n  --cmf-server-url http://x.x.x.x:80  --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://X.X.X.X:7687\n</code></pre> Check here for more details.</p> <p>Check status of CMF initialization (Optional) <pre><code>cmf init show\n</code></pre> Check here for more details.</p> <p>Track metadata using cmflib</p> <p>Use Sample projects as a reference to create a new project to track metadata for ML pipelines.</p> <p>More info is available here.</p> <p>Push artifacts</p> <p>Push artifacts in the artifact repo initialized in the Initialize cmf step. <pre><code>cmf artifact push\n</code></pre> Check here for more details.</p> <p>Push metadata to cmf-server <pre><code>cmf metadata push -p 'Test-env'\n</code></pre> Check here for more details.</p>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/#cmf-client-with-collaborative-development","title":"cmf-client with collaborative development","text":"<p>In the case of collaborative development, in addition to the above commands, users can follow the commands below to pull metadata and artifacts from a common cmf server and a central artifact repository.</p> <p>Pull metadata from the server</p> <p>Execute <code>cmf metadata</code> command in the <code>example_folder</code>. <pre><code>cmf metadata pull -p 'Test-env'\n</code></pre> Check here for more details.</p> <p>Pull artifacts from the central artifact repo</p> <p>Execute <code>cmf artifact</code> command in the <code>example_folder</code>. <pre><code>cmf artifact pull -p \"Test-env\"\n</code></pre> Check here for more details.</p>"},{"location":"cmf_client/Getting%20Started%20with%20cmf/#flow-chart-for-cmf","title":"Flow Chart for cmf","text":""},{"location":"cmf_client/cmf-dvc-ingest-guide/","title":"Guide to <code>cmf dvc ingest</code> Command","text":"<p>The <code>cmf dvc ingest</code> command is used to ingest metadata from the <code>dvc.lock</code> file into the <code>cmf-server</code>. If an existing MLMD (Metadata) file is provided, the command will merge and update execution metadata based on matching commands or create new executions if none exist.</p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#steps-to-get-started","title":"\ud83d\udccc Steps to Get Started","text":"<p>Follow the steps below to set up <code>cmf</code> client and use the <code>dvc ingest</code> command.</p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#1-navigate-to-your-project-directory","title":"1. Navigate to Your Project Directory","text":"<p>Open your terminal and go to the directory (for e.g., <code>example-get-started</code>) where you want to use <code>cmf</code> commands.</p> <pre><code>cd /path/to/your/project\n</code></pre>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#2-initialize-cmf-with-neo4j-credentials","title":"2. Initialize <code>cmf</code> with Neo4j Credentials","text":"<p>Use the following command to initialize <code>cmf</code>. You can choose from various storage options like <code>local</code>, <code>ssh</code>, <code>amazons3</code>, <code>osdfremote</code>, or <code>minios3</code>.</p> <pre><code>   cmf init local \\\n  --path /home/XXXX/local-storage \\\n  --git-remote-url https://github.com/user/experiment-repo.git \\\n  --cmf-server-url http://x.x.x.x:80 \\\n  --neo4j-user neo4j \\\n  --neo4j-password password \\\n  --neo4j-uri bolt://x.x.x.x:7687\n</code></pre> <p>\ud83d\udd01 Replace the following: - <code>x.x.x.x</code> with your IP address - <code>XXXX</code> with your system username - Provide your correct Neo4j username and password</p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#3-start-the-neo4j-server","title":"3. Start the Neo4j Server","text":"<p>Start the Neo4j server using Docker. Follow the guide provided below:</p> <p>\ud83d\udcc4 Start Neo4j with Docker </p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#4-create-a-dvcyaml-file","title":"4. Create a <code>dvc.yaml</code> File","text":"<p>Inside your project directory (for e.g., <code>example-get-started</code>), create a <code>dvc.yaml</code> file.</p> <p>Here\u2019s a sample <code>dvc.yaml</code>:</p> <pre><code>stages:\n  prepare:\n    cmd: python src/parse.py artifacts/data.xml.gz artifacts/parsed/\n    deps:\n      - artifacts/data.xml.gz\n    outs:\n      - artifacts/parsed/train.tsv\n      - artifacts/parsed/test.tsv\n\n  featurize:\n    cmd: python src/featurize.py artifacts/parsed/ artifacts/features/\n    deps:\n      - artifacts/parsed/train.tsv\n      - artifacts/parsed/test.tsv\n    outs:\n      - artifacts/features/train.pkl\n      - artifacts/features/test.pkl\n\n  train:\n    cmd: python src/train.py artifacts/features/ artifacts/model/\n    deps:\n      - artifacts/features/train.pkl\n      - artifacts/features/test.pkl\n    outs:\n      - artifacts/model/model.pkl\n\n  test:\n    cmd: python src/test.py artifacts/model/ artifacts/features/ artifacts/test_results/\n    deps:\n      - artifacts/model/model.pkl\n    outs:\n      - artifacts/test_results/prc.json\n      - artifacts/test_results/roc.json\n      - artifacts/test_results/scores.json\n</code></pre> <p>\u26a0\ufe0f Be Consistent with deps and outs: When defining deps and outs in your dvc.yaml, ensure consistency in the format used. Either define both as directories (e.g., artifacts/parsed/) or both as individual files (e.g., artifacts/parsed/train.tsv, artifacts/parsed/test.tsv).</p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#5-remove-cmf-code-from-src-directory","title":"5. Remove <code>cmf</code> code from <code>src</code> Directory","text":"<p>Ensure that your source files inside the <code>example-get-started/src</code> directory do not contain any <code>cmf</code>-related code. Keep them clean and focused on their tasks.</p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#6-run-the-dvc-pipeline","title":"6. Run the DVC Pipeline","text":"<p>Execute your pipeline using the following command. This will also generate a <code>dvc.lock</code> file.</p> <pre><code>dvc repro\n</code></pre>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#7-ingest-metadata-with-cmf","title":"7. Ingest Metadata with <code>cmf</code>","text":"<p>Run the following command to create metadata file based on your <code>dvc.lock</code> file:</p> <pre><code>cmf dvc ingest\n</code></pre> <p>\u26a0\ufe0f Troubleshooting: If you see an error like the one below, your Neo4j server might not be running properly.</p> <p></p>"},{"location":"cmf_client/cmf-dvc-ingest-guide/#8-pushpull-metadata-and-artifacts","title":"8. Push/Pull Metadata and Artifacts","text":"<p>Use cmf-client commands to push or pull your metadata and artifacts as required:</p> <ul> <li><code>cmf metadata push</code></li> <li><code>cmf metadata pull</code></li> <li><code>cmf artifact push</code></li> <li><code>cmf artifact pull</code></li> </ul>"},{"location":"cmf_client/cmf_client_commands/","title":"Getting started with cmf-client commands","text":""},{"location":"cmf_client/cmf_client_commands/#cmf","title":"cmf","text":"<pre><code>Usage: cmf [-h] {init, artifact, metadata, execution, pipeline, repo, dvc}\n</code></pre> <p>The <code>cmf</code> command is a comprehensive tool designed to initialize an artifact repository and perform various operations on artifacts, execution, pipeline and metadata.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-init","title":"cmf init","text":"<pre><code>Usage: cmf init [-h] {minioS3, amazonS3, local, sshremote, osdfremote, show}\n</code></pre> <p><code>cmf init</code> initializes an artifact repository for cmf. Local directory, Minio S3 bucket, Amazon S3 bucket, SSH Remote and Remote OSDF directory are the options available. Additionally, user can provide cmf-server url.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-init-show","title":"cmf init show","text":"<pre><code>Usage: cmf init show\n</code></pre> <p><code>cmf init show</code> displays current cmf configuration.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-init-minios3","title":"cmf init minioS3","text":"<pre><code>Usage: cmf init minioS3 [-h] --url [url]\n                             --endpoint-url [endpoint_url]\n                             --access-key-id [access_key_id]\n                             --secret-key [secret_key]\n                             --git-remote-url[git_remote_url]\n                             --cmf-server-url [cmf_server_url]\n                             --neo4j-user [neo4j_user]\n                             --neo4j-password [neo4j_password]\n                             --neo4j-uri [neo4j_uri]\n</code></pre> <p><code>cmf init minioS3</code> configures Minio S3 bucket as a cmf artifact repository. Refer minio-server.md to set up a minio server.</p> <pre><code>cmf init minioS3 --url s3://dvc-art --endpoint-url http://x.x.x.x:9000 --access-key-id minioadmin --secret-key minioadmin --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-url http://x.x.x.x:80 --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre> <p>Here, \"dvc-art\" is provided as an example bucket name. However, users can change it as needed, if the user chooses to change it, they will need to update the Dockerfile for MinIOS3 accordingly.</p> <p>Required Arguments</p> <pre><code>  --url [url]                           Specify MinioS3 bucket url.\n  --endpoint-url [endpoint_url]         Specify the endpoint url which is used to accedd Minio's locally/remotely running UI.\n  --access-key-id [access_key_id]       Specify Access Key Id.\n  --secret-key [secret_key]             Specify Secret Key.\n  --git-remote-url [git_remote_url]     Specify git repo url. eg: https://github.com/XXX/example.git\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                          show this help message and exit\n  --cmf-server-url [cmf_server_url]   Specify cmf-server url. (default: http://127.0.0.1:80)\n  --neo4j-user [neo4j_user]           Specify neo4j user. (default: None)\n  --neo4j-password [neo4j_password]   Specify neo4j password. (default: None)\n  --neo4j-uri [neo4j_uri]             Specify neo4j uri. Eg bolt://localhost:7687 (default: None)\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-init-local","title":"cmf init local","text":"<pre><code>Usage: cmf init local [-h] --path [path] -\n                           --git-remote-url [git_remote_url]\n                           --cmf-server-url [cmf_server_url]\n                           --neo4j-user [neo4j_user]\n                           --neo4j-password [neo4j_password]\n                           --neo4j-uri [neo4j_uri]\n</code></pre> <p><code>cmf init local</code> initialises local directory as a cmf artifact repository.</p> <pre><code>cmf init local --path /home/XXXX/local-storage --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-url http://x.x.x.x:80 --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre> <p>Replace 'XXXX' with your system username in the following path: /home/XXXX/local-storage</p> <p>Required Arguments</p> <pre><code>  --path [path]                         Specify local directory path.\n  --git-remote-url [git_remote_url]     Specify git repo url. eg: https://github.com/XXX/example.git\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                          show this help message and exit\n  --cmf-server-url [cmf_server_url]   Specify cmf-server url. (default: http://127.0.0.1:80)\n  --neo4j-user [neo4j_user]           Specify neo4j user. (default: None)\n  --neo4j-password [neo4j_password]   Specify neo4j password. (default: None)\n  --neo4j-uri [neo4j_uri]             Specify neo4j uri. Eg bolt://localhost:7687 (default: None)\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-init-amazons3","title":"cmf init amazonS3","text":"<p>Before setting up, obtain AWS temporary security credentials using the AWS Security Token Service (STS). These credentials are short-term and can last from minutes to hours. They are dynamically generated and provided to trusted users upon request, and expire after use. Users with appropriate permissions can request new credentials before or upon expiration. For further information, refer to the Temporary security credentials in IAM page.</p> <p>To retrieve temporary security credentials using multi-factor authentication (MFA) for an IAM user, you can use the below command.</p> <pre><code>aws sts get-session-token --duration-seconds &lt;duration&gt; --serial-number &lt;MFA_device_serial_number&gt; --token-code &lt;MFA_token_code&gt;\n</code></pre> <p>Required Arguments</p> <pre><code>  --serial-number                Specifies the serial number of the MFA device associated with the IAM user.\n  --token-code                   Specifies the one-time code generated by the MFA device.\n</code></pre> <p>Optional Arguments</p> <pre><code>  --duration-seconds             Specifies the duration for which the temporary credentials will be valid, in seconds.\n</code></pre> <p>Example</p> <pre><code>aws sts get-session-token --duration-seconds 3600 --serial-number arn:aws:iam::123456789012:mfa/user --token-code 123456\n</code></pre> <p>This will return output like</p> <pre><code>{\n    \"Credentials\": {\n        \"AccessKeyId\": \"ABCDEFGHIJKLMNO123456\",\n        \"SecretAccessKey\": \"PQRSTUVWXYZ789101112131415\",\n        \"SessionToken\": \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijlmnopqrstuvwxyz12345678910ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijlmnopqrstuvwxyz12345678910ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijlmnopqrstuvwxyz12345678910\",\n        \"Expiration\": \"2021-05-10T15:31:08+00:00\"\n    }\n}\n</code></pre> <p>Initialization of amazonS3</p> <pre><code>Usage: cmf init amazonS3 [-h] --url [url]\n                              --access-key-id [access_key_id]\n                              --secret-key [secret_key]\n                              --session-token [session_token]\n                              --git-remote-url [git_remote_url]\n                              --cmf-server-url [cmf_server_url]\n                              --neo4j-user [neo4j_user]\n                              --neo4j-password [neo4j_password]\n                              --neo4j-uri [neo4j_uri]\n</code></pre> <p><code>cmf init amazonS3</code> initialises Amazon S3 bucket as a CMF artifact repository.</p> <pre><code>cmf init amazonS3 --url s3://bucket-name --access-key-id XXXXXXXXXXXXX --secret-key XXXXXXXXXXXXX --session-token XXXXXXXXXXXXX --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-url http://x.x.x.x:80 --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre> <p>Here, use the --access-key-id, --secret-key and --session-token generated from the <code>aws sts</code> command which is mentioned above.</p> <p>The bucket-name must exist within Amazon S3 before executing the <code>cmf artifact push</code> command.</p> <p>Required Arguments</p> <pre><code>  --url [url]                           Specify Amazon S3 bucket url.\n  --access-key-id [access_key_id]       Specify Access Key Id.\n  --secret-key [secret_key]             Specify Secret Key.\n  --session-token                       Specify session token. (default: None)\n  --git-remote-url [git_remote_url]     Specify git repo url. eg: https://github.com/XXX/example.git\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                          show this help message and exit\n  --cmf-server-url [cmf_server_url]   Specify cmf-server url. (default: http://127.0.0.1:80)\n  --neo4j-user [neo4j_user]           Specify neo4j user. (default: None)\n  --neo4j-password [neo4j_password]   Specify neo4j password. (default: None)\n  --neo4j-uri [neo4j_uri]             Specify neo4j uri. Eg bolt://localhost:7687 (default: None)\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-init-sshremote","title":"cmf init sshremote","text":"<pre><code>Usage: cmf init sshremote [-h] --path [path]\n                               --user [user]\n                               --port [port]\n                               --password [password]\n                               --git-remote-url [git_remote_url]\n                               --cmf-server-url [cmf_server_url]\n                               --neo4j-user [neo4j_user]\n                               --neo4j-password [neo4j_password]\n                               --neo4j-uri [neo4j_uri]\n</code></pre> <p><code>cmf init sshremote</code> command initialises remote ssh directory as a cmf artifact repository.</p> <pre><code>cmf init sshremote --path ssh://127.0.0.1/home/user/ssh-storage --user XXXXX --port 22 --password example@123 --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-url http://x.x.x.x:80 --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre> <p>Required Arguments</p> <pre><code>  --path [path]                           Specify remote ssh directory path.\n  --user [user]                           Specify username.\n  --port [port]                           Specify port.\n  --password [password]                   Specify password. This will be saved only on local.\n  --git-remote-url [git_remote_url]       Specify git repo url. eg: https://github.com/XXX/example.git\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                          show this help message and exit\n  --cmf-server-url [cmf_server_url]   Specify cmf-server url. (default: http://127.0.0.1:80)\n  --neo4j-user [neo4j_user]           Specify neo4j user. (default: None)\n  --neo4j-password [neo4j_password]   Specify neo4j password. (default: None)\n  --neo4j-uri [neo4j_uri]             Specify neo4j uri. Eg bolt://localhost:7687 (default: None)\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-init-osdfremote","title":"cmf init osdfremote","text":"<pre><code>Usage: cmf init osdfremote [-h] --path [path]\n                             --cache [cache]\n                             --key-id [key_id]\n                             --key-path [key_path]\n                             --key-issuer [key_issuer]\n                             --git-remote-url[git_remote_url]\n                             --cmf-server-url [cmf_server_url]\n                             --neo4j-user [neo4j_user]\n                             --neo4j-password [neo4j_password]\n                             --neo4j-uri [neo4j_uri]\n</code></pre> <p><code>cmf init osdfremote</code> configures a OSDF Origin as a cmf artifact repository.</p> <pre><code>cmf init osdfremote --path https://[Some Origin]:8443/nrp/fdp/ --cache http://[Some Redirector] --key-id XXXX --key-path ~/.ssh/private.pem --key-issuer https://[Token Issuer] --git-remote-url https://github.com/user/experiment-repo.git --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-url http://127.0.0.1:80 --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre> <p>Required Arguments</p> <pre><code>  --path [path]                        Specify FQDN for OSDF origin including including port and directory path if any\n  --key-id [key_id]                    Specify key_id for provided private key. eg. b2d3\n  --key-path [key_path]                Specify path for private key on local filesystem. eg. ~/.ssh/XXX.pem\n  --key-issuer [key_issuer]            Specify URL for Key Issuer. eg. https://t.nationalresearchplatform.org/XXX\n  --git-remote-url [git_remote_url]    Specify git repo url. eg: https://github.com/XXX/example.git\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                          show this help message and exit\n  --cmf-server-url [cmf_server_url]   Specify cmf-server url. (default: http://127.0.0.1:80)\n  --neo4j-user [neo4j_user]           Specify neo4j user. (default: None)\n  --neo4j-password [neo4j_password]   Specify neo4j password. (default: None)\n  --neo4j-uri [neo4j_uri]             Specify neo4j uri. Eg bolt://localhost:7687 (default: None)\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-artifact","title":"cmf artifact","text":"<pre><code>Usage: cmf artifact [-h] {pull, push, list}\n</code></pre> <p><code>cmf artifact</code> pull, push or list artifacts from or to the user configured artifact repository, respectively.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-artifact-pull","title":"cmf artifact pull","text":"<pre><code>Usage: cmf artifact pull [-h] -p [pipeline_name] -f [file_name] -a [artifact_name]\n</code></pre> <p><code>cmf artifact pull</code> command pull artifacts from the user configured repository to the user's local machine.</p> <pre><code>cmf artifact pull -p 'pipeline-name' -f '/path/to/mlmd-file-name' -a 'artifact-name'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline-name [pipeline_name]   Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                            show this help message and exit.\n  -a [artifact_name], --artifact_name [artifact_name]   Specify artifact name only; don't use folder name or absolute path.\n  -f [file_name], --file_name [file_name]               Specify input metadata file name.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-artifact-push","title":"cmf artifact push","text":"<pre><code>Usage: cmf artifact push [-h] -p [pipeline_name] -f [file_name] -j [jobs]\n</code></pre> <p><code>cmf artifact push</code> command push artifacts from the user's local machine to the user configured artifact repository.</p> <pre><code>cmf artifact push -p 'pipeline_name' -f '/path/to/mlmd-file-name' -j 'jobs'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline_name [pipeline_name]   Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                            show this help message and exit.\n  -f [file_name], --file-name [file_name]               Specify mlmd file name.\n  -j [jobs], --jobs [jobs]                              Number of parallel jobs for uploading artifacts to remote storage. Default is 4 * cpu_count().\n                                                        Increasing jobs may speed up uploads but will use more resources.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-artifact-list","title":"cmf artifact list","text":"<pre><code>Usage: cmf artifact list [-h] -p [pipeline_name] -f [file_name] -a [artifact_name]\n</code></pre> <p><code>cmf artifact list</code> command displays artifacts from the input metadata file with a few properties in a 7-column table, limited to 20 records per page.</p> <pre><code>cmf artifact list -p 'pipeline_name' -f '/path/to/mlmd-file-name' -a 'artifact_name'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline_name [pipeline_name]   Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                            show this help message and exit.\n  -f [file_name], --file_name [file_name]               Specify input metadata file name.\n  -a [artifact_name], --artifact_name [artifact_name]   Specify the artifact name to display detailed information about the given artifact name.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-metadata","title":"cmf metadata","text":"<pre><code>Usage: cmf metadata [-h] {pull, push, export}\n</code></pre> <p><code>cmf metadata</code> push, pull or export the metadata file to and from the cmf-server, respectively.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-metadata-pull","title":"cmf metadata pull","text":"<pre><code>Usage: cmf metadata pull [-h] -p [pipeline_name] -f [file_name]  -e [exec_uuid]\n</code></pre> <p><code>cmf metadata pull</code> command pulls the metadata file from the cmf-server to the user's local machine.</p> <pre><code>cmf metadata pull -p 'pipeline-name' -f '/path/to/mlmd-file-name' -e 'execution_uuid'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline_name [pipeline_name]     Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>-h, --help                                                show this help message and exit.\n-e [exec_uuid], --execution_uuid [exec_uuid]              Specify execution uuid.\n-f [file_name], --file_name [file_name]                   Specify output metadata file name.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-metadata-push","title":"cmf metadata push","text":"<pre><code>Usage: cmf metadata push [-h] -p [pipeline_name] -f [file_name] -e [exec_uuid] -t [tensorboard_path]\n</code></pre> <p><code>cmf metadata push</code> command pushes the metadata file from the local machine to the cmf-server.</p> <pre><code>cmf metadata push -p 'pipeline-name' -f '/path/to/mlmd-file-name' -e 'execution_uuid' -t '/path/to/tensorboard-log'\n</code></pre> <p>Required Arguments</p> <pre><code>-p [pipeline_name], --pipeline_name [pipeline_name]     Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                                            show this help message and exit.\n  -f [file_name], --file_name [file_name]                               Specify input metadata file name.\n  -e [exec_uuid], --execution_uuid [exec_uuid]                          Specify execution uuid.\n  -t [tensorboard_path], --tensorboard_path [tensorboard_path]          Specify path to tensorboard logs for the pipeline.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-metadata-export","title":"cmf metadata export","text":"<pre><code>Usage: cmf metadata export [-h] -p [pipeline_name] -j [json_file_name] -f [file_name]\n</code></pre> <p><code>cmf metadata export</code> export local metadata's metadata in json format to a json file.</p> <pre><code>cmf metadata export -p 'pipeline-name' -j '/path/to/json-file-name' -f '/path/to/mlmd-file-name'\n</code></pre> <p>Required Arguments</p> <pre><code>-p [pipeline_name], --pipeline_name [pipeline_name]        Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                               show this help message and exit.\n  -f [file_name], --file_name [file_name]                  Specify the absolute or relative path for the input metadata file.\n  -j [json_file_name], --json_file_name [json_file_name]   Specify output json file name with full path.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-execution","title":"cmf execution","text":"<pre><code>Usage: cmf execution [-h] {list}\n</code></pre> <p><code>cmf execution</code> command to displays executions from the metadata file.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-execution-list","title":"cmf execution list","text":"<pre><code>Usage: cmf execution list [-h] -p [pipeline_name] -f [file_name] -e [execution_uuid]\n</code></pre> <p><code>cmf execution list</code> command to displays executions from the input metadata file with a few properties in a 7-column table, limited to 20 records per page.</p> <pre><code>cmf execution list -p 'pipeline_name' -f '/path/to/mlmd-file-name' -e 'execution_uuid'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline_name [pipeline_name]       Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                                    show this help message and exit.\n  --f [file_name], --file_name [file_name]                      Specify input metadata file name.\n  -e [exec_uuid], --execution_uuid [exec_uuid]                  Specify the execution uuid to retrieve execution.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-pipeline","title":"cmf pipeline","text":"<pre><code>Usage: cmf pipeline [-h] {list}\n</code></pre> <p><code>cmf pipeline</code> command displays a list of pipeline name(s) from the available metadatafile.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-pipeline-list","title":"cmf pipeline list","text":"<pre><code>Usage: cmf pipeline list [-h] -f [file_name]\n</code></pre> <p><code>cmf pipeline list</code> command displays a list of pipeline name(s) from the available metadatafile.</p> <pre><code>cmf pipeline list -f '/path/to/mlmd-file-name'\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                            show this help message and exit.\n  --f [file_name], --file_name [file_name]              Specify input metadata file name.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-repo","title":"cmf repo","text":"<pre><code>Usage: cmf repo [-h] {push, pull}\n</code></pre> <p><code>cmf repo</code> command push and pull artifacts, metadata files, and source code to and from the user's artifact repository, cmf-server, and git respectively.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-repo-push","title":"cmf repo push","text":"<pre><code>Usage: cmf repo push [-h] -p [pipeline_name] -f [file_name] -e [exec_uuid] -t [tensorboard] -j [jobs]\n</code></pre> <p><code>cmf repo push</code> command push artifacts, metadata files, and source code to the user's artifact repository, cmf-server, and git respectively.</p> <pre><code>cmf repo push -p 'pipeline-name' -f '/path/to/mlmd-file-name' -e 'execution_uuid' -t 'tensorboard_log_path' -j 'jobs'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline_name [pipeline_name]            Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                                     show this help message and exit.\n  -f [file_name], --file-name [file_name]                        Specify mlmd file name.\n  -e [exec_uuid], --execution_uuid [exec_uuid]                   Specify execution uuid.\n  -t [tensorboard], --tensorboard [tensorboard]                  Specify path to tensorboard logs for the pipeline.\n  -j [jobs], --jobs [jobs]                                       Number of parallel jobs for uploading artifacts to remote storage. Default is 4 * cpu_count().\n                                                                 Increasing jobs may speed up uploads but will use more resources.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-repo-pull","title":"cmf repo pull","text":"<pre><code>Usage: cmf repo pull [-h] -p [pipeline_name] -f [file_name] -e [exec_uuid]\n</code></pre> <p><code>cmf repo pull</code> command pull artifacts, metadata files, and source code from the user's artifact repository, cmf-server, and git respectively.</p> <pre><code>cmf repo pull -p 'pipeline-name' -f '/path/to/mlmd-file-name' -e 'execution_uuid'\n</code></pre> <p>Required Arguments</p> <pre><code>  -p [pipeline_name], --pipeline_name [pipeline_name]            Specify Pipeline name.\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                                     show this help message and exit.\n  -f [file_name], --file_name [file_name]                        Specify output metadata file name.\n  -e [exec_uuid], --execution_uuid [exec_uuid]                   Specify execution uuid.\n</code></pre>"},{"location":"cmf_client/cmf_client_commands/#cmf-dvc","title":"cmf dvc","text":"<pre><code>Usage: cmf dvc [-h] {ingest}\n</code></pre> <p><code>cmf dvc</code> command ingests metadata from the dvc.lock file into CMF.</p>"},{"location":"cmf_client/cmf_client_commands/#cmf-dvc-ingest","title":"cmf dvc ingest","text":"<pre><code>Usage: cmf dvc ingest [-h] -f [file_name]\n</code></pre> <p><code>cmf dvc ingest</code> command ingests metadata from the dvc.lock file into the CMF. If an existing MLMD file is provided, it merges and updates execution metadata based on matching commands, or creates new executions if none exist.</p> <pre><code>cmf dvc ingest -f '/path/to/mlmd-file-name'\n</code></pre> <p>Optional Arguments</p> <pre><code>  -h, --help                                                     show this help message and exit.\n  -f [file_name], --file-name [file_name]                        Specify input mlmd file name. (default: mlmd)\n</code></pre>"},{"location":"cmf_client/cmf_osdf/","title":"OSDF Remote Artifact Setup","text":""},{"location":"cmf_client/cmf_osdf/#steps-to-set-up-an-osdf-remote-repo","title":"Steps to set up an OSDF Remote Repo","text":"<p>Backed by the Pelican Platform, the Open Science Data Federation (OSDF) is an OSG service hosting data origins and caches across the globe.</p> <p>OSDF facilitates the distributed nature of a national compute pool. CMF supports OSDF Origins and caches as remotes to push/pull data from/to. </p> <p>OSDF provides a distributed data federation that allows researchers to store and access data across multiple institutions and geographic locations. It uses tokens for authentication and supports caching for improved performance.</p> <p>Proceed with the following steps to set up an OSDF Remote Repository:</p> <ol> <li>Initialize the <code>project directory</code> with an OSDF remote.</li> <li> <p>Check whether cmf is initialized in your project directory with the following command.    <pre><code>cmf init show\n</code></pre>    If cmf is not initialized, the following message will appear on the screen.    <pre><code>'cmf' is not configured.\nExecute the 'cmf init' command.\n</code></pre></p> </li> <li> <p>Execute the following command to initialize the OSDF remote storage as a CMF artifact repository.    <pre><code>cmf init osdfremote --path https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test \\\n    --cache https://osdf-director.osg-htc.org/ \\\n    --key-id XXXX \\\n    --key-path ~/private_hpe.pem \\\n    --key-issuer https://t.nationalresearchplatform.org/fdp-hpe \\\n    --git-remote-url https://github.com/user/experiment-repo.git\n</code></pre></p> </li> </ol> <p>Parameters:    - <code>--path</code>: The OSDF origin server URL where data will be stored    - <code>--cache</code>: The OSDF cache/director URL for retrieving data    - <code>--key-id</code>: The key identifier for authentication    - <code>--key-path</code>: Path to the private key file for token generation    - <code>--key-issuer</code>: The issuer URL for token generation    - <code>--git-remote-url</code>: Git repository URL for version control</p> <ol> <li> <p>Execute <code>cmf init show</code> to check the CMF configuration.</p> </li> <li> <p>After initialization, you can push artifacts to OSDF (It is presumed you have done cmf artifact add to register the artifacts):    <pre><code>cmf artifact push -p Test-env\n</code></pre></p> </li> <li> <p>To pull artifacts from OSDF (using the cache for performance):    <pre><code>cmf artifact pull -p Test-env\n</code></pre></p> </li> </ol> <p>The pull command will automatically use the configured cache server for faster downloads.</p>"},{"location":"cmf_client/cmf_osdf/#notes","title":"Notes","text":"<ul> <li>Ensure you have the proper authentication credentials from your OSDF provider</li> <li>private key, </li> <li>key ID, </li> <li>issuerURL, </li> <li>origin FQDN and Federation Path </li> <li>The cache server (<code>--cache</code>) is used for reading data to improve performance (It identifies files directly from their path and contacts the topologically nearest cache to pull from)</li> <li>The origin server (<code>--path</code>) is used for writing data (push). Caches cannot be written to. </li> <li>OSDF uses token-based authentication that is automatically generated from your private key. This is handled internally by CMF under the covers. </li> <li>CMF also verifies that the retrieved file has the same Hash value as is recorded in MLMD</li> </ul>"},{"location":"cmf_client/cmf_osdf/#end-to-end-examples","title":"End to End Examples","text":""},{"location":"cmf_client/cmf_osdf/#single-site","title":"Single Site","text":""},{"location":"cmf_client/cmf_osdf/#pushing-artifacts","title":"Pushing Artifacts","text":"<ul> <li>Running CMF's examples/example-getting-started workflow and pushing artifacts to a specific remote</li> </ul> <pre><code>(cmf) tripataa@ai07:~/cmf-examples/example-get-started$ cmf init osdfremote --path https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test           --cache https://osdf-director.osg-htc.org/   --key-id XXXX    --key-path ~/private_hpe.pem    --key-issuer https://t.nationalresearchplatform.org/fdp-hpe --git-remote-url https://github.com/user/experiment-repo.git\ngit_dir /home/tripataa/cmf-examples/example-get-started/.git\nStarting cmf init.\nSetting 'osdf' as a default remote.\nSUCCESS: cmf init complete.\n(cmf) tripataa@ai07:~/cmf-examples/example-get-started$\n\n\n(cmf) tripataa@ai07:~/cmf-examples/example-get-started$ chmod +x test_script.sh\n(cmf) tripataa@ai07:~/cmf-examples/example-get-started$ ./test_script.sh\n\n[1/5] [RUNNING PARSE STEP         ]\n*** Note: CMF will check out a new branch in git to commit the metadata files ***\n*** The checked out branch is mlmd. ***\n100% Adding...|########################################|1/1 [00:00, 39.58file/s]\n100% Adding...|########################################|1/1 [00:00, 25.10file/s]\n100% Adding...|########################################|1/1 [00:00, 17.65file/s]\n100% Adding...|########################################|1/1 [00:00, 37.17file/s]\n\n[2/5] [RUNNING FEATURIZE STEP     ]\n*** Note: CMF will check out a new branch in git to commit the metadata files ***\n*** The checked out branch is mlmd. ***\n100% Adding...|########################################|1/1 [00:00, 62.53file/s]\n100% Adding...|########################################|1/1 [00:00, 31.05file/s]\n100% Adding...|########################################|1/1 [00:00, 47.68file/s]\nThe input data frame artifacts/parsed/train.tsv size is (20017, 3)\nThe output matrix artifacts/features/train.pkl size is (20017, 3002) and data type is float64\nThe input data frame artifacts/parsed/test.tsv size is (4983, 3)\nThe output matrix artifacts/features/test.pkl size is (4983, 3002) and data type is float64\n100% Adding...|########################################|1/1 [00:00, 26.49file/s]\n100% Adding...|########################################|1/1 [00:00, 47.62file/s]\n\n(cmf) tripataa@ai07:~/cmf-examples/example-get-started$ cmf artifact push -p Test-env\nCollecting                                            |0.00 [00:00,    ?entry/s]\nPushing...\nEverything is up to date.\n</code></pre>"},{"location":"cmf_client/cmf_osdf/#pulling-artifacts-via-cache","title":"Pulling Artifacts (via Cache)","text":"<ul> <li>Pulling artifacts via a cache. </li> <li>The --cache path is specified </li> <li>The artifacts for the pipeline: <code>Test-env</code> is pulled into a test/ folder (as an example)</li> <li>CMF looks at it's MLMD index and fetches the necessary files form the cache when configured. </li> </ul> <pre><code>(cmf) tripataa@ai07:~/cmf-examples/test$ ls\nmlmd\n(cmf) tripataa@ai07:~/cmf-examples/test$ cmf init osdfremote --path https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test \\\n          --cache https://osdf-director.osg-htc.org/ \\\n        --key-id XXX \\\n        --key-path ~/private_hpe.pem \\\n        --key-issuer https://t.nationalresearchplatform.org/fdp-hpe \\\n--git-remote-url https://github.com/user/experiment-repo.git\ngit_dir /home/tripataa/cmf-examples/test/.git\nStarting git init.\n*** Note: CMF will check out a new branch in git to commit the metadata files\n***\n*** The checked out branch is master. ***\ngit init complete.\nStarting cmf init.\nSetting 'osdf' as a default remote.\nSUCCESS: cmf init complete.\n\n(cmf) tripataa@ai07:~/cmf-examples/test$ cmf artifact pull -p Test-env\nFetching\nartifact=cmf_artifacts/python_env_4927239f4c14b700b637ff03ab787e65.yaml,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/49/27239f4c14b\n700b637ff03ab787e65 to\n./cmf_artifacts/python_env_4927239f4c14b700b637ff03ab787e65.yaml\nobject cmf_artifacts/python_env_4927239f4c14b700b637ff03ab787e65.yaml downloaded\nat ./cmf_artifacts/python_env_4927239f4c14b700b637ff03ab787e65.yaml in 0.00\nseconds and matches MLMD records.\nFetching artifact=artifacts/data.xml.gz,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/23/6d9502e0283\nd91f689d7038b8508a2 to ./artifacts/data.xml.gz\nobject artifacts/data.xml.gz downloaded at ./artifacts/data.xml.gz in 0.02\nseconds and matches MLMD records.\nFetching artifact=artifacts/parsed/train.tsv,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/32/b715ef0d71f\nf4c9e61f55b09c15e75 to ./artifacts/parsed/train.tsv\nobject artifacts/parsed/train.tsv downloaded at ./artifacts/parsed/train.tsv in\n0.03 seconds and matches MLMD records.\nFetching artifact=artifacts/parsed/test.tsv,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/6f/597d341ceb7\nd8fbbe88859a892ef81 to ./artifacts/parsed/test.tsv\nobject artifacts/parsed/test.tsv downloaded at ./artifacts/parsed/test.tsv in\n0.01 seconds and matches MLMD records.\nFetching artifact=artifacts/features/train.pkl,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/f2/17d9bdb2523\nc54e71dcb95fcf16ff2 to ./artifacts/features/train.pkl\nobject artifacts/features/train.pkl downloaded at ./artifacts/features/train.pkl\nin 0.02 seconds and matches MLMD records.\nFetching artifact=artifacts/features/test.pkl,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/30/4cfed883c9c\n0cecaeb7f0079e4b98c to ./artifacts/features/test.pkl\nobject artifacts/features/test.pkl downloaded at ./artifacts/features/test.pkl\nin 0.00 seconds and matches MLMD records.\nFetching\nartifact=cmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce8c58/metrics/training_met\nrics,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/64/d2da21bb624\n9afc9ea5a3a5cd67cc7 to\n./cmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce8c58/metrics/training_metrics\nobject\ncmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce8c58/metrics/training_metrics\ndownloaded at\n./cmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce8c58/metrics/training_metrics in\n0.00 seconds and matches MLMD records.\nFetching artifact=artifacts/model/model.pkl,\nsurl=https://osdf-director.osg-htc.org/fdp-hpe/cmf_test/files/md5/a7/ea44151cfca\n33d7cfe3a1760973373 to ./artifacts/model/model.pkl\nobject artifacts/model/model.pkl downloaded at ./artifacts/model/model.pkl in\n0.00 seconds and matches MLMD records.\nSUCCESS: Number of files downloaded = 8.\n</code></pre>"},{"location":"cmf_client/cmf_osdf/#pulling-artifacts-without-cache-directly-form-origin","title":"Pulling Artifacts (without Cache. Directly form Origin)","text":"<ul> <li>Pulling artifacts without cache</li> <li>The --cache path is not specified. CMF automatically pulls from the origin which is configured  </li> <li>The artifacts for the pipeline: <code>Test-env</code> is pulled into a test2/ folder (as an example)</li> <li>CMF looks at it's MLMD index and fetches the necessary files form the cache when configured. </li> </ul> <pre><code>(cmf) tripataa@ai07:~/cmf-examples/test2$ ls\nmlmd\n(cmf) tripataa@ai07:~/cmf-examples/test2$ cmf init osdfremote --path https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test \\\n        --key-id XXXX \\\n        --key-path ~/private_hpe.pem \\\n        --key-issuer https://t.nationalresearchplatform.org/fdp-hpe \\\n--git-remote-url https://github.com/user/experiment-repo.git\ngit_dir /home/tripataa/cmf-examples/test2/.git\nStarting git init.\n*** Note: CMF will check out a new branch in git to commit the metadata files\n***\n*** The checked out branch is master. ***\ngit init complete.\nStarting cmf init.\nSetting 'osdf' as a default remote.\nSUCCESS: cmf init complete.\n(cmf) tripataa@ai07:~/cmf-examples/test2$ cmf artifact pull -p Test-env\nFetching\nartifact=/home/tripataa/cmf-examples/test2/cmf_artifacts/python_env_4927239f4c14\nb700b637ff03ab787e65.yaml,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/49/27239f4c\n14b700b637ff03ab787e65 to\n./cmf_artifacts/python_env_4927239f4c14b700b637ff03ab787e65.yaml\nobject\n/home/tripataa/cmf-examples/test2/cmf_artifacts/python_env_4927239f4c14b700b637f\nf03ab787e65.yaml downloaded at\n./cmf_artifacts/python_env_4927239f4c14b700b637ff03ab787e65.yaml in 0.00 seconds\nand matches MLMD records.\nFetching artifact=/home/tripataa/cmf-examples/test2/artifacts/data.xml.gz,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/23/6d9502e0\n283d91f689d7038b8508a2 to ./artifacts/data.xml.gz\nobject /home/tripataa/cmf-examples/test2/artifacts/data.xml.gz downloaded at\n./artifacts/data.xml.gz in 0.02 seconds and matches MLMD records.\nFetching artifact=/home/tripataa/cmf-examples/test2/artifacts/parsed/train.tsv,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/32/b715ef0d\n71ff4c9e61f55b09c15e75 to ./artifacts/parsed/train.tsv\nobject /home/tripataa/cmf-examples/test2/artifacts/parsed/train.tsv downloaded\nat ./artifacts/parsed/train.tsv in 0.03 seconds and matches MLMD records.\nFetching artifact=/home/tripataa/cmf-examples/test2/artifacts/parsed/test.tsv,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/6f/597d341c\neb7d8fbbe88859a892ef81 to ./artifacts/parsed/test.tsv\nobject /home/tripataa/cmf-examples/test2/artifacts/parsed/test.tsv downloaded at\n./artifacts/parsed/test.tsv in 0.01 seconds and matches MLMD records.\nFetching\nartifact=/home/tripataa/cmf-examples/test2/artifacts/features/train.pkl,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/f2/17d9bdb2\n523c54e71dcb95fcf16ff2 to ./artifacts/features/train.pkl\nobject /home/tripataa/cmf-examples/test2/artifacts/features/train.pkl downloaded\nat ./artifacts/features/train.pkl in 0.01 seconds and matches MLMD records.\nFetching artifact=/home/tripataa/cmf-examples/test2/artifacts/features/test.pkl,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/30/4cfed883\nc9c0cecaeb7f0079e4b98c to ./artifacts/features/test.pkl\nobject /home/tripataa/cmf-examples/test2/artifacts/features/test.pkl downloaded\nat ./artifacts/features/test.pkl in 0.00 seconds and matches MLMD records.\nFetching\nartifact=/home/tripataa/cmf-examples/test2/cmf_artifacts/f3d287ba-5cf8-11f0-bf02\n-d4c9efce8c58/metrics/training_metrics,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/64/d2da21bb\n6249afc9ea5a3a5cd67cc7 to\n./cmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce8c58/metrics/training_metrics\nobject\n/home/tripataa/cmf-examples/test2/cmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce\n8c58/metrics/training_metrics downloaded at\n./cmf_artifacts/f3d287ba-5cf8-11f0-bf02-d4c9efce8c58/metrics/training_metrics in\n0.00 seconds and matches MLMD records.\nFetching artifact=/home/tripataa/cmf-examples/test2/artifacts/model/model.pkl,\nsurl=https://fdp-origin.labs.hpe.com:8443/fdp-hpe/cmf_test/files/md5/a7/ea44151c\nfca33d7cfe3a1760973373 to ./artifacts/model/model.pkl\nobject /home/tripataa/cmf-examples/test2/artifacts/model/model.pkl downloaded at\n./artifacts/model/model.pkl in 0.00 seconds and matches MLMD records.\nSUCCESS: Number of files downloaded = 8.\n(cmf) tripataa@ai07:~/cmf-examples/test2$\n</code></pre>"},{"location":"cmf_client/cmf_osdf/#multi-site","title":"Multi-Site","text":"<p>CMF supports multi-site deployments where artifacts can be shared across different institutions through OSDF's distributed federation. In a multi-site setup:</p> <ul> <li>Origins can be deployed at different institutions to store local data</li> <li>Caches are distributed geographically to provide optimal performance</li> <li>OSDF Director coordinates access to the nearest available cache</li> <li>Token-based authentication ensures secure access across sites</li> <li>CMF metadata tracks artifact lineage across the entire federation</li> </ul> <p>This enables collaborative workflows where: 1. Researchers at Site A can push artifacts to their local origin 2. Researchers at Site B can pull those same artifacts through their nearest cache 3. All metadata and provenance information is preserved across sites 4. Performance is optimized through intelligent cache selection</p> <p>TBD</p>"},{"location":"cmf_client/minio-server/","title":"MinIO S3 Artifact Repo Setup","text":""},{"location":"cmf_client/minio-server/#steps-to-set-up-a-minio-server","title":"Steps to set up a MinIO server","text":"<p>Object storage is an abstraction layer above the file system that provides an API to interact with data. MinIO is an easy way to start working with object storage. It is compatible with S3, easy to deploy, manage locally, and scale if needed.</p> <p>Follow the steps below to set up a MinIO server:</p> <ol> <li> <p>Copy the contents of the <code>example-get-started</code> directory to a separate directory outside the cmf repository.</p> </li> <li> <p>Check whether cmf is initialized.    <pre><code>cmf init show\n</code></pre>    If cmf is not initialized, the following message will appear on the screen:    <pre><code>'cmf' is not configured.\nExecute the 'cmf init' command.\n</code></pre></p> </li> <li> <p>Execute the following command to initialize the MinIO S3 bucket as a CMF artifact repository:     <pre><code>cmf init minioS3 --url s3://dvc-art --endpoint-url http://x.x.x.x:9000 \\\n  --access-key-id minioadmin --secret-key minioadmin \\\n  --git-remote-url https://github.com/user/experiment-repo.git \\\n  --cmf-server-url http://x.x.x.x:80  --neo4j-user neo4j \\\n  --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre></p> </li> </ol> <p>Here, \"dvc-art\" is provided as an example bucket name. However, users can change it as needed. If the user chooses to change it, they will need to update the Dockerfile for minioS3 accordingly.</p> <ol> <li> <p>Execute <code>cmf init show</code> to check the CMF configuration. The sample output looks as follows:    <pre><code>remote.minio.url=s3://bucket-name\nremote.minio.endpointurl=http://localhost:9000\nremote.minio.access_key_id=minioadmin\nremote.minio.secret_access_key=minioadmin\ncore.remote=minio\n</code></pre></p> </li> <li> <p>Build a MinIO server using a Docker container. The <code>docker-compose.yml</code> available in the    <code>example-get-started</code> directory provides two services: <code>minio</code> and <code>aws-cli</code>. The user    will initialize the repository with the bucket name, storage URL, and credentials to    access MinIO.</p> </li> <li> <p>Execute the following command to start the Docker container. The MYIP variable is the IP address of the machine on which you are executing the following command. The following command requires root privileges.    <pre><code>MYIP=XX.XX.XXX.XXX docker-compose up\n</code></pre>    or    <pre><code>MYIP=XX.XX.XXX.XXX docker compose up\n</code></pre>    You should see output confirming that MinIO is up and running.</p> <p>Also, you can adjust <code>$MYIP</code> in <code>examples/example-get-started/docker-compose.yml</code> to specify     the server IP and run the <code>docker compose</code> command without specifying MYIP above.</p> </li> <li> <p>Login into <code>remote.minio.endpointurl</code> (in the above example - http://localhost:9000) using    the access-key and the secret-key defined in the <code>cmf init</code> command.</p> </li> <li> <p>The following image is a snapshot of this example using a MinIO server and a bucket named 'dvc-art'.</p> </li> </ol> <p></p>"},{"location":"cmf_client/neo4j_docker/","title":"Start Neo4j with Docker","text":""},{"location":"cmf_client/neo4j_docker/#prerequisite","title":"\ud83d\udee0\ufe0f Prerequisite","text":"<p>Docker should be installed.</p>"},{"location":"cmf_client/neo4j_docker/#command-to-start-neo4j-docker-container","title":"\ud83d\ude80 Command to Start Neo4j Docker Container","text":"<pre><code>docker run     --name testneo4j \\\n        -p7474:7474 -p7687:7687  \\\n        -d   \\\n        -v $HOME/neo4j/data:/data  \\\n        -v $HOME/neo4j/logs:/logs     \\\n        -v $HOME/neo4j/import:/var/lib/neo4j/import   \\\n        -v $HOME/neo4j/plugins:/plugins   \\\n        --env NEO4J_AUTH=neo4j/test1234    \\\n        neo4j:latest\n</code></pre> <p>The <code>docker run</code> command creates and starts a container. On the next line, <code>--name testneo4j</code> defines the name we want to use for the container as <code>testneo4j</code>.</p> <p>Using the <code>-p</code> option with ports <code>7474</code> and <code>7687</code> allows us to expose and listen for traffic on both the HTTP and Bolt ports. Having the HTTP port means we can connect to our database with Neo4j Browser, and the Bolt port means efficient and type-safe communication requests between other layers and the database.</p> <p>Next, we have <code>-d</code>. This detaches the container to run in the background, meaning we can access the container separately and see into all of its processes.</p> <p>The next several lines start with the <code>-v</code> option. These lines define volumes we want to bind in our local directory structure so we can access certain files locally.</p> <ul> <li>The first one is for our <code>/data</code> directory, which stores the system information and graph data.</li> <li>The second <code>-v</code> option is for the <code>/logs</code> directory. Outputting the Neo4j logs to a place outside the container ensures we can troubleshoot any errors in Neo4j, even if the container crashes.</li> <li>The third line with the <code>-v</code> option binds the import directory, so we can copy CSV or other flat files into that directory for importing into Neo4j. Load scripts for importing that data can also be placed in this folder for us to execute.</li> <li>The next <code>-v</code> option line sets up our plugins directory.</li> </ul> <p>On the next line with the <code>--env</code> parameter, we initiate our Neo4j instance with a username and password. Neo4j automatically sets up basic authentication with the <code>neo4j</code> username as a foundation for security. Since it will initiate authentication and require a password change when first connecting, we can handle all of that in this parameter.</p> <p>Finally, the last line of the command above references the Docker image we want to pull from DockerHub (<code>neo4j</code>), as well as any specified version (in this case, just the latest edition).</p> <p>\ud83d\udd17 More details</p> <p>To access Neo4j from browser, open: \u27a1\ufe0f http://IP:7474/</p> <p>Replace <code>IP</code> with your system IP address.</p>"},{"location":"cmf_client/neo4j_docker/#some-useful-neo4j-cypher-queries","title":"\ud83e\uddea Some Useful Neo4j Cypher Queries","text":"<p>Replace <code>&lt;pipelinename&gt;</code> with your actual pipeline name.</p>"},{"location":"cmf_client/neo4j_docker/#1-artifact-lineage-with-processing-steps","title":"1. Artifact Lineage with Processing Steps","text":"<pre><code>MATCH (a:Execution{pipeline_name:'&lt;pipelinename&gt;'})-[r]-(b) \nWHERE (b:Dataset or b:Model or b:Metrics) \nRETURN a, r, b \n</code></pre>"},{"location":"cmf_client/neo4j_docker/#2-artifact-lineage","title":"2. Artifact Lineage","text":"<pre><code>MATCH (b) \nWHERE (b:Dataset or b:Model or b:Metrics) \nAND '&lt;pipelinename&gt;' IN b.pipeline_name \nRETURN b\n</code></pre>"},{"location":"cmf_client/neo4j_docker/#3-lineage-of-processing-steps","title":"3. Lineage of Processing Steps","text":"<pre><code>MATCH (n:Execution{pipeline_name:'&lt;pipelinename&gt;'}) \nRETURN n\n</code></pre>"},{"location":"cmf_client/neo4j_docker/#4-clean-up","title":"4. Clean Up","text":"<p>All execution/stage nodes related to a pipeline</p> <pre><code>MATCH (n {pipeline_name:'&lt;pipelinename&gt;'}) \nDETACH DELETE n\n</code></pre> <p>All Dataset nodes</p> <pre><code>MATCH (n) \nWHERE '&lt;pipelinename&gt;' IN n.pipeline_name \nDETACH DELETE n\n</code></pre>"},{"location":"cmf_client/ssh-setup/","title":"SSH Remote Artifact Repo Setup","text":""},{"location":"cmf_client/ssh-setup/#steps-to-set-up-an-ssh-remote-repo","title":"Steps to set up an SSH Remote Repo","text":"<p>SSH (Secure Shell) remote storage refers to using the SSH protocol to securely access and manage files and data on a remote server or storage system over a network. SSH is a cryptographic network protocol that allows secure communication and data transfer between a local computer and a remote server.</p> <p>Proceed with the following steps to set up an SSH Remote Repository:</p> <ol> <li>Initialize the <code>project directory</code> with an SSH remote.</li> <li> <p>Check whether cmf is initialized in your project directory with the following command.    <pre><code>cmf init show\n</code></pre>    If cmf is not initialized, the following message will appear on the screen.    <pre><code>'cmf' is not configured.\nExecute the 'cmf init' command.\n</code></pre></p> </li> <li> <p>Execute the following command to initialize the SSH remote storage as a CMF artifact repository.     <pre><code>cmf init sshremote --path ssh://127.0.0.1/home/user/ssh-storage \\\n    --user XXXXX --port 22 --password example@123 \\\n    --git-remote-url https://github.com/user/experiment-repo.git \\\n    --cmf-server-url http://127.0.0.1:80\n</code></pre>     &gt; When running <code>cmf init sshremote</code>, ensure that the specified IP address allows access for \\     the specified user XXXX. If the IP address or user does not exist, this command will fail.</p> </li> <li> <p>Execute <code>cmf init show</code> to check the CMF configuration.</p> </li> <li>To troubleshoot SSH permissions-related issues, check the <code>/etc/ssh/sshd_config</code> file on the    remote machine. This configuration file serves as the primary starting point for diagnosing    and resolving SSH permission-related challenges.</li> </ol>"},{"location":"cmf_client/tensorflow_guide/","title":"How to Use TensorBoard with CMF","text":"<ol> <li> <p>Copy the contents of the <code>example-get-started</code> directory from <code>cmf/examples/example-get-started</code> into a separate directory outside the CMF repository.</p> </li> <li> <p>Execute the following command to install the TensorFlow library in the     current directory:     <pre><code>pip install tensorflow\n</code></pre></p> </li> <li> <p>Create a new Python file (e.g., <code>tensorflow_log.py</code>) and copy the following code:</p> <pre><code>import datetime\nimport tensorflow as tf\n\nmnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\ndef create_model():\n     return tf.keras.models.Sequential([\n          tf.keras.layers.Flatten(input_shape=(28, 28), name='layers_flatten'),\n          tf.keras.layers.Dense(512, activation='relu', name='layers_dense'),\n          tf.keras.layers.Dropout(0.2, name='layers_dropout'),\n          tf.keras.layers.Dense(10, activation='softmax', name='layers_dense_2')\n     ])\n\nmodel = create_model()\nmodel.compile(\n     optimizer='adam',\n     loss='sparse_categorical_crossentropy',\n     metrics=['accuracy']\n)\n\nlog_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\nmodel.fit(x=x_train,y=y_train,epochs=5,validation_data=(x_test, y_test),callbacks=[tensorboard_callback])\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n\ntrain_dataset = train_dataset.shuffle(60000).batch(64)\ntest_dataset = test_dataset.batch(64)\n\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\noptimizer = tf.keras.optimizers.Adam()\n\n# Define our metrics\ntrain_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\ntest_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')\n\ndef train_step(model, optimizer, x_train, y_train):\n     with tf.GradientTape() as tape:\n          predictions = model(x_train, training=True)\n          loss = loss_object(y_train, predictions)\n     grads = tape.gradient(loss, model.trainable_variables)\n     optimizer.apply_gradients(zip(grads, model.trainable_variables))\n     train_loss(loss)\n     train_accuracy(y_train, predictions)\n\ndef test_step(model, x_test, y_test):\n     predictions = model(x_test)\n     loss = loss_object(y_test, predictions)\n     test_loss(loss)\n     test_accuracy(y_test, predictions)\n\ncurrent_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntrain_log_dir = 'logs/gradient_tape/' + current_time + '/train'\ntest_log_dir = 'logs/gradient_tape/' + current_time + '/test'\ntrain_summary_writer = tf.summary.create_file_writer(train_log_dir)\ntest_summary_writer = tf.summary.create_file_writer(test_log_dir)\n\nmodel = create_model()  # reset our model\nEPOCHS = 5\nfor epoch in range(EPOCHS):\n     for (x_batch, y_batch) in train_dataset:\n          train_step(model, optimizer, x_batch, y_batch)\n     with train_summary_writer.as_default():\n          tf.summary.scalar('loss', train_loss.result(), step=epoch)\n          tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n\n     for (x_batch, y_batch) in test_dataset:\n          test_step(model, x_batch, y_batch)\n     with test_summary_writer.as_default():\n          tf.summary.scalar('loss', test_loss.result(), step=epoch)\n          tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n     template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n     print(template.format(\n          epoch + 1,\n          train_loss.result(),\n          train_accuracy.result() * 100,\n          test_loss.result(),\n          test_accuracy.result() * 100\n     ))\n</code></pre> <p>For more detailed information, check out the TensorBoard documentation.</p> </li> <li> <p>Execute the TensorFlow log script using the following command:     <pre><code>python3 tensorflow_log.py\n</code></pre></p> </li> <li> <p>The above script will automatically create a <code>logs</code> directory inside your current directory.</p> </li> <li> <p>Start the cmf-server and configure the cmf-client.</p> </li> <li> <p>Use the following command to run the test script, which will generate the MLMD file:     <pre><code>sh test_script.sh\n</code></pre> <code>Note</code> - <code>MLMD</code> stands for ML Metadata, which is typically stored as a SQLite file generated during pipeline runs. The file extension and name may vary depending on your setup.</p> </li> <li> <p>Use the following command to push the generated MLMD and TensorFlow log files to the CMF server:     <pre><code>cmf metadata push -p 'pipeline-name' -t 'tensorboard-log-file-name'\n</code></pre></p> </li> <li> <p>Go to the CMF server and navigate to the TensorBoard tab. You will see an interface similar to the following image: </p> </li> </ol>"},{"location":"cmf_server/index_1/","title":"Getting started with cmf-server","text":"<p>cmf-server is a key interface for the user to explore and track their ML training runs, storing the metadata file on the cmf-server. The user can retrieve the saved metadata file and can view the content of the saved metadata file using the UI provided by the cmf-server.</p>"},{"location":"cmf_server/index_1/#setup-a-cmf-server","title":"Setup a cmf-server","text":"<p>There are two ways to start a cmf server -</p> <ul> <li>Using docker compose file</li> <li>Using docker run</li> </ul>"},{"location":"cmf_server/index_1/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Clone the GitHub repository.    <pre><code>git clone https://github.com/HewlettPackard/cmf\n</code></pre></p> </li> <li> <p>Install Docker Engine with non-root user privileges.</p> </li> <li>Install Docker Compose Plugin. <p>In earlier versions of Docker Compose, <code>docker compose</code> was independent of Docker. Hence, <code>docker-compose</code> was the command. However, after the introduction of Docker Compose Desktop V2, the compose command became part of Docker Engine. The recommended way to install Docker Compose is by installing a Docker Compose plugin on Docker Engine. For more information - Docker Compose Reference.</p> </li> <li>Docker Proxy Settings are needed for some of the server packages. Refer to the official Docker documentation for comprehensive instructions: Configure the Docker Client for Proxy.</li> </ol>"},{"location":"cmf_server/index_1/#using-docker-compose-file","title":"Using <code>docker compose</code> file","text":"<p>This is the recommended way as docker compose starts cmf-server, postgres db and ui-server in one go. It is neccessary to start postgres db before cmf-server.</p> <ol> <li>Go to root <code>cmf</code> directory. </li> <li>Replace <code>xxxx</code> with your user-name in docker-compose-server.yml available in the root cmf directory.     <pre><code>......\nservices:\nserver:\n  image: server:latest\n  volumes:\n     - /home/xxxx/cmf-server/data:/cmf-server/data                 # for example /home/hpe-user/cmf-server/data:/cmf-server/data\n     - /home/xxxx/cmf-server/data/static:/cmf-server/data/static   # for example /home/hpe-user/cmf-server/data/static:/cmf-server/data/static\n  container_name: cmf-server\n  build:\n....\n</code></pre></li> <li>Create a <code>.env</code> file in the same directory as <code>docker-compose-server.yml</code> and add the necessary environment variables.    <pre><code>POSTGRES_USER: myuser\nPOSTGRES_PASSWORD: mypassword\nPOSTGRES_PORT: 5470\n</code></pre> <p>\u26a0\ufe0fWarning: Avoid using <code>@</code> character in <code>POSTGRES_PASSWORD</code> to prevent connection issues.</p> </li> <li> <p>Execute one of the following commands to start both containers. <code>IP</code> variable is the IP address and <code>hostname</code> is the host name of the machine on which you are executing the following command.    <pre><code>IP=200.200.200.200 docker compose -f docker-compose-server.yml up\n           OR\nhostname=host_name docker compose -f docker-compose-server.yml up\n</code></pre></p> <p>Replace <code>docker compose</code> with <code>docker-compose</code> for older versions. Also, you can adjust <code>$IP</code> in <code>docker-compose-server.yml</code> to reflect the server IP and run the <code>docker compose</code> command without specifying     IP=200.200.200.200.      <pre><code>.......\nenvironment:\nREACT_APP_MY_IP: ${IP}\n......\n</code></pre></p> </li> <li> <p>Stop the containers.     <pre><code>  docker compose -f docker-compose-server.yml stop\n</code></pre></p> </li> </ol> <p>It is necessary to rebuild images for cmf-server and ui-server after <code>cmf version update</code> or after pulling the latest cmf code from git.</p> <p>OR</p>"},{"location":"cmf_server/index_1/#using-docker-run-command","title":"Using <code>docker run</code> command","text":"<ol> <li> <p>Install cmflib on your system.</p> </li> <li> <p>Go to <code>cmf/server</code> directory.    <pre><code>cd server\n</code></pre></p> </li> <li> <p>List all docker images.    <pre><code>docker images\n</code></pre></p> </li> <li> <p>Execute the following command to create a <code>cmf-server</code> docker image.    <pre><code>Usage:  docker build -t [image_name] -f ./Dockerfile ../\n</code></pre>    Example:    <pre><code>docker build -t server_image -f ./Dockerfile ../\n</code></pre> <code>Note</code> - <code>'../'</code>  represents the Build context for the docker image.</p> </li> <li> <p>Launch a new docker container using the image with directory /home/user/cmf-server/data mounted.    <code>Pre-requisite: mkdir /home/&lt;user&gt;/cmf-server/data/static</code> <pre><code>Usage: docker run --name [container_name] -p 0.0.0.0:8080:80 -v /home/&lt;user&gt;/cmf-server/data:/cmf-server/data -e MYIP=XX.XX.XX.XX [image_name]\n</code></pre>    Example:    <pre><code>docker run --name cmf-server -p 0.0.0.0:8080:80 -v /home/user/cmf-server/data:/cmf-server/data -e MYIP=0.0.0.0 server_image\n</code></pre></p> </li> <li> <p>After the cmf-server container is up, start <code>ui-server</code>. Go to <code>cmf/ui</code> folder.    <pre><code>cd cmf/ui\n</code></pre></p> </li> <li> <p>Execute the below-mentioned command to create a <code>ui-server</code> docker image.    <pre><code>Usage:  docker build -t [image_name] -f ./Dockerfile ./\n</code></pre>    Example:    <pre><code>docker build -t ui_image -f ./Dockerfile ./\n</code></pre></p> </li> <li> <p>Launch a new docker container for UI.    <pre><code>Usage: docker run --name [container_name] -p 0.0.0.0:3000:3000 -e REACT_APP_MY_IP=XX.XX.XX.XX [image_name]\n</code></pre>    Example:    <pre><code>docker run --name ui-server -p 0.0.0.0:3000:3000 -e REACT_APP_MY_IP=0.0.0.0 ui_image\n</code></pre>       Note:       If you face issues regarding <code>Libzbar-dev</code> similar to the snapshot, add proxies to '/.docker/config.json'</p> <p></p> <pre><code>{\n   proxies: {\n        \"default\": {\n                     \"httpProxy\": \"http://web-proxy.labs.xxxx.net:8080\",\n                     \"httpsProxy\": \"http://web-proxy.labs.xxxx.net:8080\",\n                     \"noProxy\": \".labs.xxxx.net,127.0.0.0/8\"\n             }\n         }\n }\n</code></pre> </li> <li> <p>To stop the docker container.     <pre><code>docker stop [container_name]\n</code></pre></p> </li> <li> <p>To delete the docker container.     <pre><code>docker rm [container_name]\n</code></pre></p> </li> <li> <p>To remove the docker image.     <pre><code>docker image rm [image_name]\n</code></pre></p> </li> </ol>"},{"location":"cmf_server/index_1/#api-reference","title":"API Reference","text":"<p>cmf-server APIs are organized around FastAPI. They accept and return JSON-encoded request bodies and responses and return standard HTTP response codes.</p>"},{"location":"cmf_server/index_1/#list-of-apis","title":"List of APIs","text":"Method URL Description <code>POST</code> <code>/mlmd_push</code> Pushes JSON-encoded data to the cmf-server. <code>GET</code> <code>/mlmd_pull/{pipeline_name}</code> Retrieves an MLMD file from the cmf-server. <code>GET</code> <code>/executions/{pipeline_name}</code> Retrieves all executions from the cmf-server. <code>GET</code> <code>/list-of-executions/{pipeline_name}</code> Retrieves a list of execution types. <code>GET</code> <code>/execution-lineage/tangled-tree/{uuid}/{pipeline_name}</code> Retrieves a dictionary of nodes and links for a given execution type. <code>GET</code> <code>/artifacts/{pipeline_name}/{type}</code> Retrieves all artifacts of the specified type from the cmf-server. <code>GET</code> <code>/artifact-lineage/tangled-tree/{pipeline_name}</code> Retrieves a nested list of dictionaries with <code>id</code> and <code>parents</code> keys for artifacts. <code>GET</code> <code>/artifact_types</code> Retrieves a list of artifact types. <code>GET</code> <code>/pipelines</code> Retrieves all pipelines present in the MLMD file. <code>POST</code> <code>/tensorboard</code> Uploads TensorBoard logs to the cmf-server. <code>GET</code> <code>/model-card</code> Retrieves model data, input/output artifacts, and executions for a model. <code>GET</code> <code>/artifact-execution-lineage/tangled-tree/{pipeline_name}</code> Retrieves a nested list of dictionaries with <code>id</code> and <code>parents</code> keys for artifacts and executions. <code>POST</code> <code>/python-env</code> Pushes Python environment data to the cmf-server. <code>GET</code> <code>/python-env</code> Retrieves environment data from the <code>/cmf-server/data/env</code> folder."},{"location":"cmf_server/index_1/#http-response-status-codes","title":"HTTP Response Status codes","text":"Code Title Description <code>200</code> <code>OK</code> mlmd is successfully pushed (e.g. when using <code>GET</code>, <code>POST</code>). <code>400</code> <code>Bad request</code> When the cmf-server is not available. <code>500</code> <code>Internal server error</code> When an internal error has happened"},{"location":"cmf_server/metahub-tab-usage/","title":"Introduction to Metahub Feature","text":"<p>The Metahub feature is introduced to synchronize metadata between two CMF servers. This document explains how to use this feature effectively via the GUI.</p>"},{"location":"cmf_server/metahub-tab-usage/#steps-to-use-metahub-feature","title":"Steps to Use Metahub feature","text":""},{"location":"cmf_server/metahub-tab-usage/#1-start-the-cmf-server","title":"1. Start the CMF Server","text":"<p>Ensure that the CMF server is up and running. you can follow below document.</p> <p>\ud83d\udcc4 Guide to start cmf-server</p>"},{"location":"cmf_server/metahub-tab-usage/#2-navigate-to-metahub-tab","title":"2. Navigate to Metahub Tab","text":"<p>In the GUI, locate and click the Metahub tab from the navigation panel.</p> <p>After clicking the Metahub tab, you will see three tabs:</p> <ul> <li>Registration \u2192 Register a new server for syncing.</li> <li>Sync Server \u2192 Perform sync with a registered server.</li> <li>Registered Server \u2192 View the list of registered servers.</li> </ul> <p></p>"},{"location":"cmf_server/metahub-tab-usage/#registration-tab","title":"Registration Tab","text":"<p>The Registration tab allows you to register another server that you want to sync with.</p>"},{"location":"cmf_server/metahub-tab-usage/#functionality","title":"\ud83d\udcdd Functionality:","text":"<ol> <li>Register the target server you want to sync with.</li> <li>You can provide either:<ul> <li>Server Name + IP Address or </li> <li>Server Name + Hostname</li> </ul> </li> <li>Click Submit. You\u2019ll receive a message confirming whether the server registration was successful.</li> </ol>"},{"location":"cmf_server/metahub-tab-usage/#sync-server-tab","title":"Sync Server Tab","text":"<p>The Sync Server tab is used to sync metadata with a registered target server.</p>"},{"location":"cmf_server/metahub-tab-usage/#functionality_1","title":"\ud83d\udcdd Functionality:","text":"<ol> <li>A dropdown will show all the registered target servers.</li> <li>Select a server from the list.</li> <li>Click to sync. If the selected server is available and properly registered, the sync will succeed. Otherwise, it will fail.</li> </ol>"},{"location":"cmf_server/metahub-tab-usage/#_1","title":"\ud83c\udfc3\ud83c\udffc\u200d\u2642\ufe0f Metahub","text":""},{"location":"cmf_server/metahub-tab-usage/#registered-server-tab","title":"Registered Server Tab","text":"<p>This section displays all registered servers in a table format.</p>"},{"location":"cmf_server/metahub-tab-usage/#functionality_2","title":"\ud83d\udcdd Functionality:","text":"<ol> <li>View the list of all registered target servers.</li> <li>The table includes a <code>last_sync_time</code> column to indicate when each server was last successfully synced.</li> </ol> <p>\ud83d\udccc Make sure all servers involved are running and reachable via the provided IP or hostname.</p>"},{"location":"cmflib/","title":"cmflib NEEDS TO BE UPDATED","text":"<p>The <code>cmflib</code> package is the foundational Python library that provides the core metadata tracking capabilities for the Common Metadata Framework (CMF). It offers a unified API for logging metadata across distributed AI/ML pipelines, integrating with multiple storage backends and versioning systems.</p> <p>This document covers the core library components, main API classes, and integration mechanisms. For CLI usage patterns, see Quick Start with cmf-client. For server-side components, see cmf-server. - NEEDS TO BE UPDATED</p>"},{"location":"cmflib/#core-architecture","title":"Core Architecture","text":"<p>Complex ML projects rely on <code>ML pipelines</code> to train and test ML models. An ML pipeline is a sequence of stages where each stage performs a particular task, such as data loading, pre-processing, ML model training, and testing stages. Each stage can have multiple Executions which:</p> <ul> <li>consume <code>inputs</code> and produce <code>outputs</code>.</li> <li>are parameterized by parameters that guide the process of producing outputs.</li> </ul> <p></p> <p>CMF uses the abstractions of <code>Pipeline</code>, <code>Context</code>, and <code>Executions</code> to store the metadata of complex ML pipelines. Each pipeline has a name. Users provide it when they initialize the CMF. Each stage is represented by a <code>Context</code> object. Metadata associated with each run of a stage is captured in the Execution object. Inputs and outputs of Executions can be logged as dataset, model, or metrics. While parameters of executions are recorded as properties of executions.</p> <p></p>"},{"location":"cmflib/#main-api-cmfcmf","title":"Main API (cmf.Cmf)","text":"<p>The <code>Cmf</code> class is the primary interface for metadata tracking in CMF. It provides methods for creating pipelines, contexts, executions, and logging artifacts.</p>"},{"location":"cmflib/#key-methods","title":"Key Methods","text":"Method Purpose Usage <code>__init__(filename, pipeline_name)</code> Initialize CMF instance <code>cmf = Cmf(filename=\"mlmd\", pipeline_name=\"my_pipeline\")</code> <code>create_context(pipeline_stage)</code> Create a pipeline stage context <code>context = cmf.create_context(pipeline_stage=\"train\")</code> <code>create_execution(execution_type)</code> Create an execution within a context <code>execution = cmf.create_execution(execution_type=\"training_run\")</code> <code>log_dataset(url, event, custom_properties)</code> Log dataset artifacts <code>cmf.log_dataset(url=\"data.csv\", event=\"input\")</code> <code>log_model(path, event, model_framework)</code> Log model artifacts <code>cmf.log_model(path=\"model.pkl\", event=\"output\")</code> <code>log_metrics(metrics_name, custom_properties)</code> Log metrics <code>cmf.log_metrics(metrics_name=\"accuracy\", custom_properties={\"value\": 0.95})</code> 1 Init2 Stage type3 New execution4 Log Artifacts <p>Start tracking the pipeline metadata by initializing the CMF runtime. The metadata will be associated with the pipeline named <code>test_pipeline</code>. <pre><code>from cmflib.cmf import Cmf\nfrom ml_metadata.proto import metadata_store_pb2 as mlpb\n\ncmf = Cmf(\n    filename=\"mlmd\",\n    pipeline_name=\"test_pipeline\",\n)\n</code></pre></p> <p>Before we can start tracking metadata, we need to let CMF know about the stage type. This is not yet associated with this particular execution. <pre><code>context: mlmd.proto.Context = cmf.create_context(\n    pipeline_stage=\"train\"\n)\n</code></pre></p> <p>Now we can create a new stage execution associated with the <code>train</code> stage. The CMF always creates a new execution, and will adjust its name, so it's unique. This is also the place where we can log execution <code>parameters</code> like seed, hyper-parameters, etc. <pre><code>execution: mlmd.proto.Execution = cmf.create_execution(\n    execution_type=\"train\",\n    custom_properties = {\"num_epochs\": 100, \"learning_rate\": 0.01}\n)\n</code></pre></p> <p>Finally, we can log an input (train dataset), and once trained, an output (ML model) artifact. <pre><code>cmf.log_dataset(\n    'artifacts/test_dataset.csv',   # Dataset path\n    \"input\"                         # This is INPUT artifact\n)\ncmf.log_model(\n    \"artifacts/model.pkl\",          # Model path\n    event=\"output\"                  # This is OUTPUT artifact\n)\n</code></pre></p>"},{"location":"cmflib/#quick-example","title":"Quick Example","text":"<p>Go through the Getting Started page to learn more about CMF API usage.</p>"},{"location":"cmflib/#api-overview","title":"API Overview","text":"<p>Import CMF. <pre><code>from cmflib import cmf\n</code></pre></p> <p>Initialize CMF. The [CMF][cmflibcmfcmf] object is responsible for managing a CMF backend to record the pipeline metadata. Internally, it creates a pipeline abstraction that groups individual stages and their executions. All stages, their executions, and produced artifacts will be associated with a pipeline with the given name. <pre><code>cmf = cmf.Cmf(\n   filename=\"mlmd\",                # Path to ML Metadata file.\n   pipeline_name=\"mnist\"           # Name of an ML pipeline.\n)\n</code></pre></p> <p>Define a stage. An ML pipeline can have multiple stages, and each stage can be associated with multiple executions. A stage is described by a context, which specifies its name and optional properties. You can create a context using the create_context method: <pre><code>context = cmf.create_context(\n    pipeline_stage=\"download\",     # Stage name\n    custom_properties={            # Optional properties\n        \"uses_network\": True,      #  Downloads from the Internet\n        \"disk_space\": \"10GB\"       #  Needs this much space\n    }\n)\n</code></pre></p> <p>Create a stage execution. A stage in an ML pipeline can have multiple executions. Every run is marked as an execution. This API helps to track the metadata associated with the execution, like stage parameters (e.g., number of epochs and learning rate for train stages). The stage execution name does not need to be the same as the name of its context. Moreover, the CMF will adjust this name to ensure every execution has a unique name. The CMF will internally associate this execution with the context created previously. Stage executions are created by calling the create_execution method. <pre><code>execution = cmf.create_execution(\n    execution_type=\"download\",            # Execution name.\n    custom_properties = {                 # Execution parameters\n        \"url\": \"https://a.com/mnist.gz\"   #  Data URL.\n    }\n)\n</code></pre></p> <p>Log artifacts. A stage execution can consume (inputs) and produce (outputs) multiple artifacts (datasets, models, and performance metrics). The path of these artifacts must be relative to the project (repository) root path. Artifacts might have optional metadata associated with them. These metadata could include feature statistics for ML datasets, or useful parameters for ML models (such as, for instance, number of trees in a random forest classifier).</p> <ul> <li> <p>Datasets are logged with the log_dataset method.     <pre><code>cmf.log_dataset('data/mnist.gz', \"input\", custom_properties={\"name\": \"mnist\", \"type\": 'raw'})\ncmf.log_dataset('data/train.csv', \"output\", custom_properties={\"name\": \"mnist\", \"type\": \"train_split\"})\ncmf.log_dataset('data/test.csv', \"output\", custom_properties={\"name\": \"mnist\", \"type\": \"test_split\"})\n</code></pre></p> </li> <li> <p>ML models produced by training stages are logged using the log_model API. ML models can be   both input and output artifacts. The metadata associated with the artifact could be logged as an optional argument.     <pre><code># In train stage\ncmf.log_model(\n   path=\"model/rf.pkl\", event=\"output\", model_framework=\"scikit-learn\", model_type=\"RandomForestClassifier\",\n   model_name=\"RandomForestClassifier:default\"\n)\n\n# In test stage\ncmf.log_model(\n   path=\"model/rf.pkl\", event=\"input\"\n)\n</code></pre></p> </li> <li> <p>Metrics of every optimization step (one epoch of Stochastic Gradient Descent, or one boosting round in   Gradient Boosting Trees) are logged using the log_metric API.     <pre><code># Can be called at every epoch or every step in the training. This is logged to a parquet file and committed at the\n# commit stage.\n\n# Inside training loop\nwhile True:\n     cmf.log_metric(\"training_metrics\", {\"loss\": loss})\ncmf.commit_metrics(\"training_metrics\")\n</code></pre></p> </li> <li> <p>Stage metrics, or final metrics, are logged with the log_execution_metrics   method. These are final metrics of a stage, such as final train or test accuracy.     <pre><code>cmf.log_execution_metrics(\"metrics\", {\"avg_prec\": avg_prec, \"roc_auc\": roc_auc})\n</code></pre></p> </li> </ul> <p>Dataslices are intended to be used to track subsets of the data. For instance, this can be used to track and compare accuracies of ML models on these subsets to identify model bias. Data slices are created with the create_dataslice method. <pre><code>dataslice = cmf.create_dataslice(\"slice-a\")\nfor i in range(1, 20, 1):\n    j = random.randrange(100)\n    dataslice.add_data(\"data/raw_data/\"+str(j)+\".xml\")\ndataslice.commit()\n</code></pre></p>"},{"location":"cmflib/#graph-layer-overview","title":"Graph Layer Overview","text":"<p>The CMF library has an optional <code>graph layer</code> which stores the relationships in a Neo4J graph database. To use the graph layer, the <code>graph</code> parameter in the library init call must be set to true (it is set to false by default). The library reads the configuration parameters of the graph database from the <code>cmf config</code> generated by the <code>cmf init</code> command.</p> <pre><code>cmf init minioS3 --url s3://dvc-art --endpoint-url http://x.x.x.x:9000 --access-key-id minioadmin --secret-key minioadmin --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-url http://x.x.x.x:80  --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre> <p>Here, \"dvc-art\" is provided as an example bucket name. However, users can change it as needed. If the user chooses to change it, they will need to update the Dockerfile for minioS3 accordingly.</p> <p>To use the graph layer, instantiate the CMF with the <code>graph=True</code> parameter: <pre><code>from cmflib import cmf\n\ncmf =  cmf.Cmf(\n   filename=\"mlmd\",\n   pipeline_name=\"anomaly_detection_pipeline\",\n   graph=True\n)\n</code></pre></p>"},{"location":"common-metadata-ontology/readme/","title":"\ud83d\udd17 Ontology","text":""},{"location":"common-metadata-ontology/readme/#common-metadata-ontology","title":"Common Metadata Ontology","text":"<p>Common Metadata Ontology (CMO) integrates and aggregates pipeline metadata from various sources such as Papers-with-Code, OpenML, and Hugging Face. CMF's data model is a manifestation of CMO, specifically designed to capture the pipeline-centric metadata of AI pipelines. It consists of nodes to represent a pipeline, components of a pipeline (stages), relationships to capture interactions among pipeline entities, and properties. CMO offers interoperability of diverse metadata, search, and recommendation with reasoning capabilities. CMO offers flexibility to incorporate various executions implemented for each stage, such as dataset preprocessing, feature engineering, training (including hyperparameter optimization), testing, and evaluation. This enables robust search capabilities to identify the best execution path for a given pipeline. Additionally, CMO also facilitates the inclusion of additional semantic and statistical properties to enhance the richness and comprehensiveness of the metadata associated with them. An overview of CMO can be found below.</p> <p></p> <p>The external link to arrows.app can be found here</p>"},{"location":"common-metadata-ontology/readme/#sample-pipeline-represented-using-cmo","title":"Sample pipeline represented using CMO","text":"<p>The sample figure shows a pipeline titled \"Robust outlier detection by de-biasing VAE likelihoods\" executed for the \"Outlier Detection\" task, focusing on the stage train/test. The model used in the pipeline was \"Variational Autoencoder\". Several datasets were used in the pipeline:</p> <ul> <li>German Traffic Sign</li> <li>Street View House Numbers</li> <li>CelebFaces Attributes dataset.</li> </ul> <p>The corresponding hyperparameters used and the metrics generated as a result of execution are included in the figure. The external link to the source figure created using arrows.app can be found here</p>"},{"location":"common-metadata-ontology/readme/#turtle-syntax","title":"Turtle Syntax","text":"<p>The Turtle format of the formal ontology can be found here</p>"},{"location":"common-metadata-ontology/readme/#properties-of-each-node","title":"Properties of each node","text":"<p>The properties of each node can be found below.</p>"},{"location":"common-metadata-ontology/readme/#pipeline","title":"Pipeline","text":"<p>AI pipeline executed to solve a machine or deep learning task</p>"},{"location":"common-metadata-ontology/readme/#properties","title":"Properties","text":"<ul> <li>pipeline_id</li> <li>pipeline_name</li> <li>pipeline_source</li> <li>source_id</li> <li>custom_properties*</li> </ul>"},{"location":"common-metadata-ontology/readme/#report","title":"Report","text":"<p>Any published text document regarding the pipeline implementation</p>"},{"location":"common-metadata-ontology/readme/#properties_1","title":"Properties","text":"<ul> <li>report_id</li> <li>report_title</li> <li>report_pdf_url</li> <li>source</li> <li>source_id</li> <li>abstract*</li> <li>custom_properties*</li> </ul>"},{"location":"common-metadata-ontology/readme/#task","title":"Task","text":"<p>The AI task for which the pipeline is implemented. Example: image classification</p>"},{"location":"common-metadata-ontology/readme/#properties_2","title":"Properties","text":"<ul> <li>task_id</li> <li>task_name</li> <li>task_description</li> <li>task_type</li> <li>modality</li> <li>category</li> <li>source</li> <li>custom_properties*</li> </ul>"},{"location":"common-metadata-ontology/readme/#framework","title":"Framework","text":"<p>The framework used to implement the pipeline and its code repository</p>"},{"location":"common-metadata-ontology/readme/#properties_3","title":"Properties","text":"<ul> <li>framework_id</li> <li>framework_name</li> <li>code_repo_url</li> <li>framework_version</li> <li>source</li> </ul>"},{"location":"common-metadata-ontology/readme/#stage","title":"Stage","text":"<p>Various stages of the pipeline, such as data preprocessing, training, testing, or evaluation</p>"},{"location":"common-metadata-ontology/readme/#properties_4","title":"Properties","text":"<ul> <li>stage_id</li> <li>stage_name</li> <li>source</li> <li>pipeline_id</li> <li>pipeline_name</li> <li>custom_properties</li> </ul>"},{"location":"common-metadata-ontology/readme/#execution","title":"Execution","text":"<p>Multiple executions of a given stage in a pipeline</p>"},{"location":"common-metadata-ontology/readme/#properties_5","title":"Properties","text":"<ul> <li>execution_id</li> <li>execution_name</li> <li>stage_id</li> <li>stage_name</li> <li>pipeline_id</li> <li>pipeline_name</li> <li>source</li> <li>command (CLI command to run the execution)</li> <li>custom_properties</li> </ul>"},{"location":"common-metadata-ontology/readme/#artifact","title":"Artifact","text":"<p>Artifacts such as model, dataset, and metric generated at the end of each execution</p>"},{"location":"common-metadata-ontology/readme/#properties_6","title":"Properties","text":"<ul> <li>artifact_id</li> <li>artifact_name</li> <li>pipeline_id</li> <li>pipeline_name</li> <li>execution_id</li> <li>source</li> <li>custom_properties</li> </ul>"},{"location":"common-metadata-ontology/readme/#dataset","title":"Dataset","text":"<p>Subclass of artifact. The dataset used in each execution of a pipeline</p>"},{"location":"common-metadata-ontology/readme/#properties_7","title":"Properties","text":"<ul> <li>dataset_id</li> <li>dataset_name</li> <li>dataset_url</li> <li>modality</li> <li>description</li> <li>source</li> <li>custom_properties</li> </ul>"},{"location":"common-metadata-ontology/readme/#model","title":"Model","text":"<p>Subclass of artifact. The model used in each execution or produced as a result of an execution</p>"},{"location":"common-metadata-ontology/readme/#properties_8","title":"Properties","text":"<ul> <li>model_id</li> <li>model_name</li> <li>model_class</li> <li>description</li> <li>artifact_id</li> <li>source</li> <li>custom_properties</li> </ul>"},{"location":"common-metadata-ontology/readme/#metric","title":"Metric","text":"<p>Subclass of artifact. The evaluation result of each execution</p>"},{"location":"common-metadata-ontology/readme/#properties_9","title":"Properties","text":"<ul> <li>metric_id</li> <li>metric_name</li> <li>artifact_id</li> <li>evaluations</li> <li>source</li> <li>custom_properties**</li> </ul>"},{"location":"common-metadata-ontology/readme/#hyperparameters","title":"Hyperparameters","text":"<p>Parameter settings used for each execution of a stage</p>"},{"location":"common-metadata-ontology/readme/#properties_10","title":"Properties","text":"<ul> <li>parameter_id</li> <li>parameter_setting (key-value pair)</li> <li>source</li> <li>model_id</li> <li>custom_properties</li> </ul> <p>NOTE: * are optional properties * There is additional information on each node, different for each source. As of now, these are included in the KG for efficient search, but they are available to be used in the future to extract the data and populate as node properties. * *For metric, there are umpteen possible metric names and values. Therefore, we capture all of them as a key-value pair under evaluations. * custom_properties are where users can enter custom properties for each node while executing a pipeline. * source is the source from which the node is obtained - Papers-with-Code, OpenML, Hugging Face.</p>"},{"location":"common-metadata-ontology/readme/#published-works","title":"Published works","text":"<ul> <li>R. Venkataramanan, A. Tripathy, M. Foltin, H. Y. Yip, A. Justine, and A. Sheth, \"Knowledge Graph Empowered Machine Learning Pipelines for Improved Efficiency, Reusability, and Explainability,\" in IEEE Internet Computing, vol. 27, no. 1, pp. 81-88, 1 Jan.-Feb. 2023, doi: 10.1109/MIC.2022.3228087. Link: https://www.computer.org/csdl/magazine/ic/2023/01/10044293/1KL6TPO5huw</li> </ul>"},{"location":"common-metadata-ontology/readme/#related-works","title":"Related works","text":"<ul> <li>Publio, G. C., Esteves, D., \u0141awrynowicz, A., Panov, P., Soldatova, L., Soru, T., ... &amp; Zafar, H. (2018). ML-schema: exposing the semantics of machine learning with schemas and ontologies. arXiv preprint arXiv:1807.05351. Link - http://ml-schema.github.io/documentation/ML%20Schema.html</li> <li>Nguyen, A., Weller, T., F\u00e4rber, M., &amp; Sure-Vetter, Y. (2020). Making neural networks fair. In Knowledge Graphs and Semantic Web: Second Iberoamerican Conference and First Indo-American Conference, KGSWC 2020, M\u00e9rida, Mexico, November 26\u201327, 2020, Proceedings 2 (pp. 29-44). Springer International Publishing. Link - https://arxiv.org/pdf/1907.11569.pdf</li> <li>Humm, B. G., &amp; Zender, A. (2021). An ontology-based concept for meta AutoML. In Artificial Intelligence Applications and Innovations: 17th IFIP WG 12.5 International Conference, AIAI 2021, Hersonissos, Crete, Greece, June 25\u201327, 2021, Proceedings 17 (pp. 117-128). Springer International Publishing. Link - https://www.researchgate.net/profile/Alexander-Zender-2/publication/352574909_An_Ontology-Based_Concept_for_Meta_AutoML/links/619691e107be5f31b796d2fd/An-Ontology-Based-Concept-for-Meta-AutoML.pdf</li> </ul>"},{"location":"examples/getting_started/","title":"Using <code>cmf</code> to track metadata for a ML Pipeline - NEDDS TO BE UPDATED","text":"<p>This example demonstrates how <code>cmf</code> tracks metadata associated with executions of various machine learning (ML) pipelines. ML pipelines differ from other pipelines (e.g., data Extract-Transform-Load pipelines) by the presence of ML steps, such as training and testing ML models. </p> <p>More comprehensive ML pipelines may include steps such as deploying a trained model and tracking its inference parameters (such as response latency, memory consumption, etc.).</p> <p>This example, located here, implements a simple pipeline consisting of five steps:</p> <ul> <li>The parse step splits   the raw data into   <code>train</code> and <code>test</code> raw datasets for training and testing a machine learning model. This step registers one   input artifact (raw <code>dataset</code>) and two output artifacts (train and test <code>datasets</code>).</li> <li>The featurize   step creates two machine learning splits - train and test splits - that will be used by an ML training algorithm to   train ML models. This step registers two input artifacts (raw train and test datasets) and two output artifacts (   train and test ML datasets).</li> <li>The next train step   trains an ML model (random forest classifier). It registers one input artifact (the dataset from the previous step)   and one output artifact (trained ML model).</li> <li>The fourth test step   evaluates the performance and execution of the ML model trained in the <code>train</code> step. This step registers two input   artifacts (ML model and test dataset) and one output artifact (performance metrics).</li> <li>The last query step   displays each step of the pipeline's metadata as retrieved from the <code>cmf-server</code>, aggregated over all executions.   For example, if you rerun the pipeline again, the output will include not only metadata associated with the latest   run, but also the metadata associated with previous runs.</li> </ul>"},{"location":"examples/getting_started/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure that the <code>cmflib</code> is installed on your system. If not, follow the installation instructions provided in the Installation &amp; Setup page.</p> <p>The initial setup requires creating a workspace directory that will contain all files for this example, cloning the <code>cmf</code> repository that contains source code and data for this example.</p> <pre><code># Create workspace directory\nmkdir cmf_example\ncd cmf_example\n\n# Clone the CMF project from GitHub and install CMF\ngit clone https://github.com/HewlettPackard/cmf\n</code></pre>"},{"location":"examples/getting_started/#project-initialization","title":"Project initialization","text":"<p>First, copy the code and data for this example into its own directory (that must be outside the <code>cmf</code> source tree). Execute the <code>cmf init</code> command specifying the Data Version Control (dvc) directory, the URL of the git remote, address of the <code>cmf-server</code>, and neo4j credentials along with the appropriate dvc backend for this project.</p> <pre><code># Create a separate copy of the example project\ncp -r ./cmf/examples/example-get-started/ ./example-get-started\ncd ./example-get-started\n</code></pre>"},{"location":"examples/getting_started/#cmf-init","title":"cmf init","text":"<pre>\nUsage: cmf init local [-h] --path [path] -\n                           --git-remote-url [git_remote_url]\n                           --cmf-server-url [cmf_server_url]\n                           --neo4j-user [neo4j_user]\n                           --neo4j-password [neo4j_password]\n                           --neo4j-uri [neo4j_uri]\n</pre> <p><code>cmf init local</code> initializes the local directory as a cmf artifact repository. <pre><code>cmf init local --path /home/XXXX/local-storage --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-url http://x.x.x.x:80 --neo4j-user neo4j --neo4j-password password --neo4j-uri bolt://localhost:7687\n</code></pre></p> <p>Replace 'XXXX' with your system username in the following path: /home/XXXX/local-storage</p> <p>Required Arguments <pre><code>  --path [path]                         Specify local directory path.\n  --git-remote-url [git_remote_url]     Specify git repo url.\n</code></pre> Optional Arguments <pre><code>  -h, --help                          show this help message and exit\n  --cmf-server-url [cmf_server_url]   Specify cmf-server url. (default: http://127.0.0.1:80)\n  --neo4j-user [neo4j_user]           Specify neo4j user. (default: None)\n  --neo4j-password [neo4j_password]   Specify neo4j password. (default: None)\n  --neo4j-uri [neo4j_uri]             Specify neo4j uri. Eg bolt://localhost:7687 (default: None)\n</code></pre> Follow here for more details.</p>"},{"location":"examples/getting_started/#project-execution","title":"Project execution","text":"<p>To execute the example pipeline, run the test_script.sh file. In brief, this script runs a sequence of steps typical of machine learning pipelines - getting raw data, splitting that data into machine learning train/test datasets, training the model, and evaluating a model. The execution of these steps (and parent pipeline) will be recorded by the <code>cmf</code>. <pre><code># Run the example pipeline\nsh ./test_script.sh\n</code></pre></p>"},{"location":"examples/getting_started/#setup-a-cmf-server","title":"Setup a <code>cmf-server</code>","text":"<p>Note: This setup step is not required if the cmf-server is already configured.</p> <p>cmf-server is a key interface for the user to explore and track their ML training runs, allowing users to store the metadata file on the cmf-server. The user can retrieve the saved metadata file and view the content of the saved metadata file using the UI provided by the cmf-server.</p> <p>Follow here to set up a common cmf-server.</p>"},{"location":"examples/getting_started/#syncing-metadata-on-the-cmf-server","title":"Syncing metadata on the <code>cmf-server</code>","text":"<p>Metadata generated at each step of the pipeline will be stored in a sqlite file named mlmd. Commits in this repository correspond to the creation of pipeline artifacts and can be viewed with <code>git log</code>.</p> <p>In production settings, the next steps would be to: 1. Execute the <code>cmf artifact push</code> command to push the artifacts to the central artifact repository. 2. Execute the <code>cmf metadata push</code> command to track the metadata of the generated artifacts on a common cmf-server.</p> <p>Follow cmf artifact and cmf metadata for more details.</p>"},{"location":"examples/getting_started/#query","title":"Query","text":"<p>The stored metadata can be explored using the query layer of <code>cmf</code>. The Jupyter notebook Query_Tester-base_mlmd.ipynb demonstrates this functionality and can be adapted for your own uses.</p>"},{"location":"examples/getting_started/#clean-up","title":"Clean Up","text":"<p>Metadata is stored in a sqlite file named \"mlmd\". To clean up, delete the \"mlmd\" file.</p>"},{"location":"examples/getting_started/#steps-to-test-dataslice","title":"Steps to test dataslice","text":"<p>Run the following command: <code>python test-data-slice.py</code>.</p>"},{"location":"setup/","title":"<code>cmf</code> Installation &amp; Setup Guide","text":"<p>This guide provides step-by-step instructions for installing, configuring, and using CMF (Common Metadata Framework) for ML pipeline metadata tracking.</p> <p>The installation process consists of following components:</p> <ol> <li>cmflib: exposes APIs to track the pipeline metadata. It also provides APIs to query the stored metadata.</li> <li>cmf-server with GUI: enables users to store, retrieve, and view ML training metadata through an intuitive UI.</li> </ol>"},{"location":"setup/#prerequisites","title":"Prerequisites","text":"<p>Before installing CMF, ensure you have the following prerequisites:</p> <ul> <li>Linux/Ubuntu/Debian</li> <li>Python: Version 3.9 to 3.11 (3.10 recommended)</li> </ul> <p>\u26a0\ufe0f Warning: \"Python 3.9 Installation Issue on Ubuntu\"</p> <p>Issue: When creating Python 3.9 virtual environments, you may encounter:</p> <pre><code>ModuleNotFoundError: No module named 'distutils.cmd'\n</code></pre> <p>Root Cause: Python 3.9 may be missing required modules like <code>distutils</code> or <code>venv</code> when installed on Ubuntu systems.</p> <p>Resolution:</p> <ol> <li>Add the deadsnakes PPA (provides newer Python versions):</li> </ol> <p><code>bash    sudo add-apt-repository ppa:deadsnakes/ppa    sudo apt-get update</code></p> <ol> <li>Install Python 3.9 with required modules:</li> </ol> <pre><code>sudo apt install python3.9 python3.9-dev python3.9-distutils\n</code></pre> <p>This ensures Python 3.9 and its essential modules are fully installed.</p> <ul> <li> <p>Git: Latest version for code versioning.</p> <p>Make sure Git is properly configured using <code>git config</code>, as it's required for the product. At minimum, set your user identity: <code>bash  git config --global user.name \"Your Name\"  git config --global user.email \"you@example.com\"</code></p> </li> <li> <p>Docker : For containerized deployment of <code>cmf-server</code> and <code>cmf-gui</code>.</p> <ol> <li>Install Docker Engine with non-root user privileges.</li> <li>Install Docker Compose Plugin. In earlier versions of Docker Compose, <code>docker compose</code> was independent of Docker. Hence, <code>docker-compose</code> was the command. However, after the introduction of Docker Compose Desktop V2, the compose command became part of Docker Engine. The recommended way to install Docker Compose is by installing a Docker Compose plugin on Docker Engine. For more information - Docker Compose Reference.</li> </ol> </li> <li>Docker Proxy Settings are needed for some of the server packages. Refer to the official Docker documentation for comprehensive instructions: Configure the Docker Client for Proxy.</li> <li>Storage Backend: S3, MinIOS3, ssh storage, OSDF or local storage for artifacts.</li> </ul>"},{"location":"setup/#components","title":"Components","text":""},{"location":"setup/#install-cmf-library-ie-cmflib","title":"Install cmf library i.e. cmflib","text":"<p>1. Set up Python Virtual Environment</p> Using CondaUsing VirtualEnv <pre><code>conda create -n cmf python=3.10\nconda activate cmf\n</code></pre> <pre><code>virtualenv --python=3.10 .cmf\nsource .cmf/bin/activate\n</code></pre> <p>2. Install CMF:</p> Latest version from GitHubStable version from PyPI <pre><code>pip install git+https://github.com/HewlettPackard/cmf\n</code></pre> <pre><code># pip install cmflib\n</code></pre>"},{"location":"setup/#install-cmf-server-with-gui","title":"Install cmf-server with GUI","text":"<ul> <li> <p>Ensure that Docker is installed on your machine, as mentioned in the prerequisites. If not, please install it before proceeding.</p> </li> <li> <p>Clone the GitHub repository.      <pre><code>git clone https://github.com/HewlettPackard/cmf\n</code></pre></p> </li> <li> <p>Using <code>docker compose</code> File</p> </li> </ul> <p>This is the recommended approach, as <code>docker compose</code> starts the <code>cmf-server</code>, PostgreSQL database, and <code>cmf-gui</code> together. Note: It's essential to start the PostgreSQL database before the <code>cmf-server</code>. 1. Navigate to the <code>cmf</code> directory:</p> <pre><code>cd cmf\n</code></pre> <ol> <li>Create a <code>.env</code> file in the same directory as <code>docker-compose-server.yml</code> with environment variables:</li> </ol> <p>Required variables: <pre><code>CMF_DATA_DIR=./data                    \nNGINX_HTTP_PORT=80                  \nNGINX_HTTPS_PORT=443\nREACT_APP_CMF_API_URL=http://your-server-ip:80\n</code></pre></p> <p>\ud83d\udcdd Note:  - <code>CMF_DATA_DIR</code> controls where all data (PostgreSQL, TensorBoard logs, etc.) is stored. Use an absolute path for better control. - <code>REACT_APP_CMF_API_URL</code> should point to your server's accessible address.</p> <ol> <li>Start the containers:</li> </ol> <pre><code>docker compose -f docker-compose-server.yml up\n</code></pre> <p>\ud83d\udcdd Note: Replace <code>docker compose</code> with <code>docker-compose</code> if you're using an older version of Docker.</p> <p>This command starts all services:    - PostgreSQL: Database backend for metadata storage    - CMF Server: API server for metadata management    - UI: Web interface for visualization    - TensorBoard: For viewing ML training metrics    - Nginx: Reverse proxy serving all components</p> <ol> <li>Stop the containers: <code>bash    docker compose -f docker-compose-server.yml stop</code></li> </ol> <p>\ud83d\udca1 Important: Rebuild the images for <code>cmf-server</code> and <code>cmf-ui</code> after a <code>cmf</code> version update or pulling the latest changes from Git to ensure compatibility.</p>"},{"location":"ui/","title":"Getting Started with cmf-gui","text":"<p>The cmf-gui provides an intuitive, browser-based interface for exploring ML pipeline metadata, visualizing lineage relationships, and monitoring experiment progress. Built with React and D3.js, it offers interactive dashboards for artifacts, executions, and pipeline lineage.</p>"},{"location":"ui/#artifacts-and-executions-pages","title":"Artifacts and Executions Pages","text":"<p>The web interface provides dedicated pages for browsing and analyzing pipeline artifacts and executions.</p>"},{"location":"ui/#artifacts-page","title":"Artifacts Page","text":"<p>The Artifacts page allows users to explore all datasets, models, and metrics tracked by CMF:</p> <pre><code>graph TB\n    subgraph \"Artifacts Page Features\"\n        FILTER[\"Filter Panel&lt;br/&gt;\u2022 Type (Dataset/Model/Metrics)&lt;br/&gt;\u2022 Pipeline Name&lt;br/&gt;\u2022 Custom Properties\"]\n        TABLE[\"Artifacts Table&lt;br/&gt;\u2022 Name and Path&lt;br/&gt;\u2022 Type and Framework&lt;br/&gt;\u2022 Creation Time&lt;br/&gt;\u2022 Associated Pipeline\"]\n        DETAILS[\"Artifact Details&lt;br/&gt;\u2022 Properties and Metadata&lt;br/&gt;\u2022 Version History\"]\n        SEARCH[\"Search and Sort&lt;br/&gt;\u2022 Full-text Search&lt;br/&gt;\u2022 Column Sorting&lt;br/&gt;\u2022 Pagination&lt;br/&gt;\"]\n    end\n\n    FILTER --&gt; TABLE\n    TABLE --&gt; DETAILS\n    TABLE --&gt; SEARCH</code></pre>"},{"location":"ui/#key-features","title":"Key Features","text":"Feature Description Usage Type Filtering Filter by artifact type Select Dataset, Model, or Metrics Pipeline Filtering Filter by pipeline name Choose from available pipelines Search Full-text search across metadata Search names, properties, descriptions Sorting Sort by any column Click column headers to sort Details View Detailed artifact information Click artifact name for details"},{"location":"ui/#artifact-details","title":"Artifact Details","text":"<p>Each artifact provides comprehensive information:</p> <ul> <li>Basic Information: Name, type, creation time</li> <li>Pipeline Context: Associated pipeline, stage, and execution</li> <li>Custom Properties: User-defined metadata and labels</li> <li>Version History: All versions of the artifact with diffs</li> </ul>"},{"location":"ui/#executions-page","title":"Executions Page","text":"<p>The Executions page provides insights into pipeline runs and their performance:</p> <pre><code>graph TB\n    subgraph \"Executions Page Features\"\n        EXEC_FILTER[\"Execution Filters&lt;br/&gt;\u2022 Pipeline Name&lt;br/&gt;\u2022 Execution Type&lt;br/&gt;\"]\n        EXEC_TABLE[\"Executions Table&lt;br/&gt;\u2022 Execution ID and Name&lt;br/&gt;\u2022 Pipeline and Stage&lt;br/&gt;\"]\n        EXEC_DETAILS[\"Execution Details&lt;br/&gt;\u2022 Properties&lt;br/&gt;\u2022 Git Commit Information&lt;br/&gt;\u2022 Environment Details&lt;br/&gt;\u2022 Custom Properties\"]\n        EXEC_SEARCH[\"Search and Sort&lt;br/&gt;\u2022 Full-text Search&lt;br/&gt;\u2022 Column Sorting&lt;br/&gt;\u2022 Pagination&lt;br/&gt;\"]\n    end\n\n    EXEC_FILTER --&gt; EXEC_TABLE\n    EXEC_TABLE --&gt; EXEC_DETAILS\n    EXEC_TABLE --&gt; EXEC_SEARCH</code></pre>"},{"location":"ui/#execution-information","title":"Execution Information","text":"<p>Each execution entry displays:</p> <ul> <li>Execution Metadata: ID, name, type</li> <li>Pipeline Context: Pipeline name, stage, context information</li> <li>Git Information: Commit hash, branch, repository URL</li> <li>Parameters: Execution parameters and configuration</li> </ul>"}]}